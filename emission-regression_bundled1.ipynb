{
    "cells": [
     {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
       "import pandas as pd\n",
       "import numpy as np\n",
       "import altair as alt\n",
       "from sklearn.linear_model import LinearRegression"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Define paths to the individual CSV files\n",
       "csv_files_output = {\n",
       "    '1': 'outputtest-meta-llama/Meta-Llama-3-8B-Instruct-OutputTest-emissiondata-1-examples.csv',\n",
       "    '5': 'outputtest-meta-llama/Meta-Llama-3-8B-Instruct-OutputTest-emissiondata-5-examples.csv',\n",
       "    '10': 'outputtest-meta-llama/Meta-Llama-3-8B-Instruct-OutputTest-emissiondata-10-examples.csv',\n",
       "    '30': 'outputtest-meta-llama/Meta-Llama-3-8B-Instruct-OutputTest-emissiondata-30-examples.csv',\n",
       "    '90': 'outputtest-meta-llama/Meta-Llama-3-8B-Instruct-OutputTest-emissiondata-90-examples.csv',\n",
       "}\n",
       "\n",
       "csv_files_input = {\n",
       "    '1': 'meta-llama/Meta-Llama-3-8B-Instruct-emissiondata-1-examples.csv',\n",
       "    '5': 'meta-llama/Meta-Llama-3-8B-Instruct-emissiondata-5-examples.csv',\n",
       "    '10': 'meta-llama/Meta-Llama-3-8B-Instruct-emissiondata-10-examples.csv',\n",
       "    '20': 'meta-llama/Meta-Llama-3-8B-Instruct-emissiondata-20-examples.csv',\n",
       "    '30': 'meta-llama/Meta-Llama-3-8B-Instruct-emissiondata-30-examples.csv',\n",
       "    '70': 'meta-llama/Meta-Llama-3-8B-Instruct-emissiondata-70-examples.csv',\n",
       "    '210': 'meta-llama/Meta-Llama-3-8B-Instruct-emissiondata-210-examples.csv',\n",
       "    '350': 'meta-llama/Meta-Llama-3-8B-Instruct-emissiondata-350-examples.csv',\n",
       "}\n",
       "\n",
       "csv_files_params_llama2 = {\n",
       "    '7': 'test_1_meta-llama/Llama-2-7b-chat-hf-emissiondata.csv',\n",
       "    '13': 'test_1_meta-llama/Llama-2-13b-chat-hf-emissiondata.csv',\n",
       "    '70': 'test_1_meta-llama/Llama-2-70b-chat-hf-emissiondata.csv',    \n",
       "}\n",
       "\n",
       "csv_files_params_llama3 = {\n",
       "    '8': 'test_1_meta-llama/Meta-Llama-3-8B-Instruct-emissiondata.csv',\n",
       "    '70': 'test_1_meta-llama/Meta-Llama-3-70B-Instruct-emissiondata.csv',  \n",
       "}\n",
       "\n",
       "framework_comp = pd.read_csv('Inference_Framework_Comparison.csv')\n",
       "\n",
       "# Read the emissions data\n",
       "emissions_data = pd.read_csv('emissions.csv')"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Initialize lists to store metadata\n",
       "parameters_output = []\n",
       "num_examples_output = []\n",
       "num_prompts_output = []\n",
       "total_emissions_output = []\n",
       "cpu_energy_output = []\n",
       "gpu_energy_output = []\n",
       "ram_energy_output = []\n",
       "total_output_tokens_output = []\n",
       "total_input_tokens_output = []\n",
       "avg_input_tokens_output = []\n",
       "avg_output_tokens_output = []\n",
       "\n",
       "parameters_input = []\n",
       "num_examples_input = []\n",
       "num_prompts_input = []\n",
       "total_emissions_input = []\n",
       "cpu_energy_input = []\n",
       "gpu_energy_input = []\n",
       "ram_energy_input = []\n",
       "total_output_tokens_input = []\n",
       "total_input_tokens_input = []\n",
       "avg_input_tokens_input = []\n",
       "avg_output_tokens_input = []\n",
       "\n",
       "parameters_llama2 = []\n",
       "num_examples_llama2 = []\n",
       "num_prompts_llama2 = []\n",
       "total_emissions_llama2 = []\n",
       "cpu_energy_llama2 = []\n",
       "gpu_energy_llama2 = []\n",
       "ram_energy_llama2 = []\n",
       "total_output_tokens_llama2 = []\n",
       "total_input_tokens_llama2 = []\n",
       "avg_input_tokens_llama2 = []\n",
       "avg_output_tokens_llama2 = []\n",
       "\n",
       "parameters_llama3 = []\n",
       "num_examples_llama3 = []\n",
       "num_prompts_llama3 = []\n",
       "total_emissions_llama3 = []\n",
       "cpu_energy_llama3 = []\n",
       "gpu_energy_llama3 = []\n",
       "ram_energy_llama3 = []\n",
       "total_output_tokens_llama3 = []\n",
       "total_input_tokens_llama3 = []\n",
       "avg_input_tokens_llama3 = []\n",
       "avg_output_tokens_llama3 = []"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Read and extract metadata from each CSV file\n",
       "for model, file in csv_files_output.items():\n",
       "    data = pd.read_csv(file)\n",
       "    prompts = data.loc[data['Metric'] == 'Total Prompts', 'Value'].values[0]\n",
       "    output_tokens = data.loc[data['Metric'] == 'Total Output Tokens', 'Value'].values[0]\n",
       "    input_tokens = data.loc[data['Metric'] == 'Total Input Tokens', 'Value'].values[0]\n",
       "    avg_i_tok = data.loc[data['Metric'] == 'Avg Input Tokens per Prompt', 'Value'].values[0]\n",
       "    avg_o_tok = data.loc[data['Metric'] == 'Avg Output Tokens per Prompt', 'Value'].values[0]\n",
       "    parameters_output.append(8)\n",
       "    num_examples_output.append(int(model))\n",
       "    num_prompts_output.append(int(prompts))\n",
       "    total_output_tokens_output.append(float(output_tokens))\n",
       "    total_input_tokens_output.append(float(input_tokens))\n",
       "    avg_input_tokens_output.append(float(avg_i_tok))\n",
       "    avg_output_tokens_output.append(float(avg_o_tok))    "
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
       "for model, file in csv_files_input.items():\n",
       "    data = pd.read_csv(file)\n",
       "    prompts = data.loc[data['Metric'] == 'Total Prompts', 'Value'].values[0]\n",
       "    output_tokens = data.loc[data['Metric'] == 'Total Output Tokens', 'Value'].values[0]\n",
       "    input_tokens = data.loc[data['Metric'] == 'Total Input Tokens', 'Value'].values[0]\n",
       "    avg_i_tok = data.loc[data['Metric'] == 'Avg Input Tokens per Prompt', 'Value'].values[0]\n",
       "    avg_o_tok = data.loc[data['Metric'] == 'Avg Output Tokens per Prompt', 'Value'].values[0]\n",
       "    parameters_input.append(8)\n",
       "    num_examples_input.append(int(model))\n",
       "    num_prompts_input.append(int(prompts))\n",
       "    total_output_tokens_input.append(float(output_tokens))\n",
       "    total_input_tokens_input.append(float(input_tokens))\n",
       "    avg_input_tokens_input.append(float(avg_i_tok))\n",
       "    avg_output_tokens_input.append(float(avg_o_tok))"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
       "for model, file in csv_files_params_llama2.items():\n",
       "    data = pd.read_csv(file)\n",
       "    prompts = data.loc[data['Metric'] == 'Total Prompts', 'Value'].values[0]\n",
       "    output_tokens = data.loc[data['Metric'] == 'Total Output Tokens', 'Value'].values[0]\n",
       "    input_tokens = data.loc[data['Metric'] == 'Total Input Tokens', 'Value'].values[0]\n",
       "    avg_i_tok = data.loc[data['Metric'] == 'Avg Input Tokens per Prompt', 'Value'].values[0]\n",
       "    avg_o_tok = data.loc[data['Metric'] == 'Avg Output Tokens per Prompt', 'Value'].values[0]\n",
       "    parameters_llama2.append(int(model))\n",
       "    num_examples_llama2.append(1)\n",
       "    num_prompts_llama2.append(int(prompts))\n",
       "    total_output_tokens_llama2.append(float(output_tokens))\n",
       "    total_input_tokens_llama2.append(float(input_tokens))\n",
       "    avg_input_tokens_llama2.append(float(avg_i_tok))\n",
       "    avg_output_tokens_llama2.append(float(avg_o_tok))"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
       "for model, file in csv_files_params_llama3.items():\n",
       "    data = pd.read_csv(file)\n",
       "    prompts = data.loc[data['Metric'] == 'Total Prompts', 'Value'].values[0]\n",
       "    output_tokens = data.loc[data['Metric'] == 'Total Output Tokens', 'Value'].values[0]\n",
       "    input_tokens = data.loc[data['Metric'] == 'Total Input Tokens', 'Value'].values[0]\n",
       "    avg_i_tok = data.loc[data['Metric'] == 'Avg Input Tokens per Prompt', 'Value'].values[0]\n",
       "    avg_o_tok = data.loc[data['Metric'] == 'Avg Output Tokens per Prompt', 'Value'].values[0]\n",
       "    parameters_llama3.append(int(model))\n",
       "    num_examples_llama3.append(1)\n",
       "    num_prompts_llama3.append(int(prompts))\n",
       "    total_output_tokens_llama3.append(float(output_tokens))\n",
       "    total_input_tokens_llama3.append(float(input_tokens))\n",
       "    avg_input_tokens_llama3.append(float(avg_i_tok))\n",
       "    avg_output_tokens_llama3.append(float(avg_o_tok))"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Extract emissions data\n",
       "for model in csv_files_output.keys():\n",
       "    model_emissions = emissions_data[emissions_data['project_name'].str.contains(\"Llama-3-8B-Instruct-OutputTest-emissiondata-\" + model + \"-\")]\n",
       "    total_emissions_output.append(model_emissions['emissions'].values[0])\n",
       "    cpu_energy_output.append(model_emissions['cpu_energy'].values[0])\n",
       "    gpu_energy_output.append(model_emissions['gpu_energy'].values[0])\n",
       "    ram_energy_output.append(model_emissions['ram_energy'].values[0])"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "[186.66, 186.66, 186.66, 186.66, 186.66]\n",
         "[18.027, 73.053, 176.913, 587.78, 2124.58]\n",
         "[2704.0, 10958.0, 26537.0, 29389.0, 106229.0]\n",
         "[0.0107639213548215, 0.0439809911016423, 0.1084166041887974, 0.1236233887845658, 0.4659625892970863]\n"
        ]
       }
      ],
      "source": [
       "print(avg_input_tokens_output)\n",
       "print(avg_output_tokens_output)\n",
       "print(total_output_tokens_output)\n",
       "print(total_emissions_output)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
       "for model in csv_files_input.keys(): \n",
       "    model_emissions = emissions_data[emissions_data['project_name'].str.contains(\"Llama-3-8B-Instruct-emissiondata-\" + model + \"-\")]\n",
       "    total_emissions_input.append(model_emissions['emissions'].values[0])\n",
       "    cpu_energy_input.append(model_emissions['cpu_energy'].values[0])\n",
       "    gpu_energy_input.append(model_emissions['gpu_energy'].values[0])\n",
       "    ram_energy_input.append(model_emissions['ram_energy'].values[0])"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "[158.66, 222.66, 301.66, 462.66, 627.66, 1343.66, 3743.66, 6143.66]\n",
         "[18.007, 18.04, 17.94, 18.047, 18.033, 18.24, 18.513, 18.807]\n",
         "[2701.0, 2706.0, 2691.0, 2707.0, 2705.0, 2736.0, 2777.0, 2821.0]\n",
         "[0.0199979467296797, 0.0203268454993705, 0.0205117219125248, 0.0211612903588763, 0.021899838445093, 0.0248178578752835, 0.0364961148272292, 0.0511827769119716]\n"
        ]
       }
      ],
      "source": [
       "print(avg_input_tokens_input)\n",
       "print(avg_output_tokens_input)\n",
       "print(total_output_tokens_input)\n",
       "print(total_emissions_input)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
       "for model in csv_files_params_llama2.keys():\n",
       "    model_emissions = emissions_data[emissions_data['project_name'].str.contains(\"meta-llama/Llama-2-\" + model + \"b-chat-hf\")]\n",
       "    total_emissions_llama2.append(model_emissions['emissions'].values[0])\n",
       "    cpu_energy_llama2.append(model_emissions['cpu_energy'].values[0])\n",
       "    gpu_energy_llama2.append(model_emissions['gpu_energy'].values[0])\n",
       "    ram_energy_llama2.append(model_emissions['ram_energy'].values[0])"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "[168.0, 168.0, 147.0]\n",
         "[37.616, 41.008, 39.848]\n",
         "[9404.0, 10252.0, 9962.0]\n",
         "[0.0672881651768405, 0.0968092248640126, 0.3127521811156471]\n"
        ]
       }
      ],
      "source": [
       "print(avg_input_tokens_llama2)\n",
       "print(avg_output_tokens_llama2)\n",
       "print(total_output_tokens_llama2)\n",
       "print(total_emissions_llama2)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
       "for model in csv_files_params_llama3.keys():\n",
       "    model_emissions = emissions_data[emissions_data['project_name'].str.contains(\"meta-llama/Meta-Llama-3-\" + model + \"B-Instruct-params\")]\n",
       "    total_emissions_llama3.append(model_emissions['emissions'].values[0])\n",
       "    cpu_energy_llama3.append(model_emissions['cpu_energy'].values[0])\n",
       "    gpu_energy_llama3.append(model_emissions['gpu_energy'].values[0])\n",
       "    ram_energy_llama3.append(model_emissions['ram_energy'].values[0])"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "[132.66, 132.66]\n",
         "[18.096, 18.79]\n",
         "[4524.0, 1879.0]\n",
         "[0.0337420699294056, 0.0532702232629774]\n"
        ]
       }
      ],
      "source": [
       "print(avg_input_tokens_llama3)\n",
       "print(avg_output_tokens_llama3)\n",
       "print(total_output_tokens_llama3)\n",
       "print(total_emissions_llama3)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Prepare data for regression and visualization\n",
       "parameters_output = np.array(parameters_output)\n",
       "num_examples_output = np.array(num_examples_output)\n",
       "num_prompts_output = np.array(num_prompts_output)\n",
       "total_output_tokens_output = np.array(total_output_tokens_output)\n",
       "total_input_tokens_output = np.array(total_input_tokens_output)\n",
       "avg_input_tokens_output = np.array(avg_input_tokens_output)\n",
       "avg_output_tokens_output = np.array(avg_output_tokens_output)\n",
       "total_emissions_output = np.array(total_emissions_output)\n",
       "cpu_energy_output = np.array(cpu_energy_output)\n",
       "gpu_energy_output = np.array(gpu_energy_output)\n",
       "ram_energy_output = np.array(ram_energy_output)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
       "parameters_input = np.array(parameters_input)\n",
       "num_examples_input = np.array(num_examples_input)\n",
       "num_prompts_input = np.array(num_prompts_input)\n",
       "total_output_tokens_input = np.array(total_output_tokens_input)\n",
       "total_input_tokens_input = np.array(total_input_tokens_input)\n",
       "avg_input_tokens_input = np.array(avg_input_tokens_input)\n",
       "avg_output_tokens_input = np.array(avg_output_tokens_input)\n",
       "total_emissions_input = np.array(total_emissions_input)\n",
       "cpu_energy_input = np.array(cpu_energy_input)\n",
       "gpu_energy_input = np.array(gpu_energy_input)\n",
       "ram_energy_input = np.array(ram_energy_input)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
       "parameters_llama2 = np.array(parameters_llama2)\n",
       "num_examples_llama2 = np.array(num_examples_llama2)\n",
       "num_prompts_llama2 = np.array(num_prompts_llama2)\n",
       "total_output_tokens_llama2 = np.array(total_output_tokens_llama2)\n",
       "total_input_tokens_llama2 = np.array(total_input_tokens_llama2)\n",
       "avg_input_tokens_llama2 = np.array(avg_input_tokens_llama2)\n",
       "avg_output_tokens_llama2 = np.array(avg_output_tokens_llama2)\n",
       "total_emissions_llama2 = np.array(total_emissions_llama2)\n",
       "cpu_energy_llama2 = np.array(cpu_energy_llama2)\n",
       "gpu_energy_llama2 = np.array(gpu_energy_llama2)\n",
       "ram_energy_llama2 = np.array(ram_energy_llama2)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
       "parameters_llama3 = np.array(parameters_llama3)\n",
       "num_examples_llama3 = np.array(num_examples_llama3)\n",
       "num_prompts_llama3 = np.array(num_prompts_llama3)\n",
       "total_output_tokens_llama3 = np.array(total_output_tokens_llama3)\n",
       "total_input_tokens_llama3 = np.array(total_input_tokens_llama3)\n",
       "avg_input_tokens_llama3 = np.array(avg_input_tokens_llama3)\n",
       "avg_output_tokens_llama3 = np.array(avg_output_tokens_llama3)\n",
       "total_emissions_llama3 = np.array(total_emissions_llama3)\n",
       "cpu_energy_llama3 = np.array(cpu_energy_llama3)\n",
       "gpu_energy_llama3 = np.array(gpu_energy_llama3)\n",
       "ram_energy_llama3 = np.array(ram_energy_llama3)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Calculate emissions per 10,000 prompts\n",
       "emissions_per_thousand_prompts = {\n",
       "    'Total Emissions Output Tok': total_emissions_output / num_prompts_output * 10_000,\n",
       "    'CPU Energy Output Tok': cpu_energy_output / num_prompts_output * 10_000,\n",
       "    'GPU Energy Output Tok': gpu_energy_output / num_prompts_output * 10_000,\n",
       "    'RAM Energy Output Tok': ram_energy_output / num_prompts_output * 10_000,\n",
       "\n",
       "    'Total Emissions Input Tok': total_emissions_input / num_prompts_input * 10_000,\n",
       "    'CPU Energy Input Tok': cpu_energy_input / num_prompts_input * 10_000,\n",
       "    'GPU Energy Input Tok': gpu_energy_input / num_prompts_input * 10_000,\n",
       "    'RAM Energy Input Tok': ram_energy_input / num_prompts_input * 10_000,\n",
       "\n",
       "    # We have to normalize the emissions to the number of output tokens for llama2, because the llama2 and llama3 output differed to drastically\n",
       "    'Total Emissions Llama2 Params': (total_emissions_llama2 / num_prompts_llama2) / avg_output_tokens_llama2 * 10_000 * 18.5, \n",
       "    'CPU Energy Llama2 Params': (cpu_energy_llama2 / num_prompts_llama2) / avg_output_tokens_llama2 * 10_000 * 18.5,\n",
       "    'GPU Energy Llama2 Params': (gpu_energy_llama2 / num_prompts_llama2) / avg_output_tokens_llama2 * 10_000 * 18.5,\n",
       "    'RAM Energy Llama2 Params': (ram_energy_llama2 / num_prompts_llama2) / avg_output_tokens_llama2 * 10_000 * 18.5,\n",
       "\n",
       "    'Total Emissions Llama3 Params': total_emissions_llama3 / num_prompts_llama3 * 10_000,\n",
       "    'CPU Energy Llama3 Params': cpu_energy_llama3 / num_prompts_llama3 * 10_000,\n",
       "    'GPU Energy Llama3 Params': gpu_energy_llama3 / num_prompts_llama3 * 10_000,\n",
       "    'RAM Energy Llama3 Params': ram_energy_llama3 / num_prompts_llama3 * 10_000,\n",
       "}\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "{'Total Emissions Output Tok': array([ 0.71759476,  2.93206607,  7.22777361, 24.72467776, 93.19251786]), 'CPU Energy Output Tok': array([ 0.34119865,  1.38108   ,  3.41097504, 11.68430222, 43.35962526]), 'GPU Energy Output Tok': array([ 0.37330112,  1.55227741,  3.81306506, 13.01042801, 50.44779913]), 'RAM Energy Output Tok': array([ 0.36467404,  1.47610751,  3.64563793, 12.48812872, 46.34260226]), 'Total Emissions Input Tok': array([1.33319645, 1.35512303, 1.36744813, 1.41075269, 1.45998923,\n",
         "       1.65452386, 2.43307432, 3.41218513]), 'CPU Energy Input Tok': array([0.34934319, 0.34979593, 0.34966325, 0.36020338, 0.36834101,\n",
         "       0.41130111, 0.58992712, 0.8125549 ]), 'GPU Energy Input Tok': array([1.09583639, 1.12760167, 1.14650559, 1.18416797, 1.23706802,\n",
         "       1.41778184, 2.12381272, 3.01694606]), 'RAM Energy Input Tok': array([0.55978307, 0.56053992, 0.56030411, 0.57722631, 0.59023429,\n",
         "       0.6591164 , 0.94530317, 1.30200367]), 'Total Emissions Llama2 Params': array([1.32372507, 1.74694758, 5.8079857 ]), 'CPU Energy Llama2 Params': array([0.34632428, 0.44688363, 1.3884783 ]), 'GPU Energy Llama2 Params': array([1.08942027, 1.46419566, 5.12116977]), 'RAM Energy Llama2 Params': array([0.55497432, 0.71611392, 2.22484444]), 'Total Emissions Llama3 Params': array([1.3496828 , 5.32702233]), 'CPU Energy Llama3 Params': array([0.34920031, 1.26304503]), 'GPU Energy Llama3 Params': array([1.12099379, 4.7242477 ]), 'RAM Energy Llama3 Params': array([0.55956198, 2.02389034])}\n"
        ]
       }
      ],
      "source": [
       "print(emissions_per_thousand_prompts)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "[8 8 8 8 8 8 8 8]\n",
         "[8 8 8 8 8]\n",
         "[ 7 13 70]\n",
         "[ 8 70]\n"
        ]
       }
      ],
      "source": [
       "print(parameters_input)\n",
       "print(parameters_output)\n",
       "print(parameters_llama2)\n",
       "print(parameters_llama3)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Perform regression analysis\n",
       "def perform_regression(x, y):\n",
       "    x = x.reshape(-1, 1)\n",
       "    model = LinearRegression()\n",
       "    model.fit(x, y)\n",
       "    predicted = model.predict(x)\n",
       "    return model, predicted"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Total Emissions Output Tok - Intercept: -0.4666966237208392, Coefficient: 0.04399751075726668\n",
         "Error, could not find model\n",
         "CPU Energy Output Tok - Intercept: -0.4666966237208392, Coefficient: 0.04399751075726668\n",
         "Error, could not find model\n",
         "GPU Energy Output Tok - Intercept: -0.4666966237208392, Coefficient: 0.04399751075726668\n",
         "Error, could not find model\n",
         "RAM Energy Output Tok - Intercept: -0.4666966237208392, Coefficient: 0.04399751075726668\n",
         "Total Emissions Input Tok - Intercept: 1.2480841224646957, Coefficient: 0.00034155061699822484\n",
         "Error, could not find model\n",
         "CPU Energy Input Tok - Intercept: 1.2480841224646957, Coefficient: 0.00034155061699822484\n",
         "Error, could not find model\n",
         "GPU Energy Input Tok - Intercept: 1.2480841224646957, Coefficient: 0.00034155061699822484\n",
         "Error, could not find model\n",
         "RAM Energy Input Tok - Intercept: 1.2480841224646957, Coefficient: 0.00034155061699822484\n",
         "Total Emissions Llama2 Params - Intercept: 0.8233785594775198, Coefficient: 0.07120580743041335\n",
         "Error, could not find model\n",
         "CPU Energy Llama2 Params - Intercept: 0.8233785594775198, Coefficient: 0.07120580743041335\n",
         "Error, could not find model\n",
         "GPU Energy Llama2 Params - Intercept: 0.8233785594775198, Coefficient: 0.07120580743041335\n",
         "Error, could not find model\n",
         "RAM Energy Llama2 Params - Intercept: 0.8233785594775198, Coefficient: 0.07120580743041335\n",
         "Total Emissions Llama3 Params - Intercept: 0.836477696644415, Coefficient: 0.06415063756647607\n",
         "Error, could not find model\n",
         "CPU Energy Llama3 Params - Intercept: 0.836477696644415, Coefficient: 0.06415063756647607\n",
         "Error, could not find model\n",
         "GPU Energy Llama3 Params - Intercept: 0.836477696644415, Coefficient: 0.06415063756647607\n",
         "Error, could not find model\n",
         "RAM Energy Llama3 Params - Intercept: 0.836477696644415, Coefficient: 0.06415063756647607\n"
        ]
       }
      ],
      "source": [
       "models = {}\n",
       "predictions = {}\n",
       "for name, y in emissions_per_thousand_prompts.items():\n",
       "    if(name == \"Total Emissions Output Tok\"):\n",
       "        model, predicted = perform_regression(avg_output_tokens_output, y)\n",
       "    elif(name == \"Total Emissions Input Tok\"):\n",
       "        model, predicted = perform_regression(avg_input_tokens_input, y)\n",
       "    elif(name == \"Total Emissions Llama2 Params\"):\n",
       "        model, predicted = perform_regression(parameters_llama2, y)\n",
       "    elif(name == \"Total Emissions Llama3 Params\"):\n",
       "        model, predicted = perform_regression(parameters_llama3, y)\n",
       "    else:\n",
       "        print(\"Error, could not find model\")\n",
       "    models[name] = model\n",
       "    predictions[name] = predicted\n",
       "    print(f\"{name} - Intercept: {model.intercept_}, Coefficient: {model.coef_[0]}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Define the test types and model types\n",
       "test_types = ['Output-tok', 'Input-tok', 'Llama2 Params', 'Llama3 Params']\n",
       "model_types = ['llama3', 'llama3', 'llama2', 'llama3']\n",
       "\n",
       "# Define the parameters\n",
       "parameters = np.concatenate([parameters_output, parameters_input, parameters_llama2, parameters_llama3])\n",
       "num_examples = np.concatenate([num_examples_output, num_examples_input, num_examples_llama2, num_examples_llama3])\n",
       "num_prompts = np.concatenate([num_prompts_output, num_prompts_input, num_prompts_llama2, num_prompts_llama3])\n",
       "total_out_tok = np.concatenate([total_output_tokens_output, total_output_tokens_input, total_output_tokens_llama2, total_output_tokens_llama3])\n",
       "total_in_tok = np.concatenate([total_input_tokens_output, total_input_tokens_input, total_input_tokens_llama2, total_input_tokens_llama3])\n",
       "avg_out_tok = np.concatenate([avg_output_tokens_output, avg_output_tokens_input, avg_output_tokens_llama2, avg_output_tokens_llama3])\n",
       "avg_in_tok = np.concatenate([avg_input_tokens_output, avg_input_tokens_input, avg_input_tokens_llama2, avg_input_tokens_llama3])\n",
       "\n",
       "pred_emissions_per_10k_prompts = np.concatenate([\n",
       "    predictions['Total Emissions Output Tok'],\n",
       "    predictions['Total Emissions Input Tok'],\n",
       "    predictions['Total Emissions Llama2 Params'],\n",
       "    predictions['Total Emissions Llama3 Params']\n",
       "])\n",
       "\n",
       "pred_cpu_energy_per_10k_prompts = np.concatenate([\n",
       "    predictions['CPU Energy Output Tok'],\n",
       "    predictions['CPU Energy Input Tok'],\n",
       "    predictions['CPU Energy Llama2 Params'],\n",
       "    predictions['CPU Energy Llama3 Params']\n",
       "])\n",
       "\n",
       "pred_gpu_energy_per_10k_prompts = np.concatenate([\n",
       "    predictions['GPU Energy Output Tok'],\n",
       "    predictions['GPU Energy Input Tok'],\n",
       "    predictions['GPU Energy Llama2 Params'],\n",
       "    predictions['GPU Energy Llama3 Params']\n",
       "])\n",
       "\n",
       "pred_ram_energy_per_10k_prompts = np.concatenate([\n",
       "    predictions['RAM Energy Output Tok'],\n",
       "    predictions['RAM Energy Input Tok'],\n",
       "    predictions['RAM Energy Llama2 Params'],\n",
       "    predictions['RAM Energy Llama3 Params']\n",
       "])\n",
       "\n",
       "actual_emissions_per_10k_prompts = np.concatenate([\n",
       "    emissions_per_thousand_prompts['Total Emissions Output Tok'],\n",
       "    emissions_per_thousand_prompts['Total Emissions Input Tok'],\n",
       "    emissions_per_thousand_prompts['Total Emissions Llama2 Params'],\n",
       "    emissions_per_thousand_prompts['Total Emissions Llama3 Params']\n",
       "])\n",
       "\n",
       "actual_cpu_energy_per_10k_prompts = np.concatenate([\n",
       "    emissions_per_thousand_prompts['CPU Energy Output Tok'],\n",
       "    emissions_per_thousand_prompts['CPU Energy Input Tok'],\n",
       "    emissions_per_thousand_prompts['CPU Energy Llama2 Params'],\n",
       "    emissions_per_thousand_prompts['CPU Energy Llama3 Params']\n",
       "])\n",
       "\n",
       "actual_gpu_energy_per_10k_prompts = np.concatenate([\n",
       "    emissions_per_thousand_prompts['GPU Energy Output Tok'],\n",
       "    emissions_per_thousand_prompts['GPU Energy Input Tok'],\n",
       "    emissions_per_thousand_prompts['GPU Energy Llama2 Params'],\n",
       "    emissions_per_thousand_prompts['GPU Energy Llama3 Params']\n",
       "])\n",
       "\n",
       "actual_ram_energy_per_10k_prompts = np.concatenate([\n",
       "    emissions_per_thousand_prompts['RAM Energy Output Tok'],\n",
       "    emissions_per_thousand_prompts['RAM Energy Input Tok'],\n",
       "    emissions_per_thousand_prompts['RAM Energy Llama2 Params'],\n",
       "    emissions_per_thousand_prompts['RAM Energy Llama3 Params']\n",
       "])"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Repeat test types and model types for each data point\n",
       "test_type_column = np.concatenate([\n",
       "    np.repeat(test_types[0], len(parameters_output)),\n",
       "    np.repeat(test_types[1], len(parameters_input)),\n",
       "    np.repeat(test_types[2], len(parameters_llama2)),\n",
       "    np.repeat(test_types[3], len(parameters_llama3))\n",
       "])\n",
       "\n",
       "model_type_column = np.concatenate([\n",
       "    np.repeat(model_types[0], len(parameters_output)),\n",
       "    np.repeat(model_types[1], len(parameters_input)),\n",
       "    np.repeat(model_types[2], len(parameters_llama2)),\n",
       "    np.repeat(model_types[3], len(parameters_llama3))\n",
       "])"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/html": [
          "<div>\n",
          "<style scoped>\n",
          "    .dataframe tbody tr th:only-of-type {\n",
          "        vertical-align: middle;\n",
          "    }\n",
          "\n",
          "    .dataframe tbody tr th {\n",
          "        vertical-align: top;\n",
          "    }\n",
          "\n",
          "    .dataframe thead th {\n",
          "        text-align: right;\n",
          "    }\n",
          "</style>\n",
          "<table border=\"1\" class=\"dataframe\">\n",
          "  <thead>\n",
          "    <tr style=\"text-align: right;\">\n",
          "      <th></th>\n",
          "      <th>test_type</th>\n",
          "      <th>model_type</th>\n",
          "      <th>parameters</th>\n",
          "      <th>num_examples</th>\n",
          "      <th>num_prompts</th>\n",
          "      <th>total_out_tok</th>\n",
          "      <th>total_in_tok</th>\n",
          "      <th>avg_out_tok</th>\n",
          "      <th>avg_in_tok</th>\n",
          "      <th>actual_emissions_per_10k_prompts</th>\n",
          "      <th>actual_cpu_energy_per_10k_prompts</th>\n",
          "      <th>actual_gpu_energy_per_10k_prompts</th>\n",
          "      <th>actual_ram_energy_per_10k_prompts</th>\n",
          "      <th>pred_emissions_per_10k_prompts</th>\n",
          "      <th>pred_cpu_energy_per_10k_prompts</th>\n",
          "      <th>pred_gpu_energy_per_10k_prompts</th>\n",
          "      <th>pred_ram_energy_per_10k_prompts</th>\n",
          "    </tr>\n",
          "  </thead>\n",
          "  <tbody>\n",
          "    <tr>\n",
          "      <th>0</th>\n",
          "      <td>Output-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8</td>\n",
          "      <td>1</td>\n",
          "      <td>150</td>\n",
          "      <td>2704.0</td>\n",
          "      <td>27999.0</td>\n",
          "      <td>18.027</td>\n",
          "      <td>186.66</td>\n",
          "      <td>0.717595</td>\n",
          "      <td>0.341199</td>\n",
          "      <td>0.373301</td>\n",
          "      <td>0.364674</td>\n",
          "      <td>0.326447</td>\n",
          "      <td>0.326447</td>\n",
          "      <td>0.326447</td>\n",
          "      <td>0.326447</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>1</th>\n",
          "      <td>Output-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8</td>\n",
          "      <td>5</td>\n",
          "      <td>150</td>\n",
          "      <td>10958.0</td>\n",
          "      <td>27999.0</td>\n",
          "      <td>73.053</td>\n",
          "      <td>186.66</td>\n",
          "      <td>2.932066</td>\n",
          "      <td>1.381080</td>\n",
          "      <td>1.552277</td>\n",
          "      <td>1.476108</td>\n",
          "      <td>2.747454</td>\n",
          "      <td>2.747454</td>\n",
          "      <td>2.747454</td>\n",
          "      <td>2.747454</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>2</th>\n",
          "      <td>Output-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8</td>\n",
          "      <td>10</td>\n",
          "      <td>150</td>\n",
          "      <td>26537.0</td>\n",
          "      <td>27999.0</td>\n",
          "      <td>176.913</td>\n",
          "      <td>186.66</td>\n",
          "      <td>7.227774</td>\n",
          "      <td>3.410975</td>\n",
          "      <td>3.813065</td>\n",
          "      <td>3.645638</td>\n",
          "      <td>7.317035</td>\n",
          "      <td>7.317035</td>\n",
          "      <td>7.317035</td>\n",
          "      <td>7.317035</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>3</th>\n",
          "      <td>Output-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8</td>\n",
          "      <td>30</td>\n",
          "      <td>50</td>\n",
          "      <td>29389.0</td>\n",
          "      <td>9333.0</td>\n",
          "      <td>587.780</td>\n",
          "      <td>186.66</td>\n",
          "      <td>24.724678</td>\n",
          "      <td>11.684302</td>\n",
          "      <td>13.010428</td>\n",
          "      <td>12.488129</td>\n",
          "      <td>25.394160</td>\n",
          "      <td>25.394160</td>\n",
          "      <td>25.394160</td>\n",
          "      <td>25.394160</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>4</th>\n",
          "      <td>Output-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8</td>\n",
          "      <td>90</td>\n",
          "      <td>50</td>\n",
          "      <td>106229.0</td>\n",
          "      <td>9333.0</td>\n",
          "      <td>2124.580</td>\n",
          "      <td>186.66</td>\n",
          "      <td>93.192518</td>\n",
          "      <td>43.359625</td>\n",
          "      <td>50.447799</td>\n",
          "      <td>46.342602</td>\n",
          "      <td>93.009535</td>\n",
          "      <td>93.009535</td>\n",
          "      <td>93.009535</td>\n",
          "      <td>93.009535</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>5</th>\n",
          "      <td>Input-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8</td>\n",
          "      <td>1</td>\n",
          "      <td>150</td>\n",
          "      <td>2701.0</td>\n",
          "      <td>23799.0</td>\n",
          "      <td>18.007</td>\n",
          "      <td>158.66</td>\n",
          "      <td>1.333196</td>\n",
          "      <td>0.349343</td>\n",
          "      <td>1.095836</td>\n",
          "      <td>0.559783</td>\n",
          "      <td>1.302275</td>\n",
          "      <td>1.302275</td>\n",
          "      <td>1.302275</td>\n",
          "      <td>1.302275</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>6</th>\n",
          "      <td>Input-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8</td>\n",
          "      <td>5</td>\n",
          "      <td>150</td>\n",
          "      <td>2706.0</td>\n",
          "      <td>33399.0</td>\n",
          "      <td>18.040</td>\n",
          "      <td>222.66</td>\n",
          "      <td>1.355123</td>\n",
          "      <td>0.349796</td>\n",
          "      <td>1.127602</td>\n",
          "      <td>0.560540</td>\n",
          "      <td>1.324134</td>\n",
          "      <td>1.324134</td>\n",
          "      <td>1.324134</td>\n",
          "      <td>1.324134</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>7</th>\n",
          "      <td>Input-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8</td>\n",
          "      <td>10</td>\n",
          "      <td>150</td>\n",
          "      <td>2691.0</td>\n",
          "      <td>45249.0</td>\n",
          "      <td>17.940</td>\n",
          "      <td>301.66</td>\n",
          "      <td>1.367448</td>\n",
          "      <td>0.349663</td>\n",
          "      <td>1.146506</td>\n",
          "      <td>0.560304</td>\n",
          "      <td>1.351116</td>\n",
          "      <td>1.351116</td>\n",
          "      <td>1.351116</td>\n",
          "      <td>1.351116</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>8</th>\n",
          "      <td>Input-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8</td>\n",
          "      <td>20</td>\n",
          "      <td>150</td>\n",
          "      <td>2707.0</td>\n",
          "      <td>69399.0</td>\n",
          "      <td>18.047</td>\n",
          "      <td>462.66</td>\n",
          "      <td>1.410753</td>\n",
          "      <td>0.360203</td>\n",
          "      <td>1.184168</td>\n",
          "      <td>0.577226</td>\n",
          "      <td>1.406106</td>\n",
          "      <td>1.406106</td>\n",
          "      <td>1.406106</td>\n",
          "      <td>1.406106</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>9</th>\n",
          "      <td>Input-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8</td>\n",
          "      <td>30</td>\n",
          "      <td>150</td>\n",
          "      <td>2705.0</td>\n",
          "      <td>94149.0</td>\n",
          "      <td>18.033</td>\n",
          "      <td>627.66</td>\n",
          "      <td>1.459989</td>\n",
          "      <td>0.368341</td>\n",
          "      <td>1.237068</td>\n",
          "      <td>0.590234</td>\n",
          "      <td>1.462462</td>\n",
          "      <td>1.462462</td>\n",
          "      <td>1.462462</td>\n",
          "      <td>1.462462</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>10</th>\n",
          "      <td>Input-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8</td>\n",
          "      <td>70</td>\n",
          "      <td>150</td>\n",
          "      <td>2736.0</td>\n",
          "      <td>201549.0</td>\n",
          "      <td>18.240</td>\n",
          "      <td>1343.66</td>\n",
          "      <td>1.654524</td>\n",
          "      <td>0.411301</td>\n",
          "      <td>1.417782</td>\n",
          "      <td>0.659116</td>\n",
          "      <td>1.707012</td>\n",
          "      <td>1.707012</td>\n",
          "      <td>1.707012</td>\n",
          "      <td>1.707012</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>11</th>\n",
          "      <td>Input-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8</td>\n",
          "      <td>210</td>\n",
          "      <td>150</td>\n",
          "      <td>2777.0</td>\n",
          "      <td>561549.0</td>\n",
          "      <td>18.513</td>\n",
          "      <td>3743.66</td>\n",
          "      <td>2.433074</td>\n",
          "      <td>0.589927</td>\n",
          "      <td>2.123813</td>\n",
          "      <td>0.945303</td>\n",
          "      <td>2.526734</td>\n",
          "      <td>2.526734</td>\n",
          "      <td>2.526734</td>\n",
          "      <td>2.526734</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>12</th>\n",
          "      <td>Input-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8</td>\n",
          "      <td>350</td>\n",
          "      <td>150</td>\n",
          "      <td>2821.0</td>\n",
          "      <td>921549.0</td>\n",
          "      <td>18.807</td>\n",
          "      <td>6143.66</td>\n",
          "      <td>3.412185</td>\n",
          "      <td>0.812555</td>\n",
          "      <td>3.016946</td>\n",
          "      <td>1.302004</td>\n",
          "      <td>3.346455</td>\n",
          "      <td>3.346455</td>\n",
          "      <td>3.346455</td>\n",
          "      <td>3.346455</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>13</th>\n",
          "      <td>Llama2 Params</td>\n",
          "      <td>llama2</td>\n",
          "      <td>7</td>\n",
          "      <td>1</td>\n",
          "      <td>250</td>\n",
          "      <td>9404.0</td>\n",
          "      <td>42000.0</td>\n",
          "      <td>37.616</td>\n",
          "      <td>168.00</td>\n",
          "      <td>1.323725</td>\n",
          "      <td>0.346324</td>\n",
          "      <td>1.089420</td>\n",
          "      <td>0.554974</td>\n",
          "      <td>1.321819</td>\n",
          "      <td>1.321819</td>\n",
          "      <td>1.321819</td>\n",
          "      <td>1.321819</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>14</th>\n",
          "      <td>Llama2 Params</td>\n",
          "      <td>llama2</td>\n",
          "      <td>13</td>\n",
          "      <td>1</td>\n",
          "      <td>250</td>\n",
          "      <td>10252.0</td>\n",
          "      <td>42000.0</td>\n",
          "      <td>41.008</td>\n",
          "      <td>168.00</td>\n",
          "      <td>1.746948</td>\n",
          "      <td>0.446884</td>\n",
          "      <td>1.464196</td>\n",
          "      <td>0.716114</td>\n",
          "      <td>1.749054</td>\n",
          "      <td>1.749054</td>\n",
          "      <td>1.749054</td>\n",
          "      <td>1.749054</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>15</th>\n",
          "      <td>Llama2 Params</td>\n",
          "      <td>llama2</td>\n",
          "      <td>70</td>\n",
          "      <td>1</td>\n",
          "      <td>250</td>\n",
          "      <td>9962.0</td>\n",
          "      <td>36750.0</td>\n",
          "      <td>39.848</td>\n",
          "      <td>147.00</td>\n",
          "      <td>5.807986</td>\n",
          "      <td>1.388478</td>\n",
          "      <td>5.121170</td>\n",
          "      <td>2.224844</td>\n",
          "      <td>5.807785</td>\n",
          "      <td>5.807785</td>\n",
          "      <td>5.807785</td>\n",
          "      <td>5.807785</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>16</th>\n",
          "      <td>Llama3 Params</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8</td>\n",
          "      <td>1</td>\n",
          "      <td>250</td>\n",
          "      <td>4524.0</td>\n",
          "      <td>33165.0</td>\n",
          "      <td>18.096</td>\n",
          "      <td>132.66</td>\n",
          "      <td>1.349683</td>\n",
          "      <td>0.349200</td>\n",
          "      <td>1.120994</td>\n",
          "      <td>0.559562</td>\n",
          "      <td>1.349683</td>\n",
          "      <td>1.349683</td>\n",
          "      <td>1.349683</td>\n",
          "      <td>1.349683</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>17</th>\n",
          "      <td>Llama3 Params</td>\n",
          "      <td>llama3</td>\n",
          "      <td>70</td>\n",
          "      <td>1</td>\n",
          "      <td>100</td>\n",
          "      <td>1879.0</td>\n",
          "      <td>13266.0</td>\n",
          "      <td>18.790</td>\n",
          "      <td>132.66</td>\n",
          "      <td>5.327022</td>\n",
          "      <td>1.263045</td>\n",
          "      <td>4.724248</td>\n",
          "      <td>2.023890</td>\n",
          "      <td>5.327022</td>\n",
          "      <td>5.327022</td>\n",
          "      <td>5.327022</td>\n",
          "      <td>5.327022</td>\n",
          "    </tr>\n",
          "  </tbody>\n",
          "</table>\n",
          "</div>"
         ],
         "text/plain": [
          "        test_type model_type  parameters  num_examples  num_prompts  \\\n",
          "0      Output-tok     llama3           8             1          150   \n",
          "1      Output-tok     llama3           8             5          150   \n",
          "2      Output-tok     llama3           8            10          150   \n",
          "3      Output-tok     llama3           8            30           50   \n",
          "4      Output-tok     llama3           8            90           50   \n",
          "5       Input-tok     llama3           8             1          150   \n",
          "6       Input-tok     llama3           8             5          150   \n",
          "7       Input-tok     llama3           8            10          150   \n",
          "8       Input-tok     llama3           8            20          150   \n",
          "9       Input-tok     llama3           8            30          150   \n",
          "10      Input-tok     llama3           8            70          150   \n",
          "11      Input-tok     llama3           8           210          150   \n",
          "12      Input-tok     llama3           8           350          150   \n",
          "13  Llama2 Params     llama2           7             1          250   \n",
          "14  Llama2 Params     llama2          13             1          250   \n",
          "15  Llama2 Params     llama2          70             1          250   \n",
          "16  Llama3 Params     llama3           8             1          250   \n",
          "17  Llama3 Params     llama3          70             1          100   \n",
          "\n",
          "    total_out_tok  total_in_tok  avg_out_tok  avg_in_tok  \\\n",
          "0          2704.0       27999.0       18.027      186.66   \n",
          "1         10958.0       27999.0       73.053      186.66   \n",
          "2         26537.0       27999.0      176.913      186.66   \n",
          "3         29389.0        9333.0      587.780      186.66   \n",
          "4        106229.0        9333.0     2124.580      186.66   \n",
          "5          2701.0       23799.0       18.007      158.66   \n",
          "6          2706.0       33399.0       18.040      222.66   \n",
          "7          2691.0       45249.0       17.940      301.66   \n",
          "8          2707.0       69399.0       18.047      462.66   \n",
          "9          2705.0       94149.0       18.033      627.66   \n",
          "10         2736.0      201549.0       18.240     1343.66   \n",
          "11         2777.0      561549.0       18.513     3743.66   \n",
          "12         2821.0      921549.0       18.807     6143.66   \n",
          "13         9404.0       42000.0       37.616      168.00   \n",
          "14        10252.0       42000.0       41.008      168.00   \n",
          "15         9962.0       36750.0       39.848      147.00   \n",
          "16         4524.0       33165.0       18.096      132.66   \n",
          "17         1879.0       13266.0       18.790      132.66   \n",
          "\n",
          "    actual_emissions_per_10k_prompts  actual_cpu_energy_per_10k_prompts  \\\n",
          "0                           0.717595                           0.341199   \n",
          "1                           2.932066                           1.381080   \n",
          "2                           7.227774                           3.410975   \n",
          "3                          24.724678                          11.684302   \n",
          "4                          93.192518                          43.359625   \n",
          "5                           1.333196                           0.349343   \n",
          "6                           1.355123                           0.349796   \n",
          "7                           1.367448                           0.349663   \n",
          "8                           1.410753                           0.360203   \n",
          "9                           1.459989                           0.368341   \n",
          "10                          1.654524                           0.411301   \n",
          "11                          2.433074                           0.589927   \n",
          "12                          3.412185                           0.812555   \n",
          "13                          1.323725                           0.346324   \n",
          "14                          1.746948                           0.446884   \n",
          "15                          5.807986                           1.388478   \n",
          "16                          1.349683                           0.349200   \n",
          "17                          5.327022                           1.263045   \n",
          "\n",
          "    actual_gpu_energy_per_10k_prompts  actual_ram_energy_per_10k_prompts  \\\n",
          "0                            0.373301                           0.364674   \n",
          "1                            1.552277                           1.476108   \n",
          "2                            3.813065                           3.645638   \n",
          "3                           13.010428                          12.488129   \n",
          "4                           50.447799                          46.342602   \n",
          "5                            1.095836                           0.559783   \n",
          "6                            1.127602                           0.560540   \n",
          "7                            1.146506                           0.560304   \n",
          "8                            1.184168                           0.577226   \n",
          "9                            1.237068                           0.590234   \n",
          "10                           1.417782                           0.659116   \n",
          "11                           2.123813                           0.945303   \n",
          "12                           3.016946                           1.302004   \n",
          "13                           1.089420                           0.554974   \n",
          "14                           1.464196                           0.716114   \n",
          "15                           5.121170                           2.224844   \n",
          "16                           1.120994                           0.559562   \n",
          "17                           4.724248                           2.023890   \n",
          "\n",
          "    pred_emissions_per_10k_prompts  pred_cpu_energy_per_10k_prompts  \\\n",
          "0                         0.326447                         0.326447   \n",
          "1                         2.747454                         2.747454   \n",
          "2                         7.317035                         7.317035   \n",
          "3                        25.394160                        25.394160   \n",
          "4                        93.009535                        93.009535   \n",
          "5                         1.302275                         1.302275   \n",
          "6                         1.324134                         1.324134   \n",
          "7                         1.351116                         1.351116   \n",
          "8                         1.406106                         1.406106   \n",
          "9                         1.462462                         1.462462   \n",
          "10                        1.707012                         1.707012   \n",
          "11                        2.526734                         2.526734   \n",
          "12                        3.346455                         3.346455   \n",
          "13                        1.321819                         1.321819   \n",
          "14                        1.749054                         1.749054   \n",
          "15                        5.807785                         5.807785   \n",
          "16                        1.349683                         1.349683   \n",
          "17                        5.327022                         5.327022   \n",
          "\n",
          "    pred_gpu_energy_per_10k_prompts  pred_ram_energy_per_10k_prompts  \n",
          "0                          0.326447                         0.326447  \n",
          "1                          2.747454                         2.747454  \n",
          "2                          7.317035                         7.317035  \n",
          "3                         25.394160                        25.394160  \n",
          "4                         93.009535                        93.009535  \n",
          "5                          1.302275                         1.302275  \n",
          "6                          1.324134                         1.324134  \n",
          "7                          1.351116                         1.351116  \n",
          "8                          1.406106                         1.406106  \n",
          "9                          1.462462                         1.462462  \n",
          "10                         1.707012                         1.707012  \n",
          "11                         2.526734                         2.526734  \n",
          "12                         3.346455                         3.346455  \n",
          "13                         1.321819                         1.321819  \n",
          "14                         1.749054                         1.749054  \n",
          "15                         5.807785                         5.807785  \n",
          "16                         1.349683                         1.349683  \n",
          "17                         5.327022                         5.327022  "
         ]
        },
        "execution_count": 27,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "# Create the dataframe\n",
       "df = pd.DataFrame({\n",
       "    'test_type': test_type_column,\n",
       "    'model_type': model_type_column,\n",
       "    'parameters': parameters,\n",
       "    'num_examples': num_examples,\n",
       "    'num_prompts': num_prompts,\n",
       "    'total_out_tok': total_out_tok,\n",
       "    'total_in_tok': total_in_tok,\n",
       "    'avg_out_tok': avg_out_tok,\n",
       "    'avg_in_tok': avg_in_tok,\n",
       "    'actual_emissions_per_10k_prompts': actual_emissions_per_10k_prompts,\n",
       "    'actual_cpu_energy_per_10k_prompts': actual_cpu_energy_per_10k_prompts,\n",
       "    'actual_gpu_energy_per_10k_prompts': actual_gpu_energy_per_10k_prompts,\n",
       "    'actual_ram_energy_per_10k_prompts': actual_ram_energy_per_10k_prompts,\n",
       "    'pred_emissions_per_10k_prompts': pred_emissions_per_10k_prompts,\n",
       "    'pred_cpu_energy_per_10k_prompts': pred_cpu_energy_per_10k_prompts,\n",
       "    'pred_gpu_energy_per_10k_prompts': pred_gpu_energy_per_10k_prompts,\n",
       "    'pred_ram_energy_per_10k_prompts': pred_ram_energy_per_10k_prompts\n",
       "})\n",
       "\n",
       "df"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/html": [
          "<div>\n",
          "<style scoped>\n",
          "    .dataframe tbody tr th:only-of-type {\n",
          "        vertical-align: middle;\n",
          "    }\n",
          "\n",
          "    .dataframe tbody tr th {\n",
          "        vertical-align: top;\n",
          "    }\n",
          "\n",
          "    .dataframe thead th {\n",
          "        text-align: right;\n",
          "    }\n",
          "</style>\n",
          "<table border=\"1\" class=\"dataframe\">\n",
          "  <thead>\n",
          "    <tr style=\"text-align: right;\">\n",
          "      <th></th>\n",
          "      <th>Name</th>\n",
          "      <th>State</th>\n",
          "      <th>Notes</th>\n",
          "      <th>User</th>\n",
          "      <th>Tags</th>\n",
          "      <th>Created</th>\n",
          "      <th>Runtime</th>\n",
          "      <th>Sweep</th>\n",
          "      <th>framework</th>\n",
          "      <th>model</th>\n",
          "      <th>...</th>\n",
          "      <th>total_time</th>\n",
          "      <th>AVG. Input Tokens</th>\n",
          "      <th>AVG. Output Tokens</th>\n",
          "      <th>AVG. Time / Prompt</th>\n",
          "      <th>AVG. Tokens / Second</th>\n",
          "      <th>Emissions / 1.000.000 Input Tokens</th>\n",
          "      <th>Emissions / 1.000.000 Output Tokens</th>\n",
          "      <th>Emissions / 10.000 Prompts</th>\n",
          "      <th>Total Emissions</th>\n",
          "      <th>Total Time</th>\n",
          "    </tr>\n",
          "  </thead>\n",
          "  <tbody>\n",
          "    <tr>\n",
          "      <th>0</th>\n",
          "      <td>vLLM_Inference_7500_prompts</td>\n",
          "      <td>finished</td>\n",
          "      <td>-</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>2024-06-07T18:20:15.000Z</td>\n",
          "      <td>168</td>\n",
          "      <td>NaN</td>\n",
          "      <td>vLLM</td>\n",
          "      <td>bigscience/bloomz-560m</td>\n",
          "      <td>...</td>\n",
          "      <td>NaN</td>\n",
          "      <td>172.22</td>\n",
          "      <td>30.482133</td>\n",
          "      <td>20.897599</td>\n",
          "      <td>1458.642858</td>\n",
          "      <td>0.007615</td>\n",
          "      <td>0.043024</td>\n",
          "      <td>0.013115</td>\n",
          "      <td>0.009836</td>\n",
          "      <td>156.731992</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>1</th>\n",
          "      <td>vLLM_Inference_7500_prompts</td>\n",
          "      <td>finished</td>\n",
          "      <td>-</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>2024-06-07T18:14:32.000Z</td>\n",
          "      <td>217</td>\n",
          "      <td>NaN</td>\n",
          "      <td>vLLM</td>\n",
          "      <td>bigscience/bloomz-1b1</td>\n",
          "      <td>...</td>\n",
          "      <td>NaN</td>\n",
          "      <td>172.22</td>\n",
          "      <td>35.405600</td>\n",
          "      <td>27.639732</td>\n",
          "      <td>1280.967557</td>\n",
          "      <td>0.010474</td>\n",
          "      <td>0.050949</td>\n",
          "      <td>0.018039</td>\n",
          "      <td>0.013529</td>\n",
          "      <td>207.297990</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>2</th>\n",
          "      <td>vLLM_Inference_7500_prompts</td>\n",
          "      <td>finished</td>\n",
          "      <td>-</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>2024-06-07T18:08:45.000Z</td>\n",
          "      <td>238</td>\n",
          "      <td>NaN</td>\n",
          "      <td>vLLM</td>\n",
          "      <td>bigscience/bloomz-1b7</td>\n",
          "      <td>...</td>\n",
          "      <td>NaN</td>\n",
          "      <td>172.22</td>\n",
          "      <td>32.533733</td>\n",
          "      <td>30.266593</td>\n",
          "      <td>1074.905689</td>\n",
          "      <td>0.011770</td>\n",
          "      <td>0.062306</td>\n",
          "      <td>0.020270</td>\n",
          "      <td>0.015203</td>\n",
          "      <td>226.999450</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>3</th>\n",
          "      <td>vLLM_Inference_7500_prompts</td>\n",
          "      <td>finished</td>\n",
          "      <td>-</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>2024-06-07T18:01:25.000Z</td>\n",
          "      <td>291</td>\n",
          "      <td>NaN</td>\n",
          "      <td>vLLM</td>\n",
          "      <td>bigscience/bloomz-3b</td>\n",
          "      <td>...</td>\n",
          "      <td>NaN</td>\n",
          "      <td>172.22</td>\n",
          "      <td>26.118533</td>\n",
          "      <td>37.542294</td>\n",
          "      <td>695.709568</td>\n",
          "      <td>0.015074</td>\n",
          "      <td>0.099394</td>\n",
          "      <td>0.025960</td>\n",
          "      <td>0.019470</td>\n",
          "      <td>281.567207</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>4</th>\n",
          "      <td>vLLM_Inference_7500_prompts</td>\n",
          "      <td>finished</td>\n",
          "      <td>-</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>2024-06-07T17:48:04.000Z</td>\n",
          "      <td>467</td>\n",
          "      <td>NaN</td>\n",
          "      <td>vLLM</td>\n",
          "      <td>bigscience/bloomz-7b1</td>\n",
          "      <td>...</td>\n",
          "      <td>NaN</td>\n",
          "      <td>172.22</td>\n",
          "      <td>34.083333</td>\n",
          "      <td>60.393297</td>\n",
          "      <td>564.356227</td>\n",
          "      <td>0.026370</td>\n",
          "      <td>0.133245</td>\n",
          "      <td>0.045414</td>\n",
          "      <td>0.034061</td>\n",
          "      <td>452.949728</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>5</th>\n",
          "      <td>vLLM_Inference_5000_prompts</td>\n",
          "      <td>finished</td>\n",
          "      <td>-</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>2024-06-07T15:36:41.000Z</td>\n",
          "      <td>253</td>\n",
          "      <td>NaN</td>\n",
          "      <td>vLLM</td>\n",
          "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
          "      <td>...</td>\n",
          "      <td>NaN</td>\n",
          "      <td>186.66</td>\n",
          "      <td>18.200400</td>\n",
          "      <td>48.790199</td>\n",
          "      <td>373.033938</td>\n",
          "      <td>0.018982</td>\n",
          "      <td>0.194673</td>\n",
          "      <td>0.035431</td>\n",
          "      <td>0.017716</td>\n",
          "      <td>243.950994</td>\n",
          "    </tr>\n",
          "  </tbody>\n",
          "</table>\n",
          "<p>6 rows × 23 columns</p>\n",
          "</div>"
         ],
         "text/plain": [
          "                          Name     State Notes  User  Tags  \\\n",
          "0  vLLM_Inference_7500_prompts  finished     -   NaN   NaN   \n",
          "1  vLLM_Inference_7500_prompts  finished     -   NaN   NaN   \n",
          "2  vLLM_Inference_7500_prompts  finished     -   NaN   NaN   \n",
          "3  vLLM_Inference_7500_prompts  finished     -   NaN   NaN   \n",
          "4  vLLM_Inference_7500_prompts  finished     -   NaN   NaN   \n",
          "5  vLLM_Inference_5000_prompts  finished     -   NaN   NaN   \n",
          "\n",
          "                    Created  Runtime  Sweep framework  \\\n",
          "0  2024-06-07T18:20:15.000Z      168    NaN      vLLM   \n",
          "1  2024-06-07T18:14:32.000Z      217    NaN      vLLM   \n",
          "2  2024-06-07T18:08:45.000Z      238    NaN      vLLM   \n",
          "3  2024-06-07T18:01:25.000Z      291    NaN      vLLM   \n",
          "4  2024-06-07T17:48:04.000Z      467    NaN      vLLM   \n",
          "5  2024-06-07T15:36:41.000Z      253    NaN      vLLM   \n",
          "\n",
          "                                 model  ...  total_time  AVG. Input Tokens  \\\n",
          "0               bigscience/bloomz-560m  ...         NaN             172.22   \n",
          "1                bigscience/bloomz-1b1  ...         NaN             172.22   \n",
          "2                bigscience/bloomz-1b7  ...         NaN             172.22   \n",
          "3                 bigscience/bloomz-3b  ...         NaN             172.22   \n",
          "4                bigscience/bloomz-7b1  ...         NaN             172.22   \n",
          "5  meta-llama/Meta-Llama-3-8B-Instruct  ...         NaN             186.66   \n",
          "\n",
          "   AVG. Output Tokens  AVG. Time / Prompt  AVG. Tokens / Second  \\\n",
          "0           30.482133           20.897599           1458.642858   \n",
          "1           35.405600           27.639732           1280.967557   \n",
          "2           32.533733           30.266593           1074.905689   \n",
          "3           26.118533           37.542294            695.709568   \n",
          "4           34.083333           60.393297            564.356227   \n",
          "5           18.200400           48.790199            373.033938   \n",
          "\n",
          "   Emissions / 1.000.000 Input Tokens  Emissions / 1.000.000 Output Tokens  \\\n",
          "0                            0.007615                             0.043024   \n",
          "1                            0.010474                             0.050949   \n",
          "2                            0.011770                             0.062306   \n",
          "3                            0.015074                             0.099394   \n",
          "4                            0.026370                             0.133245   \n",
          "5                            0.018982                             0.194673   \n",
          "\n",
          "   Emissions / 10.000 Prompts  Total Emissions  Total Time  \n",
          "0                    0.013115         0.009836  156.731992  \n",
          "1                    0.018039         0.013529  207.297990  \n",
          "2                    0.020270         0.015203  226.999450  \n",
          "3                    0.025960         0.019470  281.567207  \n",
          "4                    0.045414         0.034061  452.949728  \n",
          "5                    0.035431         0.017716  243.950994  \n",
          "\n",
          "[6 rows x 23 columns]"
         ]
        },
        "execution_count": 29,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "framework_comp"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Mapping columns from framework_comp to df\n",
       "mapped_data = {\n",
       "    'test_type': ['framework_comp_vllm'] * len(framework_comp),\n",
       "    'model_type': ['vLLM'] * len(framework_comp),\n",
       "    'parameters': [.560, 1.100, 1.700, 3.000, 7.100, 8.000],\n",
       "    'num_examples': [np.nan] * len(framework_comp),\n",
       "    'num_prompts': framework_comp['total_prompts'],\n",
       "    'total_out_tok': [np.nan] * len(framework_comp),\n",
       "    'total_in_tok': [np.nan] * len(framework_comp),\n",
       "    'avg_out_tok': framework_comp['AVG. Output Tokens'],\n",
       "    'avg_in_tok': framework_comp['AVG. Input Tokens'],\n",
       "    'actual_emissions_per_10k_prompts': framework_comp['Emissions / 10.000 Prompts'],\n",
       "    'actual_emissions_per_1M_out_tok': framework_comp['Emissions / 1.000.000 Output Tokens'],\n",
       "    'actual_cpu_energy_per_10k_prompts': [np.nan] * len(framework_comp),\n",
       "    'actual_gpu_energy_per_10k_prompts': [np.nan] * len(framework_comp),\n",
       "    'actual_ram_energy_per_10k_prompts': [np.nan] * len(framework_comp),\n",
       "    'pred_emissions_per_10k_prompts': [np.nan] * len(framework_comp),\n",
       "    'pred_cpu_energy_per_10k_prompts': [np.nan] * len(framework_comp),\n",
       "    'pred_gpu_energy_per_10k_prompts': [np.nan] * len(framework_comp),\n",
       "    'pred_ram_energy_per_10k_prompts': [np.nan] * len(framework_comp)\n",
       "}\n",
       "\n",
       "# Add the model names\n",
       "mapped_data['model_type'] = framework_comp['model'].apply(lambda x: x.split('/')[-1])\n",
       "\n",
       "# Create the mapped dataframe\n",
       "mapped_df = pd.DataFrame(mapped_data)\n",
       "\n",
       "# Append to the original dataframe\n",
       "result_df = pd.concat([df, mapped_df], ignore_index=True)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
       {
        "data": {
         "text/html": [
          "<div>\n",
          "<style scoped>\n",
          "    .dataframe tbody tr th:only-of-type {\n",
          "        vertical-align: middle;\n",
          "    }\n",
          "\n",
          "    .dataframe tbody tr th {\n",
          "        vertical-align: top;\n",
          "    }\n",
          "\n",
          "    .dataframe thead th {\n",
          "        text-align: right;\n",
          "    }\n",
          "</style>\n",
          "<table border=\"1\" class=\"dataframe\">\n",
          "  <thead>\n",
          "    <tr style=\"text-align: right;\">\n",
          "      <th></th>\n",
          "      <th>test_type</th>\n",
          "      <th>model_type</th>\n",
          "      <th>parameters</th>\n",
          "      <th>num_examples</th>\n",
          "      <th>num_prompts</th>\n",
          "      <th>total_out_tok</th>\n",
          "      <th>total_in_tok</th>\n",
          "      <th>avg_out_tok</th>\n",
          "      <th>avg_in_tok</th>\n",
          "      <th>actual_emissions_per_10k_prompts</th>\n",
          "      <th>actual_cpu_energy_per_10k_prompts</th>\n",
          "      <th>actual_gpu_energy_per_10k_prompts</th>\n",
          "      <th>actual_ram_energy_per_10k_prompts</th>\n",
          "      <th>pred_emissions_per_10k_prompts</th>\n",
          "      <th>pred_cpu_energy_per_10k_prompts</th>\n",
          "      <th>pred_gpu_energy_per_10k_prompts</th>\n",
          "      <th>pred_ram_energy_per_10k_prompts</th>\n",
          "      <th>actual_emissions_per_1M_out_tok</th>\n",
          "    </tr>\n",
          "  </thead>\n",
          "  <tbody>\n",
          "    <tr>\n",
          "      <th>0</th>\n",
          "      <td>Output-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8.00</td>\n",
          "      <td>1.0</td>\n",
          "      <td>150</td>\n",
          "      <td>2704.0</td>\n",
          "      <td>27999.0</td>\n",
          "      <td>18.027000</td>\n",
          "      <td>186.66</td>\n",
          "      <td>0.717595</td>\n",
          "      <td>0.341199</td>\n",
          "      <td>0.373301</td>\n",
          "      <td>0.364674</td>\n",
          "      <td>0.326447</td>\n",
          "      <td>0.326447</td>\n",
          "      <td>0.326447</td>\n",
          "      <td>0.326447</td>\n",
          "      <td>NaN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>1</th>\n",
          "      <td>Output-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8.00</td>\n",
          "      <td>5.0</td>\n",
          "      <td>150</td>\n",
          "      <td>10958.0</td>\n",
          "      <td>27999.0</td>\n",
          "      <td>73.053000</td>\n",
          "      <td>186.66</td>\n",
          "      <td>2.932066</td>\n",
          "      <td>1.381080</td>\n",
          "      <td>1.552277</td>\n",
          "      <td>1.476108</td>\n",
          "      <td>2.747454</td>\n",
          "      <td>2.747454</td>\n",
          "      <td>2.747454</td>\n",
          "      <td>2.747454</td>\n",
          "      <td>NaN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>2</th>\n",
          "      <td>Output-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8.00</td>\n",
          "      <td>10.0</td>\n",
          "      <td>150</td>\n",
          "      <td>26537.0</td>\n",
          "      <td>27999.0</td>\n",
          "      <td>176.913000</td>\n",
          "      <td>186.66</td>\n",
          "      <td>7.227774</td>\n",
          "      <td>3.410975</td>\n",
          "      <td>3.813065</td>\n",
          "      <td>3.645638</td>\n",
          "      <td>7.317035</td>\n",
          "      <td>7.317035</td>\n",
          "      <td>7.317035</td>\n",
          "      <td>7.317035</td>\n",
          "      <td>NaN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>3</th>\n",
          "      <td>Output-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8.00</td>\n",
          "      <td>30.0</td>\n",
          "      <td>50</td>\n",
          "      <td>29389.0</td>\n",
          "      <td>9333.0</td>\n",
          "      <td>587.780000</td>\n",
          "      <td>186.66</td>\n",
          "      <td>24.724678</td>\n",
          "      <td>11.684302</td>\n",
          "      <td>13.010428</td>\n",
          "      <td>12.488129</td>\n",
          "      <td>25.394160</td>\n",
          "      <td>25.394160</td>\n",
          "      <td>25.394160</td>\n",
          "      <td>25.394160</td>\n",
          "      <td>NaN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>4</th>\n",
          "      <td>Output-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8.00</td>\n",
          "      <td>90.0</td>\n",
          "      <td>50</td>\n",
          "      <td>106229.0</td>\n",
          "      <td>9333.0</td>\n",
          "      <td>2124.580000</td>\n",
          "      <td>186.66</td>\n",
          "      <td>93.192518</td>\n",
          "      <td>43.359625</td>\n",
          "      <td>50.447799</td>\n",
          "      <td>46.342602</td>\n",
          "      <td>93.009535</td>\n",
          "      <td>93.009535</td>\n",
          "      <td>93.009535</td>\n",
          "      <td>93.009535</td>\n",
          "      <td>NaN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>5</th>\n",
          "      <td>Input-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8.00</td>\n",
          "      <td>1.0</td>\n",
          "      <td>150</td>\n",
          "      <td>2701.0</td>\n",
          "      <td>23799.0</td>\n",
          "      <td>18.007000</td>\n",
          "      <td>158.66</td>\n",
          "      <td>1.333196</td>\n",
          "      <td>0.349343</td>\n",
          "      <td>1.095836</td>\n",
          "      <td>0.559783</td>\n",
          "      <td>1.302275</td>\n",
          "      <td>1.302275</td>\n",
          "      <td>1.302275</td>\n",
          "      <td>1.302275</td>\n",
          "      <td>NaN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>6</th>\n",
          "      <td>Input-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8.00</td>\n",
          "      <td>5.0</td>\n",
          "      <td>150</td>\n",
          "      <td>2706.0</td>\n",
          "      <td>33399.0</td>\n",
          "      <td>18.040000</td>\n",
          "      <td>222.66</td>\n",
          "      <td>1.355123</td>\n",
          "      <td>0.349796</td>\n",
          "      <td>1.127602</td>\n",
          "      <td>0.560540</td>\n",
          "      <td>1.324134</td>\n",
          "      <td>1.324134</td>\n",
          "      <td>1.324134</td>\n",
          "      <td>1.324134</td>\n",
          "      <td>NaN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>7</th>\n",
          "      <td>Input-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8.00</td>\n",
          "      <td>10.0</td>\n",
          "      <td>150</td>\n",
          "      <td>2691.0</td>\n",
          "      <td>45249.0</td>\n",
          "      <td>17.940000</td>\n",
          "      <td>301.66</td>\n",
          "      <td>1.367448</td>\n",
          "      <td>0.349663</td>\n",
          "      <td>1.146506</td>\n",
          "      <td>0.560304</td>\n",
          "      <td>1.351116</td>\n",
          "      <td>1.351116</td>\n",
          "      <td>1.351116</td>\n",
          "      <td>1.351116</td>\n",
          "      <td>NaN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>8</th>\n",
          "      <td>Input-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8.00</td>\n",
          "      <td>20.0</td>\n",
          "      <td>150</td>\n",
          "      <td>2707.0</td>\n",
          "      <td>69399.0</td>\n",
          "      <td>18.047000</td>\n",
          "      <td>462.66</td>\n",
          "      <td>1.410753</td>\n",
          "      <td>0.360203</td>\n",
          "      <td>1.184168</td>\n",
          "      <td>0.577226</td>\n",
          "      <td>1.406106</td>\n",
          "      <td>1.406106</td>\n",
          "      <td>1.406106</td>\n",
          "      <td>1.406106</td>\n",
          "      <td>NaN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>9</th>\n",
          "      <td>Input-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8.00</td>\n",
          "      <td>30.0</td>\n",
          "      <td>150</td>\n",
          "      <td>2705.0</td>\n",
          "      <td>94149.0</td>\n",
          "      <td>18.033000</td>\n",
          "      <td>627.66</td>\n",
          "      <td>1.459989</td>\n",
          "      <td>0.368341</td>\n",
          "      <td>1.237068</td>\n",
          "      <td>0.590234</td>\n",
          "      <td>1.462462</td>\n",
          "      <td>1.462462</td>\n",
          "      <td>1.462462</td>\n",
          "      <td>1.462462</td>\n",
          "      <td>NaN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>10</th>\n",
          "      <td>Input-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8.00</td>\n",
          "      <td>70.0</td>\n",
          "      <td>150</td>\n",
          "      <td>2736.0</td>\n",
          "      <td>201549.0</td>\n",
          "      <td>18.240000</td>\n",
          "      <td>1343.66</td>\n",
          "      <td>1.654524</td>\n",
          "      <td>0.411301</td>\n",
          "      <td>1.417782</td>\n",
          "      <td>0.659116</td>\n",
          "      <td>1.707012</td>\n",
          "      <td>1.707012</td>\n",
          "      <td>1.707012</td>\n",
          "      <td>1.707012</td>\n",
          "      <td>NaN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>11</th>\n",
          "      <td>Input-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8.00</td>\n",
          "      <td>210.0</td>\n",
          "      <td>150</td>\n",
          "      <td>2777.0</td>\n",
          "      <td>561549.0</td>\n",
          "      <td>18.513000</td>\n",
          "      <td>3743.66</td>\n",
          "      <td>2.433074</td>\n",
          "      <td>0.589927</td>\n",
          "      <td>2.123813</td>\n",
          "      <td>0.945303</td>\n",
          "      <td>2.526734</td>\n",
          "      <td>2.526734</td>\n",
          "      <td>2.526734</td>\n",
          "      <td>2.526734</td>\n",
          "      <td>NaN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>12</th>\n",
          "      <td>Input-tok</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8.00</td>\n",
          "      <td>350.0</td>\n",
          "      <td>150</td>\n",
          "      <td>2821.0</td>\n",
          "      <td>921549.0</td>\n",
          "      <td>18.807000</td>\n",
          "      <td>6143.66</td>\n",
          "      <td>3.412185</td>\n",
          "      <td>0.812555</td>\n",
          "      <td>3.016946</td>\n",
          "      <td>1.302004</td>\n",
          "      <td>3.346455</td>\n",
          "      <td>3.346455</td>\n",
          "      <td>3.346455</td>\n",
          "      <td>3.346455</td>\n",
          "      <td>NaN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>13</th>\n",
          "      <td>Llama2 Params</td>\n",
          "      <td>llama2</td>\n",
          "      <td>7.00</td>\n",
          "      <td>1.0</td>\n",
          "      <td>250</td>\n",
          "      <td>9404.0</td>\n",
          "      <td>42000.0</td>\n",
          "      <td>37.616000</td>\n",
          "      <td>168.00</td>\n",
          "      <td>1.323725</td>\n",
          "      <td>0.346324</td>\n",
          "      <td>1.089420</td>\n",
          "      <td>0.554974</td>\n",
          "      <td>1.321819</td>\n",
          "      <td>1.321819</td>\n",
          "      <td>1.321819</td>\n",
          "      <td>1.321819</td>\n",
          "      <td>NaN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>14</th>\n",
          "      <td>Llama2 Params</td>\n",
          "      <td>llama2</td>\n",
          "      <td>13.00</td>\n",
          "      <td>1.0</td>\n",
          "      <td>250</td>\n",
          "      <td>10252.0</td>\n",
          "      <td>42000.0</td>\n",
          "      <td>41.008000</td>\n",
          "      <td>168.00</td>\n",
          "      <td>1.746948</td>\n",
          "      <td>0.446884</td>\n",
          "      <td>1.464196</td>\n",
          "      <td>0.716114</td>\n",
          "      <td>1.749054</td>\n",
          "      <td>1.749054</td>\n",
          "      <td>1.749054</td>\n",
          "      <td>1.749054</td>\n",
          "      <td>NaN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>15</th>\n",
          "      <td>Llama2 Params</td>\n",
          "      <td>llama2</td>\n",
          "      <td>70.00</td>\n",
          "      <td>1.0</td>\n",
          "      <td>250</td>\n",
          "      <td>9962.0</td>\n",
          "      <td>36750.0</td>\n",
          "      <td>39.848000</td>\n",
          "      <td>147.00</td>\n",
          "      <td>5.807986</td>\n",
          "      <td>1.388478</td>\n",
          "      <td>5.121170</td>\n",
          "      <td>2.224844</td>\n",
          "      <td>5.807785</td>\n",
          "      <td>5.807785</td>\n",
          "      <td>5.807785</td>\n",
          "      <td>5.807785</td>\n",
          "      <td>NaN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>16</th>\n",
          "      <td>Llama3 Params</td>\n",
          "      <td>llama3</td>\n",
          "      <td>8.00</td>\n",
          "      <td>1.0</td>\n",
          "      <td>250</td>\n",
          "      <td>4524.0</td>\n",
          "      <td>33165.0</td>\n",
          "      <td>18.096000</td>\n",
          "      <td>132.66</td>\n",
          "      <td>1.349683</td>\n",
          "      <td>0.349200</td>\n",
          "      <td>1.120994</td>\n",
          "      <td>0.559562</td>\n",
          "      <td>1.349683</td>\n",
          "      <td>1.349683</td>\n",
          "      <td>1.349683</td>\n",
          "      <td>1.349683</td>\n",
          "      <td>NaN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>17</th>\n",
          "      <td>Llama3 Params</td>\n",
          "      <td>llama3</td>\n",
          "      <td>70.00</td>\n",
          "      <td>1.0</td>\n",
          "      <td>100</td>\n",
          "      <td>1879.0</td>\n",
          "      <td>13266.0</td>\n",
          "      <td>18.790000</td>\n",
          "      <td>132.66</td>\n",
          "      <td>5.327022</td>\n",
          "      <td>1.263045</td>\n",
          "      <td>4.724248</td>\n",
          "      <td>2.023890</td>\n",
          "      <td>5.327022</td>\n",
          "      <td>5.327022</td>\n",
          "      <td>5.327022</td>\n",
          "      <td>5.327022</td>\n",
          "      <td>NaN</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>18</th>\n",
          "      <td>framework_comp_vllm</td>\n",
          "      <td>bloomz-560m</td>\n",
          "      <td>0.56</td>\n",
          "      <td>NaN</td>\n",
          "      <td>7500</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>30.482133</td>\n",
          "      <td>172.22</td>\n",
          "      <td>0.013115</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>0.043024</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>19</th>\n",
          "      <td>framework_comp_vllm</td>\n",
          "      <td>bloomz-1b1</td>\n",
          "      <td>1.10</td>\n",
          "      <td>NaN</td>\n",
          "      <td>7500</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>35.405600</td>\n",
          "      <td>172.22</td>\n",
          "      <td>0.018039</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>0.050949</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>20</th>\n",
          "      <td>framework_comp_vllm</td>\n",
          "      <td>bloomz-1b7</td>\n",
          "      <td>1.70</td>\n",
          "      <td>NaN</td>\n",
          "      <td>7500</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>32.533733</td>\n",
          "      <td>172.22</td>\n",
          "      <td>0.020270</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>0.062306</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>21</th>\n",
          "      <td>framework_comp_vllm</td>\n",
          "      <td>bloomz-3b</td>\n",
          "      <td>3.00</td>\n",
          "      <td>NaN</td>\n",
          "      <td>7500</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>26.118533</td>\n",
          "      <td>172.22</td>\n",
          "      <td>0.025960</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>0.099394</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>22</th>\n",
          "      <td>framework_comp_vllm</td>\n",
          "      <td>bloomz-7b1</td>\n",
          "      <td>7.10</td>\n",
          "      <td>NaN</td>\n",
          "      <td>7500</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>34.083333</td>\n",
          "      <td>172.22</td>\n",
          "      <td>0.045414</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>0.133245</td>\n",
          "    </tr>\n",
          "    <tr>\n",
          "      <th>23</th>\n",
          "      <td>framework_comp_vllm</td>\n",
          "      <td>Meta-Llama-3-8B-Instruct</td>\n",
          "      <td>8.00</td>\n",
          "      <td>NaN</td>\n",
          "      <td>5000</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>18.200400</td>\n",
          "      <td>186.66</td>\n",
          "      <td>0.035431</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>NaN</td>\n",
          "      <td>0.194673</td>\n",
          "    </tr>\n",
          "  </tbody>\n",
          "</table>\n",
          "</div>"
         ],
         "text/plain": [
          "              test_type                model_type  parameters  num_examples  \\\n",
          "0            Output-tok                    llama3        8.00           1.0   \n",
          "1            Output-tok                    llama3        8.00           5.0   \n",
          "2            Output-tok                    llama3        8.00          10.0   \n",
          "3            Output-tok                    llama3        8.00          30.0   \n",
          "4            Output-tok                    llama3        8.00          90.0   \n",
          "5             Input-tok                    llama3        8.00           1.0   \n",
          "6             Input-tok                    llama3        8.00           5.0   \n",
          "7             Input-tok                    llama3        8.00          10.0   \n",
          "8             Input-tok                    llama3        8.00          20.0   \n",
          "9             Input-tok                    llama3        8.00          30.0   \n",
          "10            Input-tok                    llama3        8.00          70.0   \n",
          "11            Input-tok                    llama3        8.00         210.0   \n",
          "12            Input-tok                    llama3        8.00         350.0   \n",
          "13        Llama2 Params                    llama2        7.00           1.0   \n",
          "14        Llama2 Params                    llama2       13.00           1.0   \n",
          "15        Llama2 Params                    llama2       70.00           1.0   \n",
          "16        Llama3 Params                    llama3        8.00           1.0   \n",
          "17        Llama3 Params                    llama3       70.00           1.0   \n",
          "18  framework_comp_vllm               bloomz-560m        0.56           NaN   \n",
          "19  framework_comp_vllm                bloomz-1b1        1.10           NaN   \n",
          "20  framework_comp_vllm                bloomz-1b7        1.70           NaN   \n",
          "21  framework_comp_vllm                 bloomz-3b        3.00           NaN   \n",
          "22  framework_comp_vllm                bloomz-7b1        7.10           NaN   \n",
          "23  framework_comp_vllm  Meta-Llama-3-8B-Instruct        8.00           NaN   \n",
          "\n",
          "    num_prompts  total_out_tok  total_in_tok  avg_out_tok  avg_in_tok  \\\n",
          "0           150         2704.0       27999.0    18.027000      186.66   \n",
          "1           150        10958.0       27999.0    73.053000      186.66   \n",
          "2           150        26537.0       27999.0   176.913000      186.66   \n",
          "3            50        29389.0        9333.0   587.780000      186.66   \n",
          "4            50       106229.0        9333.0  2124.580000      186.66   \n",
          "5           150         2701.0       23799.0    18.007000      158.66   \n",
          "6           150         2706.0       33399.0    18.040000      222.66   \n",
          "7           150         2691.0       45249.0    17.940000      301.66   \n",
          "8           150         2707.0       69399.0    18.047000      462.66   \n",
          "9           150         2705.0       94149.0    18.033000      627.66   \n",
          "10          150         2736.0      201549.0    18.240000     1343.66   \n",
          "11          150         2777.0      561549.0    18.513000     3743.66   \n",
          "12          150         2821.0      921549.0    18.807000     6143.66   \n",
          "13          250         9404.0       42000.0    37.616000      168.00   \n",
          "14          250        10252.0       42000.0    41.008000      168.00   \n",
          "15          250         9962.0       36750.0    39.848000      147.00   \n",
          "16          250         4524.0       33165.0    18.096000      132.66   \n",
          "17          100         1879.0       13266.0    18.790000      132.66   \n",
          "18         7500            NaN           NaN    30.482133      172.22   \n",
          "19         7500            NaN           NaN    35.405600      172.22   \n",
          "20         7500            NaN           NaN    32.533733      172.22   \n",
          "21         7500            NaN           NaN    26.118533      172.22   \n",
          "22         7500            NaN           NaN    34.083333      172.22   \n",
          "23         5000            NaN           NaN    18.200400      186.66   \n",
          "\n",
          "    actual_emissions_per_10k_prompts  actual_cpu_energy_per_10k_prompts  \\\n",
          "0                           0.717595                           0.341199   \n",
          "1                           2.932066                           1.381080   \n",
          "2                           7.227774                           3.410975   \n",
          "3                          24.724678                          11.684302   \n",
          "4                          93.192518                          43.359625   \n",
          "5                           1.333196                           0.349343   \n",
          "6                           1.355123                           0.349796   \n",
          "7                           1.367448                           0.349663   \n",
          "8                           1.410753                           0.360203   \n",
          "9                           1.459989                           0.368341   \n",
          "10                          1.654524                           0.411301   \n",
          "11                          2.433074                           0.589927   \n",
          "12                          3.412185                           0.812555   \n",
          "13                          1.323725                           0.346324   \n",
          "14                          1.746948                           0.446884   \n",
          "15                          5.807986                           1.388478   \n",
          "16                          1.349683                           0.349200   \n",
          "17                          5.327022                           1.263045   \n",
          "18                          0.013115                                NaN   \n",
          "19                          0.018039                                NaN   \n",
          "20                          0.020270                                NaN   \n",
          "21                          0.025960                                NaN   \n",
          "22                          0.045414                                NaN   \n",
          "23                          0.035431                                NaN   \n",
          "\n",
          "    actual_gpu_energy_per_10k_prompts  actual_ram_energy_per_10k_prompts  \\\n",
          "0                            0.373301                           0.364674   \n",
          "1                            1.552277                           1.476108   \n",
          "2                            3.813065                           3.645638   \n",
          "3                           13.010428                          12.488129   \n",
          "4                           50.447799                          46.342602   \n",
          "5                            1.095836                           0.559783   \n",
          "6                            1.127602                           0.560540   \n",
          "7                            1.146506                           0.560304   \n",
          "8                            1.184168                           0.577226   \n",
          "9                            1.237068                           0.590234   \n",
          "10                           1.417782                           0.659116   \n",
          "11                           2.123813                           0.945303   \n",
          "12                           3.016946                           1.302004   \n",
          "13                           1.089420                           0.554974   \n",
          "14                           1.464196                           0.716114   \n",
          "15                           5.121170                           2.224844   \n",
          "16                           1.120994                           0.559562   \n",
          "17                           4.724248                           2.023890   \n",
          "18                                NaN                                NaN   \n",
          "19                                NaN                                NaN   \n",
          "20                                NaN                                NaN   \n",
          "21                                NaN                                NaN   \n",
          "22                                NaN                                NaN   \n",
          "23                                NaN                                NaN   \n",
          "\n",
          "    pred_emissions_per_10k_prompts  pred_cpu_energy_per_10k_prompts  \\\n",
          "0                         0.326447                         0.326447   \n",
          "1                         2.747454                         2.747454   \n",
          "2                         7.317035                         7.317035   \n",
          "3                        25.394160                        25.394160   \n",
          "4                        93.009535                        93.009535   \n",
          "5                         1.302275                         1.302275   \n",
          "6                         1.324134                         1.324134   \n",
          "7                         1.351116                         1.351116   \n",
          "8                         1.406106                         1.406106   \n",
          "9                         1.462462                         1.462462   \n",
          "10                        1.707012                         1.707012   \n",
          "11                        2.526734                         2.526734   \n",
          "12                        3.346455                         3.346455   \n",
          "13                        1.321819                         1.321819   \n",
          "14                        1.749054                         1.749054   \n",
          "15                        5.807785                         5.807785   \n",
          "16                        1.349683                         1.349683   \n",
          "17                        5.327022                         5.327022   \n",
          "18                             NaN                              NaN   \n",
          "19                             NaN                              NaN   \n",
          "20                             NaN                              NaN   \n",
          "21                             NaN                              NaN   \n",
          "22                             NaN                              NaN   \n",
          "23                             NaN                              NaN   \n",
          "\n",
          "    pred_gpu_energy_per_10k_prompts  pred_ram_energy_per_10k_prompts  \\\n",
          "0                          0.326447                         0.326447   \n",
          "1                          2.747454                         2.747454   \n",
          "2                          7.317035                         7.317035   \n",
          "3                         25.394160                        25.394160   \n",
          "4                         93.009535                        93.009535   \n",
          "5                          1.302275                         1.302275   \n",
          "6                          1.324134                         1.324134   \n",
          "7                          1.351116                         1.351116   \n",
          "8                          1.406106                         1.406106   \n",
          "9                          1.462462                         1.462462   \n",
          "10                         1.707012                         1.707012   \n",
          "11                         2.526734                         2.526734   \n",
          "12                         3.346455                         3.346455   \n",
          "13                         1.321819                         1.321819   \n",
          "14                         1.749054                         1.749054   \n",
          "15                         5.807785                         5.807785   \n",
          "16                         1.349683                         1.349683   \n",
          "17                         5.327022                         5.327022   \n",
          "18                              NaN                              NaN   \n",
          "19                              NaN                              NaN   \n",
          "20                              NaN                              NaN   \n",
          "21                              NaN                              NaN   \n",
          "22                              NaN                              NaN   \n",
          "23                              NaN                              NaN   \n",
          "\n",
          "    actual_emissions_per_1M_out_tok  \n",
          "0                               NaN  \n",
          "1                               NaN  \n",
          "2                               NaN  \n",
          "3                               NaN  \n",
          "4                               NaN  \n",
          "5                               NaN  \n",
          "6                               NaN  \n",
          "7                               NaN  \n",
          "8                               NaN  \n",
          "9                               NaN  \n",
          "10                              NaN  \n",
          "11                              NaN  \n",
          "12                              NaN  \n",
          "13                              NaN  \n",
          "14                              NaN  \n",
          "15                              NaN  \n",
          "16                              NaN  \n",
          "17                              NaN  \n",
          "18                         0.043024  \n",
          "19                         0.050949  \n",
          "20                         0.062306  \n",
          "21                         0.099394  \n",
          "22                         0.133245  \n",
          "23                         0.194673  "
         ]
        },
        "execution_count": 31,
        "metadata": {},
        "output_type": "execute_result"
       }
      ],
      "source": [
       "result_df"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Define chart width and height\n",
       "chart_width = 600\n",
       "chart_height = 350\n",
       "\n",
       "llama2_note = 'Note: Emissions normalized to number of output tokens for Llama2 because the Llama2 and Llama3 output differed drastically'\n",
       "\n",
       "# Function to create charts for each test type\n",
       "def create_chart(df, test_type, color, remove_x_title=False):\n",
       "    chart_data = df[df['test_type'] == test_type]\n",
       "\n",
       "    if test_type == 'Output-tok': \n",
       "        x_title = 'Average Output Tokens per Prompt'\n",
       "        x_data = 'avg_out_tok'\n",
       "    elif test_type == 'Input-tok':\n",
       "        x_title = 'Average Input Tokens per Prompt'\n",
       "        x_data = 'avg_in_tok'\n",
       "    elif test_type == 'Llama2 Params':\n",
       "        x_title = 'Parameters (billions)'\n",
       "        x_data = 'parameters'\n",
       "    elif test_type == 'Llama3 Params':\n",
       "        x_title = 'Parameters (billions)'\n",
       "        x_data = 'parameters'\n",
       "    \n",
       "    scatter = alt.Chart(chart_data).mark_circle(size=100).encode(\n",
       "        x=alt.X(x_data, title=x_title),\n",
       "        y=alt.Y('actual_emissions_per_10k_prompts', title='Actual Emissions per 10,000 Prompts'),\n",
       "        color = alt.Color('test_type:N', title='Test Type').sort(df['test_type'].unique()),\n",
       "        tooltip=[\n",
       "            alt.Tooltip('parameters', title='Parameters (billions)'),\n",
       "            alt.Tooltip('actual_emissions_per_10k_prompts', title='Actual Emissions per 10,000 Prompts'),\n",
       "            alt.Tooltip('pred_emissions_per_10k_prompts', title='Predicted Emissions per 10,000 Prompts'),\n",
       "            alt.Tooltip('avg_out_tok', title='Average Output Tokens per Prompt'),\n",
       "            alt.Tooltip('avg_in_tok', title='Average Input Tokens per Prompt'),\n",
       "            alt.Tooltip('num_examples', title='Number of Examples'),\n",
       "            alt.Tooltip('num_prompts', title='Number of Prompts'),\n",
       "            alt.Tooltip('model_type', title='Model Type'),\n",
       "            alt.Tooltip('test_type', title='Test Type'),\n",
       "            alt.Tooltip('test_type', title='Test Type'),\n",
       "        ]\n",
       "    ).properties(\n",
       "        title=f'Actual Emissions for {test_type} per 10,000 Prompts',\n",
       "        width=chart_width,\n",
       "        height=chart_height\n",
       "    )\n",
       "\n",
       "    # Create line plots for predicted emissions\n",
       "    line = alt.Chart(chart_data).mark_line().encode(\n",
       "        x=alt.X(x_data, title=x_title),\n",
       "        y=alt.Y('pred_emissions_per_10k_prompts', title='Predicted Emissions per 10,000 Prompts'),\n",
       "        color = alt.Color('test_type:N', title='Test Type').sort(df['test_type'].unique()),\n",
       "        tooltip=[\n",
       "            alt.Tooltip('parameters', title='Parameters (billions)'),\n",
       "            alt.Tooltip('actual_emissions_per_10k_prompts', title='Actual Emissions per 10,000 Prompts'),\n",
       "            alt.Tooltip('pred_emissions_per_10k_prompts', title='Predicted Emissions per 10,000 Prompts'),\n",
       "            alt.Tooltip('avg_out_tok', title='Average Output Tokens per Prompt'),\n",
       "            alt.Tooltip('avg_in_tok', title='Average Input Tokens per Prompt'),\n",
       "            alt.Tooltip('num_examples', title='Number of Examples'),\n",
       "            alt.Tooltip('num_prompts', title='Number of Prompts'),\n",
       "            alt.Tooltip('model_type', title='Model Type'),\n",
       "            alt.Tooltip('test_type', title='Test Type'),\n",
       "        ]\n",
       "    ).properties(\n",
       "        width=chart_width,\n",
       "        height=chart_height\n",
       "    )\n",
       "    \n",
       "    # Add note for Llama2\n",
       "    if test_type == 'Llama2 Params':\n",
       "        chart_data['Note'] = llama2_note\n",
       "        #print(chart_data)\n",
       "        scatter = scatter.encode(\n",
       "            tooltip=[\n",
       "                alt.Tooltip('parameters', title='Parameters (billions)'),\n",
       "                alt.Tooltip('actual_emissions_per_10k_prompts', title='Actual Emissions per 10,000 Prompts'),\n",
       "                alt.Tooltip('pred_emissions_per_10k_prompts', title='Predicted Emissions per 10,000 Prompts'),\n",
       "                alt.Tooltip('avg_out_tok', title='Average Output Tokens per Prompt'),\n",
       "                alt.Tooltip('avg_in_tok', title='Average Input Tokens per Prompt'),\n",
       "                alt.Tooltip('num_examples', title='Number of Examples'),\n",
       "                alt.Tooltip('num_prompts', title='Number of Prompts'),\n",
       "                alt.Tooltip('model_type', title='Model Type'),\n",
       "                alt.Tooltip('test_type', title='Test Type'),\n",
       "                alt.Tooltip('Note', title='Normalization Note')\n",
       "            ]\n",
       "        )\n",
       "    \n",
       "    if remove_x_title:\n",
       "        scatter = scatter.encode(\n",
       "            x=alt.X(x_data, title=None, axis=None),\n",
       "        )\n",
       "        line = line.encode(\n",
       "            x=alt.X(x_data, title=None, axis=None),\n",
       "        )\n",
       "\n",
       "\n",
       "    return scatter + line"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {},
      "outputs": [
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "/tmp/ipykernel_9243/2212380206.py:69: SettingWithCopyWarning: \n",
         "A value is trying to be set on a copy of a slice from a DataFrame.\n",
         "Try using .loc[row_indexer,col_indexer] = value instead\n",
         "\n",
         "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
         "  chart_data['Note'] = llama2_note\n"
        ]
       },
       {
        "data": {
         "text/html": [
          "\n",
          "<style>\n",
          "  #altair-viz-f0460f3da24c4ae082823beddbbcc698.vega-embed {\n",
          "    width: 100%;\n",
          "    display: flex;\n",
          "  }\n",
          "\n",
          "  #altair-viz-f0460f3da24c4ae082823beddbbcc698.vega-embed details,\n",
          "  #altair-viz-f0460f3da24c4ae082823beddbbcc698.vega-embed details summary {\n",
          "    position: relative;\n",
          "  }\n",
          "</style>\n",
          "<div id=\"altair-viz-f0460f3da24c4ae082823beddbbcc698\"></div>\n",
          "<script type=\"text/javascript\">\n",
          "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
          "  (function(spec, embedOpt){\n",
          "    let outputDiv = document.currentScript.previousElementSibling;\n",
          "    if (outputDiv.id !== \"altair-viz-f0460f3da24c4ae082823beddbbcc698\") {\n",
          "      outputDiv = document.getElementById(\"altair-viz-f0460f3da24c4ae082823beddbbcc698\");\n",
          "    }\n",
          "    const paths = {\n",
          "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
          "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
          "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
          "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
          "    };\n",
          "\n",
          "    function maybeLoadScript(lib, version) {\n",
          "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
          "      return (VEGA_DEBUG[key] == version) ?\n",
          "        Promise.resolve(paths[lib]) :\n",
          "        new Promise(function(resolve, reject) {\n",
          "          var s = document.createElement('script');\n",
          "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
          "          s.async = true;\n",
          "          s.onload = () => {\n",
          "            VEGA_DEBUG[key] = version;\n",
          "            return resolve(paths[lib]);\n",
          "          };\n",
          "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
          "          s.src = paths[lib];\n",
          "        });\n",
          "    }\n",
          "\n",
          "    function showError(err) {\n",
          "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
          "      throw err;\n",
          "    }\n",
          "\n",
          "    function displayChart(vegaEmbed) {\n",
          "      vegaEmbed(outputDiv, spec, embedOpt)\n",
          "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
          "    }\n",
          "\n",
          "    if(typeof define === \"function\" && define.amd) {\n",
          "      requirejs.config({paths});\n",
          "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
          "    } else {\n",
          "      maybeLoadScript(\"vega\", \"5\")\n",
          "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
          "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
          "        .catch(showError)\n",
          "        .then(() => displayChart(vegaEmbed));\n",
          "    }\n",
          "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"vconcat\": [{\"hconcat\": [{\"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual Emissions for Output-tok per 10,000 Prompts\"}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-c32efc453175a0323079639fc03125b4\"}, \"height\": 350, \"width\": 600}, {\"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual Emissions for Input-tok per 10,000 Prompts\"}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-9fcc7a1f817d5c9ea95e1e50750e95c1\"}, \"height\": 350, \"width\": 600}]}, {\"hconcat\": [{\"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}, {\"field\": \"Note\", \"title\": \"Normalization Note\", \"type\": \"nominal\"}], \"x\": {\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual Emissions for Llama2 Params per 10,000 Prompts\"}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-fadedba24b64f4fa256e03b1bb63cf90\"}, \"height\": 350, \"width\": 600}, {\"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual Emissions for Llama3 Params per 10,000 Prompts\"}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-66b9b8483ecb74decf4dc0c6e75f3c2e\"}, \"height\": 350, \"width\": 600}]}], \"resolve\": {\"scale\": {\"y\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-c32efc453175a0323079639fc03125b4\": [{\"test_type\": \"Output-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 1, \"num_prompts\": 150, \"total_out_tok\": 2704.0, \"total_in_tok\": 27999.0, \"avg_out_tok\": 18.027, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.7175947569881, \"actual_cpu_energy_per_10k_prompts\": 0.3411986538399133, \"actual_gpu_energy_per_10k_prompts\": 0.37330111752954, \"actual_ram_energy_per_10k_prompts\": 0.36467403594864, \"pred_emissions_per_10k_prompts\": 0.3264465027004072, \"pred_cpu_energy_per_10k_prompts\": 0.3264465027004072, \"pred_gpu_energy_per_10k_prompts\": 0.3264465027004072, \"pred_ram_energy_per_10k_prompts\": 0.3264465027004072}, {\"test_type\": \"Output-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 5, \"num_prompts\": 150, \"total_out_tok\": 10958.0, \"total_in_tok\": 27999.0, \"avg_out_tok\": 73.053, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 2.93206607344282, \"actual_cpu_energy_per_10k_prompts\": 1.3810799971297933, \"actual_gpu_energy_per_10k_prompts\": 1.5522774144135267, \"actual_ram_energy_per_10k_prompts\": 1.4761075085550066, \"pred_emissions_per_10k_prompts\": 2.7474535296297633, \"pred_cpu_energy_per_10k_prompts\": 2.7474535296297633, \"pred_gpu_energy_per_10k_prompts\": 2.7474535296297633, \"pred_ram_energy_per_10k_prompts\": 2.7474535296297633}, {\"test_type\": \"Output-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 10, \"num_prompts\": 150, \"total_out_tok\": 26537.0, \"total_in_tok\": 27999.0, \"avg_out_tok\": 176.913, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 7.227773612586493, \"actual_cpu_energy_per_10k_prompts\": 3.410975038538153, \"actual_gpu_energy_per_10k_prompts\": 3.8130650637829397, \"actual_ram_energy_per_10k_prompts\": 3.6456379324887465, \"pred_emissions_per_10k_prompts\": 7.317034996879481, \"pred_cpu_energy_per_10k_prompts\": 7.317034996879481, \"pred_gpu_energy_per_10k_prompts\": 7.317034996879481, \"pred_ram_energy_per_10k_prompts\": 7.317034996879481}, {\"test_type\": \"Output-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 30, \"num_prompts\": 50, \"total_out_tok\": 29389.0, \"total_in_tok\": 9333.0, \"avg_out_tok\": 587.78, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 24.724677756913163, \"actual_cpu_energy_per_10k_prompts\": 11.68430221903522, \"actual_gpu_energy_per_10k_prompts\": 13.0104280094452, \"actual_ram_energy_per_10k_prompts\": 12.48812871774732, \"pred_emissions_per_10k_prompts\": 25.394160249185365, \"pred_cpu_energy_per_10k_prompts\": 25.394160249185365, \"pred_gpu_energy_per_10k_prompts\": 25.394160249185365, \"pred_ram_energy_per_10k_prompts\": 25.394160249185365}, {\"test_type\": \"Output-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 90, \"num_prompts\": 50, \"total_out_tok\": 106229.0, \"total_in_tok\": 9333.0, \"avg_out_tok\": 2124.58, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 93.19251785941725, \"actual_cpu_energy_per_10k_prompts\": 43.35962525777024, \"actual_gpu_energy_per_10k_prompts\": 50.44779913042922, \"actual_ram_energy_per_10k_prompts\": 46.34260225640008, \"pred_emissions_per_10k_prompts\": 93.00953478095279, \"pred_cpu_energy_per_10k_prompts\": 93.00953478095279, \"pred_gpu_energy_per_10k_prompts\": 93.00953478095279, \"pred_ram_energy_per_10k_prompts\": 93.00953478095279}], \"data-9fcc7a1f817d5c9ea95e1e50750e95c1\": [{\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 1, \"num_prompts\": 150, \"total_out_tok\": 2701.0, \"total_in_tok\": 23799.0, \"avg_out_tok\": 18.007, \"avg_in_tok\": 158.66, \"actual_emissions_per_10k_prompts\": 1.3331964486453134, \"actual_cpu_energy_per_10k_prompts\": 0.34934319376017997, \"actual_gpu_energy_per_10k_prompts\": 1.09583639333508, \"actual_ram_energy_per_10k_prompts\": 0.5597830653999732, \"pred_emissions_per_10k_prompts\": 1.3022745433576342, \"pred_cpu_energy_per_10k_prompts\": 1.3022745433576342, \"pred_gpu_energy_per_10k_prompts\": 1.3022745433576342, \"pred_ram_energy_per_10k_prompts\": 1.3022745433576342}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 5, \"num_prompts\": 150, \"total_out_tok\": 2706.0, \"total_in_tok\": 33399.0, \"avg_out_tok\": 18.04, \"avg_in_tok\": 222.66, \"actual_emissions_per_10k_prompts\": 1.3551230332913666, \"actual_cpu_energy_per_10k_prompts\": 0.3497959287815601, \"actual_gpu_energy_per_10k_prompts\": 1.1276016717102402, \"actual_ram_energy_per_10k_prompts\": 0.5605399248038867, \"pred_emissions_per_10k_prompts\": 1.3241337828455204, \"pred_cpu_energy_per_10k_prompts\": 1.3241337828455204, \"pred_gpu_energy_per_10k_prompts\": 1.3241337828455204, \"pred_ram_energy_per_10k_prompts\": 1.3241337828455204}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 10, \"num_prompts\": 150, \"total_out_tok\": 2691.0, \"total_in_tok\": 45249.0, \"avg_out_tok\": 17.94, \"avg_in_tok\": 301.66, \"actual_emissions_per_10k_prompts\": 1.3674481275016532, \"actual_cpu_energy_per_10k_prompts\": 0.34966324762878, \"actual_gpu_energy_per_10k_prompts\": 1.1465055868333598, \"actual_ram_energy_per_10k_prompts\": 0.5603041089298201, \"pred_emissions_per_10k_prompts\": 1.3511162815883802, \"pred_cpu_energy_per_10k_prompts\": 1.3511162815883802, \"pred_gpu_energy_per_10k_prompts\": 1.3511162815883802, \"pred_ram_energy_per_10k_prompts\": 1.3511162815883802}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 20, \"num_prompts\": 150, \"total_out_tok\": 2707.0, \"total_in_tok\": 69399.0, \"avg_out_tok\": 18.047, \"avg_in_tok\": 462.66, \"actual_emissions_per_10k_prompts\": 1.4107526905917533, \"actual_cpu_energy_per_10k_prompts\": 0.36020338171411337, \"actual_gpu_energy_per_10k_prompts\": 1.1841679658521334, \"actual_ram_energy_per_10k_prompts\": 0.5772263055161067, \"pred_emissions_per_10k_prompts\": 1.4061059309250945, \"pred_cpu_energy_per_10k_prompts\": 1.4061059309250945, \"pred_gpu_energy_per_10k_prompts\": 1.4061059309250945, \"pred_ram_energy_per_10k_prompts\": 1.4061059309250945}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 30, \"num_prompts\": 150, \"total_out_tok\": 2705.0, \"total_in_tok\": 94149.0, \"avg_out_tok\": 18.033, \"avg_in_tok\": 627.66, \"actual_emissions_per_10k_prompts\": 1.4599892296728665, \"actual_cpu_energy_per_10k_prompts\": 0.36834101310592665, \"actual_gpu_energy_per_10k_prompts\": 1.2370680215054732, \"actual_ram_energy_per_10k_prompts\": 0.5902342867564133, \"pred_emissions_per_10k_prompts\": 1.4624617827298014, \"pred_cpu_energy_per_10k_prompts\": 1.4624617827298014, \"pred_gpu_energy_per_10k_prompts\": 1.4624617827298014, \"pred_ram_energy_per_10k_prompts\": 1.4624617827298014}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 70, \"num_prompts\": 150, \"total_out_tok\": 2736.0, \"total_in_tok\": 201549.0, \"avg_out_tok\": 18.24, \"avg_in_tok\": 1343.66, \"actual_emissions_per_10k_prompts\": 1.6545238583522335, \"actual_cpu_energy_per_10k_prompts\": 0.4113011142075, \"actual_gpu_energy_per_10k_prompts\": 1.4177818408912268, \"actual_ram_energy_per_10k_prompts\": 0.6591163999395001, \"pred_emissions_per_10k_prompts\": 1.7070120245005305, \"pred_cpu_energy_per_10k_prompts\": 1.7070120245005305, \"pred_gpu_energy_per_10k_prompts\": 1.7070120245005305, \"pred_ram_energy_per_10k_prompts\": 1.7070120245005305}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 210, \"num_prompts\": 150, \"total_out_tok\": 2777.0, \"total_in_tok\": 561549.0, \"avg_out_tok\": 18.513, \"avg_in_tok\": 3743.66, \"actual_emissions_per_10k_prompts\": 2.4330743218152797, \"actual_cpu_energy_per_10k_prompts\": 0.5899271231960332, \"actual_gpu_energy_per_10k_prompts\": 2.123812717567327, \"actual_ram_energy_per_10k_prompts\": 0.9453031689458534, \"pred_emissions_per_10k_prompts\": 2.52673350529627, \"pred_cpu_energy_per_10k_prompts\": 2.52673350529627, \"pred_gpu_energy_per_10k_prompts\": 2.52673350529627, \"pred_ram_energy_per_10k_prompts\": 2.52673350529627}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 350, \"num_prompts\": 150, \"total_out_tok\": 2821.0, \"total_in_tok\": 921549.0, \"avg_out_tok\": 18.807, \"avg_in_tok\": 6143.66, \"actual_emissions_per_10k_prompts\": 3.4121851274647734, \"actual_cpu_energy_per_10k_prompts\": 0.81255489918236, \"actual_gpu_energy_per_10k_prompts\": 3.016946061703067, \"actual_ram_energy_per_10k_prompts\": 1.3020036653968667, \"pred_emissions_per_10k_prompts\": 3.34645498609201, \"pred_cpu_energy_per_10k_prompts\": 3.34645498609201, \"pred_gpu_energy_per_10k_prompts\": 3.34645498609201, \"pred_ram_energy_per_10k_prompts\": 3.34645498609201}], \"data-fadedba24b64f4fa256e03b1bb63cf90\": [{\"test_type\": \"Llama2 Params\", \"model_type\": \"llama2\", \"parameters\": 7, \"num_examples\": 1, \"num_prompts\": 250, \"total_out_tok\": 9404.0, \"total_in_tok\": 42000.0, \"avg_out_tok\": 37.616, \"avg_in_tok\": 168.0, \"actual_emissions_per_10k_prompts\": 1.3237250699399716, \"actual_cpu_energy_per_10k_prompts\": 0.3463242828334113, \"actual_gpu_energy_per_10k_prompts\": 1.0894202655906442, \"actual_ram_energy_per_10k_prompts\": 0.5549743213254642, \"pred_emissions_per_10k_prompts\": 1.3218192114904133, \"pred_cpu_energy_per_10k_prompts\": 1.3218192114904133, \"pred_gpu_energy_per_10k_prompts\": 1.3218192114904133, \"pred_ram_energy_per_10k_prompts\": 1.3218192114904133, \"Note\": \"Note: Emissions normalized to number of output tokens for Llama2 because the Llama2 and Llama3 output differed drastically\"}, {\"test_type\": \"Llama2 Params\", \"model_type\": \"llama2\", \"parameters\": 13, \"num_examples\": 1, \"num_prompts\": 250, \"total_out_tok\": 10252.0, \"total_in_tok\": 42000.0, \"avg_out_tok\": 41.008, \"avg_in_tok\": 168.0, \"actual_emissions_per_10k_prompts\": 1.746947580944433, \"actual_cpu_energy_per_10k_prompts\": 0.44688362843588986, \"actual_gpu_energy_per_10k_prompts\": 1.464195657117174, \"actual_ram_energy_per_10k_prompts\": 0.7161139230843433, \"pred_emissions_per_10k_prompts\": 1.7490540560728935, \"pred_cpu_energy_per_10k_prompts\": 1.7490540560728935, \"pred_gpu_energy_per_10k_prompts\": 1.7490540560728935, \"pred_ram_energy_per_10k_prompts\": 1.7490540560728935, \"Note\": \"Note: Emissions normalized to number of output tokens for Llama2 because the Llama2 and Llama3 output differed drastically\"}, {\"test_type\": \"Llama2 Params\", \"model_type\": \"llama2\", \"parameters\": 70, \"num_examples\": 1, \"num_prompts\": 250, \"total_out_tok\": 9962.0, \"total_in_tok\": 36750.0, \"avg_out_tok\": 39.848, \"avg_in_tok\": 147.0, \"actual_emissions_per_10k_prompts\": 5.807985696285356, \"actual_cpu_energy_per_10k_prompts\": 1.3884783044494993, \"actual_gpu_energy_per_10k_prompts\": 5.121169772053554, \"actual_ram_energy_per_10k_prompts\": 2.2248444426685543, \"pred_emissions_per_10k_prompts\": 5.807785079606454, \"pred_cpu_energy_per_10k_prompts\": 5.807785079606454, \"pred_gpu_energy_per_10k_prompts\": 5.807785079606454, \"pred_ram_energy_per_10k_prompts\": 5.807785079606454, \"Note\": \"Note: Emissions normalized to number of output tokens for Llama2 because the Llama2 and Llama3 output differed drastically\"}], \"data-66b9b8483ecb74decf4dc0c6e75f3c2e\": [{\"test_type\": \"Llama3 Params\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 1, \"num_prompts\": 250, \"total_out_tok\": 4524.0, \"total_in_tok\": 33165.0, \"avg_out_tok\": 18.096, \"avg_in_tok\": 132.66, \"actual_emissions_per_10k_prompts\": 1.349682797176224, \"actual_cpu_energy_per_10k_prompts\": 0.349200310841268, \"actual_gpu_energy_per_10k_prompts\": 1.120993789460976, \"actual_ram_energy_per_10k_prompts\": 0.559561982384068, \"pred_emissions_per_10k_prompts\": 1.3496827971762235, \"pred_cpu_energy_per_10k_prompts\": 1.3496827971762235, \"pred_gpu_energy_per_10k_prompts\": 1.3496827971762235, \"pred_ram_energy_per_10k_prompts\": 1.3496827971762235}, {\"test_type\": \"Llama3 Params\", \"model_type\": \"llama3\", \"parameters\": 70, \"num_examples\": 1, \"num_prompts\": 100, \"total_out_tok\": 1879.0, \"total_in_tok\": 13266.0, \"avg_out_tok\": 18.79, \"avg_in_tok\": 132.66, \"actual_emissions_per_10k_prompts\": 5.327022326297739, \"actual_cpu_energy_per_10k_prompts\": 1.26304502757291, \"actual_gpu_energy_per_10k_prompts\": 4.72424770106179, \"actual_ram_energy_per_10k_prompts\": 2.02389033751271, \"pred_emissions_per_10k_prompts\": 5.32702232629774, \"pred_cpu_energy_per_10k_prompts\": 5.32702232629774, \"pred_gpu_energy_per_10k_prompts\": 5.32702232629774, \"pred_ram_energy_per_10k_prompts\": 5.32702232629774}]}}, {\"mode\": \"vega-lite\"});\n",
          "</script>"
         ],
         "text/plain": [
          "alt.VConcatChart(...)"
         ]
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ],
      "source": [
       "# Create charts for each test type\n",
       "charts = []\n",
       "colors = ['blue', 'green', 'red', 'purple']\n",
       "for test_type, color in zip(df['test_type'].unique(), colors):\n",
       "    charts.append(create_chart(df, test_type, color))\n",
       "\n",
       "# Arrange the charts in a grid\n",
       "grid_chart = alt.vconcat(*[alt.hconcat(*charts[i:i+2]) for i in range(0, len(charts), 2)]).resolve_scale(\n",
       "    y='independent'\n",
       ")\n",
       "\n",
       "grid_chart.show()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {},
      "outputs": [
       {
        "name": "stderr",
        "output_type": "stream",
        "text": [
         "/tmp/ipykernel_9243/2212380206.py:69: SettingWithCopyWarning: \n",
         "A value is trying to be set on a copy of a slice from a DataFrame.\n",
         "Try using .loc[row_indexer,col_indexer] = value instead\n",
         "\n",
         "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
         "  chart_data['Note'] = llama2_note\n"
        ]
       },
       {
        "data": {
         "text/html": [
          "\n",
          "<style>\n",
          "  #altair-viz-efd79bf4825e4fb6a04177b7724fe029.vega-embed {\n",
          "    width: 100%;\n",
          "    display: flex;\n",
          "  }\n",
          "\n",
          "  #altair-viz-efd79bf4825e4fb6a04177b7724fe029.vega-embed details,\n",
          "  #altair-viz-efd79bf4825e4fb6a04177b7724fe029.vega-embed details summary {\n",
          "    position: relative;\n",
          "  }\n",
          "</style>\n",
          "<div id=\"altair-viz-efd79bf4825e4fb6a04177b7724fe029\"></div>\n",
          "<script type=\"text/javascript\">\n",
          "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
          "  (function(spec, embedOpt){\n",
          "    let outputDiv = document.currentScript.previousElementSibling;\n",
          "    if (outputDiv.id !== \"altair-viz-efd79bf4825e4fb6a04177b7724fe029\") {\n",
          "      outputDiv = document.getElementById(\"altair-viz-efd79bf4825e4fb6a04177b7724fe029\");\n",
          "    }\n",
          "    const paths = {\n",
          "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
          "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
          "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
          "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
          "    };\n",
          "\n",
          "    function maybeLoadScript(lib, version) {\n",
          "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
          "      return (VEGA_DEBUG[key] == version) ?\n",
          "        Promise.resolve(paths[lib]) :\n",
          "        new Promise(function(resolve, reject) {\n",
          "          var s = document.createElement('script');\n",
          "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
          "          s.async = true;\n",
          "          s.onload = () => {\n",
          "            VEGA_DEBUG[key] = version;\n",
          "            return resolve(paths[lib]);\n",
          "          };\n",
          "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
          "          s.src = paths[lib];\n",
          "        });\n",
          "    }\n",
          "\n",
          "    function showError(err) {\n",
          "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
          "      throw err;\n",
          "    }\n",
          "\n",
          "    function displayChart(vegaEmbed) {\n",
          "      vegaEmbed(outputDiv, spec, embedOpt)\n",
          "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
          "    }\n",
          "\n",
          "    if(typeof define === \"function\" && define.amd) {\n",
          "      requirejs.config({paths});\n",
          "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
          "    } else {\n",
          "      maybeLoadScript(\"vega\", \"5\")\n",
          "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
          "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
          "        .catch(showError)\n",
          "        .then(() => displayChart(vegaEmbed));\n",
          "    }\n",
          "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"layer\": [{\"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"avg_out_tok\", \"title\": null, \"type\": \"quantitative\"}, \"y\": {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual Emissions for Output-tok per 10,000 Prompts\"}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"avg_out_tok\", \"title\": null, \"type\": \"quantitative\"}, \"y\": {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-c840eb828dcb78b6fb295d1b7edfecf0\"}}, {\"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"avg_in_tok\", \"title\": null, \"type\": \"quantitative\"}, \"y\": {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual Emissions for Input-tok per 10,000 Prompts\"}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"avg_in_tok\", \"title\": null, \"type\": \"quantitative\"}, \"y\": {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-9fcc7a1f817d5c9ea95e1e50750e95c1\"}}, {\"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}, {\"field\": \"Note\", \"title\": \"Normalization Note\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"parameters\", \"title\": null, \"type\": \"quantitative\"}, \"y\": {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual Emissions for Llama2 Params per 10,000 Prompts\"}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"parameters\", \"title\": null, \"type\": \"quantitative\"}, \"y\": {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-fadedba24b64f4fa256e03b1bb63cf90\"}}, {\"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"parameters\", \"title\": null, \"type\": \"quantitative\"}, \"y\": {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual Emissions for Llama3 Params per 10,000 Prompts\"}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"parameters\", \"title\": null, \"type\": \"quantitative\"}, \"y\": {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-66b9b8483ecb74decf4dc0c6e75f3c2e\"}}], \"height\": 700, \"resolve\": {\"scale\": {\"x\": \"independent\"}}, \"title\": \"Emissions per Ten-Thousand Prompts by Test Type\", \"width\": 1300, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-c840eb828dcb78b6fb295d1b7edfecf0\": [{\"test_type\": \"Output-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 1, \"num_prompts\": 150, \"total_out_tok\": 2704.0, \"total_in_tok\": 27999.0, \"avg_out_tok\": 18.027, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.7175947569881, \"actual_cpu_energy_per_10k_prompts\": 0.3411986538399133, \"actual_gpu_energy_per_10k_prompts\": 0.37330111752954, \"actual_ram_energy_per_10k_prompts\": 0.36467403594864, \"pred_emissions_per_10k_prompts\": 0.3264465027004072, \"pred_cpu_energy_per_10k_prompts\": 0.3264465027004072, \"pred_gpu_energy_per_10k_prompts\": 0.3264465027004072, \"pred_ram_energy_per_10k_prompts\": 0.3264465027004072}, {\"test_type\": \"Output-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 5, \"num_prompts\": 150, \"total_out_tok\": 10958.0, \"total_in_tok\": 27999.0, \"avg_out_tok\": 73.053, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 2.93206607344282, \"actual_cpu_energy_per_10k_prompts\": 1.3810799971297933, \"actual_gpu_energy_per_10k_prompts\": 1.5522774144135267, \"actual_ram_energy_per_10k_prompts\": 1.4761075085550066, \"pred_emissions_per_10k_prompts\": 2.7474535296297633, \"pred_cpu_energy_per_10k_prompts\": 2.7474535296297633, \"pred_gpu_energy_per_10k_prompts\": 2.7474535296297633, \"pred_ram_energy_per_10k_prompts\": 2.7474535296297633}, {\"test_type\": \"Output-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 10, \"num_prompts\": 150, \"total_out_tok\": 26537.0, \"total_in_tok\": 27999.0, \"avg_out_tok\": 176.913, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 7.227773612586493, \"actual_cpu_energy_per_10k_prompts\": 3.410975038538153, \"actual_gpu_energy_per_10k_prompts\": 3.8130650637829397, \"actual_ram_energy_per_10k_prompts\": 3.6456379324887465, \"pred_emissions_per_10k_prompts\": 7.317034996879481, \"pred_cpu_energy_per_10k_prompts\": 7.317034996879481, \"pred_gpu_energy_per_10k_prompts\": 7.317034996879481, \"pred_ram_energy_per_10k_prompts\": 7.317034996879481}, {\"test_type\": \"Output-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 30, \"num_prompts\": 50, \"total_out_tok\": 29389.0, \"total_in_tok\": 9333.0, \"avg_out_tok\": 587.78, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 24.724677756913163, \"actual_cpu_energy_per_10k_prompts\": 11.68430221903522, \"actual_gpu_energy_per_10k_prompts\": 13.0104280094452, \"actual_ram_energy_per_10k_prompts\": 12.48812871774732, \"pred_emissions_per_10k_prompts\": 25.394160249185365, \"pred_cpu_energy_per_10k_prompts\": 25.394160249185365, \"pred_gpu_energy_per_10k_prompts\": 25.394160249185365, \"pred_ram_energy_per_10k_prompts\": 25.394160249185365}], \"data-9fcc7a1f817d5c9ea95e1e50750e95c1\": [{\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 1, \"num_prompts\": 150, \"total_out_tok\": 2701.0, \"total_in_tok\": 23799.0, \"avg_out_tok\": 18.007, \"avg_in_tok\": 158.66, \"actual_emissions_per_10k_prompts\": 1.3331964486453134, \"actual_cpu_energy_per_10k_prompts\": 0.34934319376017997, \"actual_gpu_energy_per_10k_prompts\": 1.09583639333508, \"actual_ram_energy_per_10k_prompts\": 0.5597830653999732, \"pred_emissions_per_10k_prompts\": 1.3022745433576342, \"pred_cpu_energy_per_10k_prompts\": 1.3022745433576342, \"pred_gpu_energy_per_10k_prompts\": 1.3022745433576342, \"pred_ram_energy_per_10k_prompts\": 1.3022745433576342}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 5, \"num_prompts\": 150, \"total_out_tok\": 2706.0, \"total_in_tok\": 33399.0, \"avg_out_tok\": 18.04, \"avg_in_tok\": 222.66, \"actual_emissions_per_10k_prompts\": 1.3551230332913666, \"actual_cpu_energy_per_10k_prompts\": 0.3497959287815601, \"actual_gpu_energy_per_10k_prompts\": 1.1276016717102402, \"actual_ram_energy_per_10k_prompts\": 0.5605399248038867, \"pred_emissions_per_10k_prompts\": 1.3241337828455204, \"pred_cpu_energy_per_10k_prompts\": 1.3241337828455204, \"pred_gpu_energy_per_10k_prompts\": 1.3241337828455204, \"pred_ram_energy_per_10k_prompts\": 1.3241337828455204}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 10, \"num_prompts\": 150, \"total_out_tok\": 2691.0, \"total_in_tok\": 45249.0, \"avg_out_tok\": 17.94, \"avg_in_tok\": 301.66, \"actual_emissions_per_10k_prompts\": 1.3674481275016532, \"actual_cpu_energy_per_10k_prompts\": 0.34966324762878, \"actual_gpu_energy_per_10k_prompts\": 1.1465055868333598, \"actual_ram_energy_per_10k_prompts\": 0.5603041089298201, \"pred_emissions_per_10k_prompts\": 1.3511162815883802, \"pred_cpu_energy_per_10k_prompts\": 1.3511162815883802, \"pred_gpu_energy_per_10k_prompts\": 1.3511162815883802, \"pred_ram_energy_per_10k_prompts\": 1.3511162815883802}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 20, \"num_prompts\": 150, \"total_out_tok\": 2707.0, \"total_in_tok\": 69399.0, \"avg_out_tok\": 18.047, \"avg_in_tok\": 462.66, \"actual_emissions_per_10k_prompts\": 1.4107526905917533, \"actual_cpu_energy_per_10k_prompts\": 0.36020338171411337, \"actual_gpu_energy_per_10k_prompts\": 1.1841679658521334, \"actual_ram_energy_per_10k_prompts\": 0.5772263055161067, \"pred_emissions_per_10k_prompts\": 1.4061059309250945, \"pred_cpu_energy_per_10k_prompts\": 1.4061059309250945, \"pred_gpu_energy_per_10k_prompts\": 1.4061059309250945, \"pred_ram_energy_per_10k_prompts\": 1.4061059309250945}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 30, \"num_prompts\": 150, \"total_out_tok\": 2705.0, \"total_in_tok\": 94149.0, \"avg_out_tok\": 18.033, \"avg_in_tok\": 627.66, \"actual_emissions_per_10k_prompts\": 1.4599892296728665, \"actual_cpu_energy_per_10k_prompts\": 0.36834101310592665, \"actual_gpu_energy_per_10k_prompts\": 1.2370680215054732, \"actual_ram_energy_per_10k_prompts\": 0.5902342867564133, \"pred_emissions_per_10k_prompts\": 1.4624617827298014, \"pred_cpu_energy_per_10k_prompts\": 1.4624617827298014, \"pred_gpu_energy_per_10k_prompts\": 1.4624617827298014, \"pred_ram_energy_per_10k_prompts\": 1.4624617827298014}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 70, \"num_prompts\": 150, \"total_out_tok\": 2736.0, \"total_in_tok\": 201549.0, \"avg_out_tok\": 18.24, \"avg_in_tok\": 1343.66, \"actual_emissions_per_10k_prompts\": 1.6545238583522335, \"actual_cpu_energy_per_10k_prompts\": 0.4113011142075, \"actual_gpu_energy_per_10k_prompts\": 1.4177818408912268, \"actual_ram_energy_per_10k_prompts\": 0.6591163999395001, \"pred_emissions_per_10k_prompts\": 1.7070120245005305, \"pred_cpu_energy_per_10k_prompts\": 1.7070120245005305, \"pred_gpu_energy_per_10k_prompts\": 1.7070120245005305, \"pred_ram_energy_per_10k_prompts\": 1.7070120245005305}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 210, \"num_prompts\": 150, \"total_out_tok\": 2777.0, \"total_in_tok\": 561549.0, \"avg_out_tok\": 18.513, \"avg_in_tok\": 3743.66, \"actual_emissions_per_10k_prompts\": 2.4330743218152797, \"actual_cpu_energy_per_10k_prompts\": 0.5899271231960332, \"actual_gpu_energy_per_10k_prompts\": 2.123812717567327, \"actual_ram_energy_per_10k_prompts\": 0.9453031689458534, \"pred_emissions_per_10k_prompts\": 2.52673350529627, \"pred_cpu_energy_per_10k_prompts\": 2.52673350529627, \"pred_gpu_energy_per_10k_prompts\": 2.52673350529627, \"pred_ram_energy_per_10k_prompts\": 2.52673350529627}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 350, \"num_prompts\": 150, \"total_out_tok\": 2821.0, \"total_in_tok\": 921549.0, \"avg_out_tok\": 18.807, \"avg_in_tok\": 6143.66, \"actual_emissions_per_10k_prompts\": 3.4121851274647734, \"actual_cpu_energy_per_10k_prompts\": 0.81255489918236, \"actual_gpu_energy_per_10k_prompts\": 3.016946061703067, \"actual_ram_energy_per_10k_prompts\": 1.3020036653968667, \"pred_emissions_per_10k_prompts\": 3.34645498609201, \"pred_cpu_energy_per_10k_prompts\": 3.34645498609201, \"pred_gpu_energy_per_10k_prompts\": 3.34645498609201, \"pred_ram_energy_per_10k_prompts\": 3.34645498609201}], \"data-fadedba24b64f4fa256e03b1bb63cf90\": [{\"test_type\": \"Llama2 Params\", \"model_type\": \"llama2\", \"parameters\": 7, \"num_examples\": 1, \"num_prompts\": 250, \"total_out_tok\": 9404.0, \"total_in_tok\": 42000.0, \"avg_out_tok\": 37.616, \"avg_in_tok\": 168.0, \"actual_emissions_per_10k_prompts\": 1.3237250699399716, \"actual_cpu_energy_per_10k_prompts\": 0.3463242828334113, \"actual_gpu_energy_per_10k_prompts\": 1.0894202655906442, \"actual_ram_energy_per_10k_prompts\": 0.5549743213254642, \"pred_emissions_per_10k_prompts\": 1.3218192114904133, \"pred_cpu_energy_per_10k_prompts\": 1.3218192114904133, \"pred_gpu_energy_per_10k_prompts\": 1.3218192114904133, \"pred_ram_energy_per_10k_prompts\": 1.3218192114904133, \"Note\": \"Note: Emissions normalized to number of output tokens for Llama2 because the Llama2 and Llama3 output differed drastically\"}, {\"test_type\": \"Llama2 Params\", \"model_type\": \"llama2\", \"parameters\": 13, \"num_examples\": 1, \"num_prompts\": 250, \"total_out_tok\": 10252.0, \"total_in_tok\": 42000.0, \"avg_out_tok\": 41.008, \"avg_in_tok\": 168.0, \"actual_emissions_per_10k_prompts\": 1.746947580944433, \"actual_cpu_energy_per_10k_prompts\": 0.44688362843588986, \"actual_gpu_energy_per_10k_prompts\": 1.464195657117174, \"actual_ram_energy_per_10k_prompts\": 0.7161139230843433, \"pred_emissions_per_10k_prompts\": 1.7490540560728935, \"pred_cpu_energy_per_10k_prompts\": 1.7490540560728935, \"pred_gpu_energy_per_10k_prompts\": 1.7490540560728935, \"pred_ram_energy_per_10k_prompts\": 1.7490540560728935, \"Note\": \"Note: Emissions normalized to number of output tokens for Llama2 because the Llama2 and Llama3 output differed drastically\"}, {\"test_type\": \"Llama2 Params\", \"model_type\": \"llama2\", \"parameters\": 70, \"num_examples\": 1, \"num_prompts\": 250, \"total_out_tok\": 9962.0, \"total_in_tok\": 36750.0, \"avg_out_tok\": 39.848, \"avg_in_tok\": 147.0, \"actual_emissions_per_10k_prompts\": 5.807985696285356, \"actual_cpu_energy_per_10k_prompts\": 1.3884783044494993, \"actual_gpu_energy_per_10k_prompts\": 5.121169772053554, \"actual_ram_energy_per_10k_prompts\": 2.2248444426685543, \"pred_emissions_per_10k_prompts\": 5.807785079606454, \"pred_cpu_energy_per_10k_prompts\": 5.807785079606454, \"pred_gpu_energy_per_10k_prompts\": 5.807785079606454, \"pred_ram_energy_per_10k_prompts\": 5.807785079606454, \"Note\": \"Note: Emissions normalized to number of output tokens for Llama2 because the Llama2 and Llama3 output differed drastically\"}], \"data-66b9b8483ecb74decf4dc0c6e75f3c2e\": [{\"test_type\": \"Llama3 Params\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 1, \"num_prompts\": 250, \"total_out_tok\": 4524.0, \"total_in_tok\": 33165.0, \"avg_out_tok\": 18.096, \"avg_in_tok\": 132.66, \"actual_emissions_per_10k_prompts\": 1.349682797176224, \"actual_cpu_energy_per_10k_prompts\": 0.349200310841268, \"actual_gpu_energy_per_10k_prompts\": 1.120993789460976, \"actual_ram_energy_per_10k_prompts\": 0.559561982384068, \"pred_emissions_per_10k_prompts\": 1.3496827971762235, \"pred_cpu_energy_per_10k_prompts\": 1.3496827971762235, \"pred_gpu_energy_per_10k_prompts\": 1.3496827971762235, \"pred_ram_energy_per_10k_prompts\": 1.3496827971762235}, {\"test_type\": \"Llama3 Params\", \"model_type\": \"llama3\", \"parameters\": 70, \"num_examples\": 1, \"num_prompts\": 100, \"total_out_tok\": 1879.0, \"total_in_tok\": 13266.0, \"avg_out_tok\": 18.79, \"avg_in_tok\": 132.66, \"actual_emissions_per_10k_prompts\": 5.327022326297739, \"actual_cpu_energy_per_10k_prompts\": 1.26304502757291, \"actual_gpu_energy_per_10k_prompts\": 4.72424770106179, \"actual_ram_energy_per_10k_prompts\": 2.02389033751271, \"pred_emissions_per_10k_prompts\": 5.32702232629774, \"pred_cpu_energy_per_10k_prompts\": 5.32702232629774, \"pred_gpu_energy_per_10k_prompts\": 5.32702232629774, \"pred_ram_energy_per_10k_prompts\": 5.32702232629774}]}}, {\"mode\": \"vega-lite\"});\n",
          "</script>"
         ],
         "text/plain": [
          "alt.LayerChart(...)"
         ]
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ],
      "source": [
       "# Create charts for each test type\n",
       "stacked_charts = []\n",
       "stacked_df = df[df.num_examples != 90]\n",
       "for test_type, color in zip(stacked_df['test_type'].unique(), colors):\n",
       "    stacked_charts.append(create_chart(stacked_df, test_type, color, remove_x_title=True))\n",
       "\n",
       "\n",
       "# Create a combined chart with overlays for all emission types\n",
       "combined_chart = alt.layer(*stacked_charts).resolve_scale(\n",
       "    x='independent'\n",
       ").properties(\n",
       "    title='Emissions per Ten-Thousand Prompts by Test Type',\n",
       "    width=1300,  # Adjusted width for combined chart\n",
       "    height=700  # Adjusted height for combined chart\n",
       ")\n",
       "\n",
       "combined_chart.show()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Store the results in a CSV file\n",
       "result_df.to_csv('results/emission_regression.csv', index=False)"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 2
   }
   