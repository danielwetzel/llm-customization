{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the individual CSV files\n",
    "csv_files = {\n",
    "    '1': 'outputtest-meta-llama/Meta-Llama-3-8B-Instruct-OutputTest-emissiondata-1-examples.csv',\n",
    "    '5': 'outputtest-meta-llama/Meta-Llama-3-8B-Instruct-OutputTest-emissiondata-5-examples.csv',\n",
    "    '10': 'outputtest-meta-llama/Meta-Llama-3-8B-Instruct-OutputTest-emissiondata-10-examples.csv',\n",
    "    '30': 'outputtest-meta-llama/Meta-Llama-3-8B-Instruct-OutputTest-emissiondata-30-examples.csv',\n",
    "    #'90': 'outputtest-meta-llama/Meta-Llama-3-8B-Instruct-OutputTest-emissiondata-90-examples.csv',\n",
    "}\n",
    "\n",
    "# Read the emissions data\n",
    "emissions_data = pd.read_csv('emissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store metadata\n",
    "parameters = []\n",
    "num_examples = []\n",
    "num_prompts = []\n",
    "total_emissions = []\n",
    "cpu_energy = []\n",
    "gpu_energy = []\n",
    "ram_energy = []\n",
    "total_output_tokens = []\n",
    "total_input_tokens = []\n",
    "avg_input_tokens = []\n",
    "avg_output_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and extract metadata from each CSV file\n",
    "for model, file in csv_files.items():\n",
    "    data = pd.read_csv(file)\n",
    "    prompts = data.loc[data['Metric'] == 'Total Prompts', 'Value'].values[0]\n",
    "    output_tokens = data.loc[data['Metric'] == 'Total Output Tokens', 'Value'].values[0]\n",
    "    input_tokens = data.loc[data['Metric'] == 'Total Input Tokens', 'Value'].values[0]\n",
    "    avg_i_tok = data.loc[data['Metric'] == 'Avg Input Tokens per Prompt', 'Value'].values[0]\n",
    "    avg_o_tok = data.loc[data['Metric'] == 'Avg Output Tokens per Prompt', 'Value'].values[0]\n",
    "    parameters.append(70)\n",
    "    num_examples.append(int(model))\n",
    "    num_prompts.append(int(prompts))\n",
    "    total_output_tokens.append(float(output_tokens))\n",
    "    total_input_tokens.append(float(input_tokens))\n",
    "    avg_input_tokens.append(float(avg_i_tok))\n",
    "    avg_output_tokens.append(float(avg_o_tok))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract emissions data\n",
    "for model in csv_files.keys():\n",
    "    model_emissions = emissions_data[emissions_data['project_name'].str.contains(\"Llama-3-8B-Instruct-OutputTest-emissiondata-\" + model + \"-\")]\n",
    "    total_emissions.append(model_emissions['emissions'].values[0])\n",
    "    cpu_energy.append(model_emissions['cpu_energy'].values[0])\n",
    "    gpu_energy.append(model_emissions['gpu_energy'].values[0])\n",
    "    ram_energy.append(model_emissions['ram_energy'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[186.66, 186.66, 186.66, 186.66]\n",
      "[18.027, 73.053, 176.913, 587.78]\n",
      "[2704.0, 10958.0, 26537.0, 29389.0]\n",
      "[0.0107639213548215, 0.0439809911016423, 0.1084166041887974, 0.1236233887845658]\n"
     ]
    }
   ],
   "source": [
    "print(avg_input_tokens)\n",
    "print(avg_output_tokens)\n",
    "print(total_output_tokens)\n",
    "print(total_emissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for regression and visualization\n",
    "parameters = np.array(parameters)\n",
    "num_examples = np.array(num_examples)\n",
    "num_prompts = np.array(num_prompts)\n",
    "total_output_tokens = np.array(total_output_tokens)\n",
    "total_input_tokens = np.array(total_input_tokens)\n",
    "avg_input_tokens = np.array(avg_input_tokens)\n",
    "avg_output_tokens = np.array(avg_output_tokens)\n",
    "total_emissions = np.array(total_emissions)\n",
    "cpu_energy = np.array(cpu_energy)\n",
    "gpu_energy = np.array(gpu_energy)\n",
    "ram_energy = np.array(ram_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate emissions per 10,000 prompts\n",
    "emissions_per_thousand_prompts = {\n",
    "    'Total Emissions': total_emissions / num_prompts * 10_000,\n",
    "    'CPU Energy': cpu_energy / num_prompts * 10_000,\n",
    "    'GPU Energy': gpu_energy / num_prompts * 10_000,\n",
    "    'RAM Energy': ram_energy / num_prompts * 10_000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total Emissions': array([ 0.71759476,  2.93206607,  7.22777361, 24.72467776]), 'CPU Energy': array([ 0.34119865,  1.38108   ,  3.41097504, 11.68430222]), 'GPU Energy': array([ 0.37330112,  1.55227741,  3.81306506, 13.01042801]), 'RAM Energy': array([ 0.36467404,  1.47610751,  3.64563793, 12.48812872])}\n"
     ]
    }
   ],
   "source": [
    "print(emissions_per_thousand_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform regression analysis\n",
    "def perform_regression(x, y):\n",
    "    x = x.reshape(-1, 1)\n",
    "    model = LinearRegression()\n",
    "    model.fit(x, y)\n",
    "    predicted = model.predict(x)\n",
    "    return model, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Emissions - Intercept: -0.1372507836507495, Coefficient: 0.04224381387883653\n",
      "CPU Energy - Intercept: -0.06708984508294691, Coefficient: 0.01996547599524041\n",
      "GPU Energy - Intercept: -0.06762141160236013, Coefficient: 0.022225002718689008\n",
      "RAM Energy - Intercept: -0.07169695534157139, Coefficient: 0.021338995289762592\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "predictions = {}\n",
    "for name, y in emissions_per_thousand_prompts.items():\n",
    "    model, predicted = perform_regression(avg_output_tokens, y)\n",
    "    models[name] = model\n",
    "    predictions[name] = predicted\n",
    "    print(f\"{name} - Intercept: {model.intercept_}, Coefficient: {model.coef_[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Altair visualization\n",
    "vis_data = pd.DataFrame({\n",
    "    'Parameters (billions)': parameters.flatten(),\n",
    "    'Number of Examples': num_examples.flatten(),\n",
    "    'Total Output Tokens': total_output_tokens.flatten(),\n",
    "    'Total Input Tokens': total_input_tokens.flatten(),\n",
    "    'Avg Input Tokens per Prompt': avg_input_tokens.flatten(),\n",
    "    'Avg Output Tokens per Prompt': avg_output_tokens.flatten(),\n",
    "    'Number of Prompts': num_prompts.flatten(),\n",
    "    'Total Emissions per Ten-Thousand Prompts': emissions_per_thousand_prompts['Total Emissions'],\n",
    "    'CPU Energy per Ten-Thousand Prompts': emissions_per_thousand_prompts['CPU Energy'],\n",
    "    'GPU Energy per Ten-Thousand Prompts': emissions_per_thousand_prompts['GPU Energy'],\n",
    "    'RAM Energy per Ten-Thousand Prompts': emissions_per_thousand_prompts['RAM Energy'],\n",
    "    'Total Emissions Predicted': predictions['Total Emissions'],\n",
    "    'CPU Energy Predicted': predictions['CPU Energy'],\n",
    "    'GPU Energy Predicted': predictions['GPU Energy'],\n",
    "    'RAM Energy Predicted': predictions['RAM Energy']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-4a879a4d4c114dabb82e46ac20fa56cd.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-4a879a4d4c114dabb82e46ac20fa56cd.vega-embed details,\n",
       "  #altair-viz-4a879a4d4c114dabb82e46ac20fa56cd.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-4a879a4d4c114dabb82e46ac20fa56cd\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-4a879a4d4c114dabb82e46ac20fa56cd\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-4a879a4d4c114dabb82e46ac20fa56cd\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"vconcat\": [{\"hconcat\": [{\"layer\": [{\"mark\": {\"type\": \"circle\", \"color\": \"blue\", \"size\": 100}, \"encoding\": {\"tooltip\": [{\"field\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"Total Emissions per Ten-Thousand Prompts\", \"type\": \"quantitative\"}, {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Avg Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"Number of Prompts\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Total Emissions per Ten-Thousand Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual Total Emissions per Ten-Thousand Prompts\"}, {\"mark\": {\"type\": \"line\", \"color\": \"blue\"}, \"encoding\": {\"tooltip\": [{\"field\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"Total Emissions per Ten-Thousand Prompts\", \"type\": \"quantitative\"}, {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Avg Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"Number of Prompts\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Total Emissions Predicted\", \"type\": \"quantitative\"}}}], \"height\": 350, \"width\": 600}, {\"layer\": [{\"mark\": {\"type\": \"circle\", \"color\": \"green\", \"size\": 100}, \"encoding\": {\"tooltip\": [{\"field\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"CPU Energy per Ten-Thousand Prompts\", \"type\": \"quantitative\"}, {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Avg Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"Number of Prompts\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"CPU Energy per Ten-Thousand Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual CPU Energy per Ten-Thousand Prompts\"}, {\"mark\": {\"type\": \"line\", \"color\": \"green\"}, \"encoding\": {\"tooltip\": [{\"field\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"CPU Energy per Ten-Thousand Prompts\", \"type\": \"quantitative\"}, {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Avg Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"Number of Prompts\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"CPU Energy Predicted\", \"type\": \"quantitative\"}}}], \"height\": 350, \"width\": 600}]}, {\"hconcat\": [{\"layer\": [{\"mark\": {\"type\": \"circle\", \"color\": \"red\", \"size\": 100}, \"encoding\": {\"tooltip\": [{\"field\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"GPU Energy per Ten-Thousand Prompts\", \"type\": \"quantitative\"}, {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Avg Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"Number of Prompts\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"GPU Energy per Ten-Thousand Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual GPU Energy per Ten-Thousand Prompts\"}, {\"mark\": {\"type\": \"line\", \"color\": \"red\"}, \"encoding\": {\"tooltip\": [{\"field\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"GPU Energy per Ten-Thousand Prompts\", \"type\": \"quantitative\"}, {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Avg Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"Number of Prompts\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"GPU Energy Predicted\", \"type\": \"quantitative\"}}}], \"height\": 350, \"width\": 600}, {\"layer\": [{\"mark\": {\"type\": \"circle\", \"color\": \"purple\", \"size\": 100}, \"encoding\": {\"tooltip\": [{\"field\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"RAM Energy per Ten-Thousand Prompts\", \"type\": \"quantitative\"}, {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Avg Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"Number of Prompts\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"RAM Energy per Ten-Thousand Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual RAM Energy per Ten-Thousand Prompts\"}, {\"mark\": {\"type\": \"line\", \"color\": \"purple\"}, \"encoding\": {\"tooltip\": [{\"field\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"RAM Energy per Ten-Thousand Prompts\", \"type\": \"quantitative\"}, {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Avg Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"Number of Prompts\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"RAM Energy Predicted\", \"type\": \"quantitative\"}}}], \"height\": 350, \"width\": 600}]}], \"data\": {\"name\": \"data-e3c2156037344db0066fa8201a70a033\"}, \"resolve\": {\"scale\": {\"y\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-e3c2156037344db0066fa8201a70a033\": [{\"Parameters (billions)\": 70, \"Number of Examples\": 1, \"Total Output Tokens\": 2704.0, \"Total Input Tokens\": 27999.0, \"Avg Input Tokens per Prompt\": 186.66, \"Avg Output Tokens per Prompt\": 18.027, \"Number of Prompts\": 150, \"Total Emissions per Ten-Thousand Prompts\": 0.7175947569881, \"CPU Energy per Ten-Thousand Prompts\": 0.3411986538399133, \"GPU Energy per Ten-Thousand Prompts\": 0.37330111752954, \"RAM Energy per Ten-Thousand Prompts\": 0.36467403594864, \"Total Emissions Predicted\": 0.6242784491430367, \"CPU Energy Predicted\": 0.292827790683252, \"GPU Energy Predicted\": 0.33302871240744664, \"RAM Energy Predicted\": 0.3129811127469789}, {\"Parameters (billions)\": 70, \"Number of Examples\": 5, \"Total Output Tokens\": 10958.0, \"Total Input Tokens\": 27999.0, \"Avg Input Tokens per Prompt\": 186.66, \"Avg Output Tokens per Prompt\": 73.053, \"Number of Prompts\": 150, \"Total Emissions per Ten-Thousand Prompts\": 2.93206607344282, \"CPU Energy per Ten-Thousand Prompts\": 1.3810799971297933, \"GPU Energy per Ten-Thousand Prompts\": 1.5522774144135267, \"RAM Energy per Ten-Thousand Prompts\": 1.4761075085550066, \"Total Emissions Predicted\": 2.9487865516398952, \"CPU Energy Predicted\": 1.3914480727973508, \"GPU Energy Predicted\": 1.555981712006028, \"RAM Energy Predicted\": 1.4871806675614552}, {\"Parameters (billions)\": 70, \"Number of Examples\": 10, \"Total Output Tokens\": 26537.0, \"Total Input Tokens\": 27999.0, \"Avg Input Tokens per Prompt\": 186.66, \"Avg Output Tokens per Prompt\": 176.913, \"Number of Prompts\": 150, \"Total Emissions per Ten-Thousand Prompts\": 7.227773612586493, \"CPU Energy per Ten-Thousand Prompts\": 3.410975038538153, \"GPU Energy per Ten-Thousand Prompts\": 3.8130650637829397, \"RAM Energy per Ten-Thousand Prompts\": 3.6456379324887465, \"Total Emissions Predicted\": 7.336229061095858, \"CPU Energy Predicted\": 3.46506240966302, \"GPU Energy Predicted\": 3.8642704943690687, \"RAM Energy Predicted\": 3.7034487183561984}, {\"Parameters (billions)\": 70, \"Number of Examples\": 30, \"Total Output Tokens\": 29389.0, \"Total Input Tokens\": 9333.0, \"Avg Input Tokens per Prompt\": 186.66, \"Avg Output Tokens per Prompt\": 587.78, \"Number of Prompts\": 50, \"Total Emissions per Ten-Thousand Prompts\": 24.724677756913163, \"CPU Energy per Ten-Thousand Prompts\": 11.68430221903522, \"GPU Energy per Ten-Thousand Prompts\": 13.0104280094452, \"RAM Energy per Ten-Thousand Prompts\": 12.48812871774732, \"Total Emissions Predicted\": 24.692818138051784, \"CPU Energy Predicted\": 11.668217635399461, \"GPU Energy Predicted\": 12.995790686388665, \"RAM Energy Predicted\": 12.470937696075083}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.VConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define chart width and height\n",
    "chart_width = 600\n",
    "chart_height = 350\n",
    "\n",
    "# Create scatter and line plots for each emission type per million output tokens\n",
    "charts = []\n",
    "for emission_type, color in zip(['Total Emissions', 'CPU Energy', 'GPU Energy', 'RAM Energy'],\n",
    "                                ['blue', 'green', 'red', 'purple']):\n",
    "    scatter = alt.Chart(vis_data).mark_circle(size=100, color=color).encode(\n",
    "        x='Avg Output Tokens per Prompt',\n",
    "        y=f'{emission_type} per Ten-Thousand Prompts',\n",
    "        tooltip=['Parameters (billions)', \n",
    "                 f'{emission_type} per Ten-Thousand Prompts', \n",
    "                 'Avg Output Tokens per Prompt', \n",
    "                 'Avg Input Tokens per Prompt', \n",
    "                 'Number of Examples', \n",
    "                 'Number of Prompts']\n",
    "    ).properties(\n",
    "        title=f'Actual {emission_type} per Ten-Thousand Prompts',\n",
    "        width=chart_width,\n",
    "        height=chart_height\n",
    "    )\n",
    "    \n",
    "    # Create line plots for predicted emissions\n",
    "    line = alt.Chart(vis_data).mark_line(color=color).encode(\n",
    "        x='Avg Output Tokens per Prompt',\n",
    "        y=f'{emission_type} Predicted',\n",
    "        tooltip=['Parameters (billions)', \n",
    "                 f'{emission_type} per Ten-Thousand Prompts', \n",
    "                 'Avg Output Tokens per Prompt', \n",
    "                 'Avg Input Tokens per Prompt', \n",
    "                 'Number of Examples', \n",
    "                 'Number of Prompts']\n",
    "    ).properties(\n",
    "        width=chart_width,\n",
    "        height=chart_height\n",
    "    )\n",
    "    \n",
    "    charts.append(scatter + line)\n",
    "\n",
    "# Arrange the charts in a 2x2 grid\n",
    "grid_chart = alt.vconcat(\n",
    "    alt.hconcat(charts[0], charts[1]),\n",
    "    alt.hconcat(charts[2], charts[3])\n",
    ").resolve_scale(\n",
    "    y='independent'\n",
    ")\n",
    "\n",
    "grid_chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-f7b2d48e3f294c469b0a5b43a1eee628.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-f7b2d48e3f294c469b0a5b43a1eee628.vega-embed details,\n",
       "  #altair-viz-f7b2d48e3f294c469b0a5b43a1eee628.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-f7b2d48e3f294c469b0a5b43a1eee628\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-f7b2d48e3f294c469b0a5b43a1eee628\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-f7b2d48e3f294c469b0a5b43a1eee628\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"layer\": [{\"layer\": [{\"mark\": {\"type\": \"circle\", \"color\": \"blue\", \"size\": 100}, \"encoding\": {\"tooltip\": [{\"field\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"Total Emissions per Ten-Thousand Prompts\", \"type\": \"quantitative\"}, {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Avg Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"Number of Prompts\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Total Emissions per Ten-Thousand Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual Total Emissions per Ten-Thousand Prompts\"}, {\"mark\": {\"type\": \"line\", \"color\": \"blue\"}, \"encoding\": {\"tooltip\": [{\"field\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"Total Emissions per Ten-Thousand Prompts\", \"type\": \"quantitative\"}, {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Avg Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"Number of Prompts\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Total Emissions Predicted\", \"type\": \"quantitative\"}}}]}, {\"layer\": [{\"mark\": {\"type\": \"circle\", \"color\": \"green\", \"size\": 100}, \"encoding\": {\"tooltip\": [{\"field\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"CPU Energy per Ten-Thousand Prompts\", \"type\": \"quantitative\"}, {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Avg Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"Number of Prompts\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"CPU Energy per Ten-Thousand Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual CPU Energy per Ten-Thousand Prompts\"}, {\"mark\": {\"type\": \"line\", \"color\": \"green\"}, \"encoding\": {\"tooltip\": [{\"field\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"CPU Energy per Ten-Thousand Prompts\", \"type\": \"quantitative\"}, {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Avg Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"Number of Prompts\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"CPU Energy Predicted\", \"type\": \"quantitative\"}}}]}, {\"layer\": [{\"mark\": {\"type\": \"circle\", \"color\": \"red\", \"size\": 100}, \"encoding\": {\"tooltip\": [{\"field\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"GPU Energy per Ten-Thousand Prompts\", \"type\": \"quantitative\"}, {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Avg Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"Number of Prompts\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"GPU Energy per Ten-Thousand Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual GPU Energy per Ten-Thousand Prompts\"}, {\"mark\": {\"type\": \"line\", \"color\": \"red\"}, \"encoding\": {\"tooltip\": [{\"field\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"GPU Energy per Ten-Thousand Prompts\", \"type\": \"quantitative\"}, {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Avg Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"Number of Prompts\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"GPU Energy Predicted\", \"type\": \"quantitative\"}}}]}, {\"layer\": [{\"mark\": {\"type\": \"circle\", \"color\": \"purple\", \"size\": 100}, \"encoding\": {\"tooltip\": [{\"field\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"RAM Energy per Ten-Thousand Prompts\", \"type\": \"quantitative\"}, {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Avg Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"Number of Prompts\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"RAM Energy per Ten-Thousand Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual RAM Energy per Ten-Thousand Prompts\"}, {\"mark\": {\"type\": \"line\", \"color\": \"purple\"}, \"encoding\": {\"tooltip\": [{\"field\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"RAM Energy per Ten-Thousand Prompts\", \"type\": \"quantitative\"}, {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Avg Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"Number of Prompts\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Avg Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"RAM Energy Predicted\", \"type\": \"quantitative\"}}}]}], \"data\": {\"name\": \"data-e3c2156037344db0066fa8201a70a033\"}, \"height\": 400, \"resolve\": {\"scale\": {\"y\": \"independent\"}}, \"title\": \"Emissions per Ten-Thousand Prompts by Type vs Avg Output Tokens\", \"width\": 600, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-e3c2156037344db0066fa8201a70a033\": [{\"Parameters (billions)\": 70, \"Number of Examples\": 1, \"Total Output Tokens\": 2704.0, \"Total Input Tokens\": 27999.0, \"Avg Input Tokens per Prompt\": 186.66, \"Avg Output Tokens per Prompt\": 18.027, \"Number of Prompts\": 150, \"Total Emissions per Ten-Thousand Prompts\": 0.7175947569881, \"CPU Energy per Ten-Thousand Prompts\": 0.3411986538399133, \"GPU Energy per Ten-Thousand Prompts\": 0.37330111752954, \"RAM Energy per Ten-Thousand Prompts\": 0.36467403594864, \"Total Emissions Predicted\": 0.6242784491430367, \"CPU Energy Predicted\": 0.292827790683252, \"GPU Energy Predicted\": 0.33302871240744664, \"RAM Energy Predicted\": 0.3129811127469789}, {\"Parameters (billions)\": 70, \"Number of Examples\": 5, \"Total Output Tokens\": 10958.0, \"Total Input Tokens\": 27999.0, \"Avg Input Tokens per Prompt\": 186.66, \"Avg Output Tokens per Prompt\": 73.053, \"Number of Prompts\": 150, \"Total Emissions per Ten-Thousand Prompts\": 2.93206607344282, \"CPU Energy per Ten-Thousand Prompts\": 1.3810799971297933, \"GPU Energy per Ten-Thousand Prompts\": 1.5522774144135267, \"RAM Energy per Ten-Thousand Prompts\": 1.4761075085550066, \"Total Emissions Predicted\": 2.9487865516398952, \"CPU Energy Predicted\": 1.3914480727973508, \"GPU Energy Predicted\": 1.555981712006028, \"RAM Energy Predicted\": 1.4871806675614552}, {\"Parameters (billions)\": 70, \"Number of Examples\": 10, \"Total Output Tokens\": 26537.0, \"Total Input Tokens\": 27999.0, \"Avg Input Tokens per Prompt\": 186.66, \"Avg Output Tokens per Prompt\": 176.913, \"Number of Prompts\": 150, \"Total Emissions per Ten-Thousand Prompts\": 7.227773612586493, \"CPU Energy per Ten-Thousand Prompts\": 3.410975038538153, \"GPU Energy per Ten-Thousand Prompts\": 3.8130650637829397, \"RAM Energy per Ten-Thousand Prompts\": 3.6456379324887465, \"Total Emissions Predicted\": 7.336229061095858, \"CPU Energy Predicted\": 3.46506240966302, \"GPU Energy Predicted\": 3.8642704943690687, \"RAM Energy Predicted\": 3.7034487183561984}, {\"Parameters (billions)\": 70, \"Number of Examples\": 30, \"Total Output Tokens\": 29389.0, \"Total Input Tokens\": 9333.0, \"Avg Input Tokens per Prompt\": 186.66, \"Avg Output Tokens per Prompt\": 587.78, \"Number of Prompts\": 50, \"Total Emissions per Ten-Thousand Prompts\": 24.724677756913163, \"CPU Energy per Ten-Thousand Prompts\": 11.68430221903522, \"GPU Energy per Ten-Thousand Prompts\": 13.0104280094452, \"RAM Energy per Ten-Thousand Prompts\": 12.48812871774732, \"Total Emissions Predicted\": 24.692818138051784, \"CPU Energy Predicted\": 11.668217635399461, \"GPU Energy Predicted\": 12.995790686388665, \"RAM Energy Predicted\": 12.470937696075083}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a combined chart with overlays for all emission types\n",
    "combined_chart = alt.layer(*charts).resolve_scale(\n",
    "    y='independent'\n",
    ").properties(\n",
    "    title='Emissions per Ten-Thousand Prompts by Type vs Avg Output Tokens',\n",
    "    width=600,  # Adjusted width for combined chart\n",
    "    height=400  # Adjusted height for combined chart\n",
    ")\n",
    "\n",
    "combined_chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
