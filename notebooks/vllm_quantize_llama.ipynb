{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BitsAndBytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "#from codecarbon import EmissionsTracker, track_emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "quantized_model_name = \"Llama-2-70b-chat-8bit-bnb-quant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantization configurations - so you quantize the model while inferencing\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    #load_in_4bit = True,\n",
    "    load_in_8bit=True,\n",
    "    #llm_int8_skip_modules=[\"lm_head\"],\n",
    "    #llm_int8_threshold=4.0,\n",
    "    #llm_int8_has_fp16_weight=True,\n",
    "    #bnb_4bit_qunat_type = \"nf4\",\n",
    "    #bnb_4bit_compute_dtype = torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3490267fb934a91a7033b062b148335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    #load_in_8bit=True,\n",
    "    #model_path,\n",
    "    #num_labels=4,  # Adjust if necessary based on the specific task\n",
    "    quantization_config=bnb_config,\n",
    "    #device_map=\"auto\",\n",
    "    device_map=\"sequential\",\n",
    "    #torch_dtype=torch.float16,\n",
    "    #device_map=device_map,\n",
    "    #load_in_8bit_fp32_cpu_offload=True,\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(quantized_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Llama-2-70b-8bit/tokenizer_config.json',\n",
       " 'Llama-2-70b-8bit/special_tokens_map.json',\n",
       " 'Llama-2-70b-8bit/tokenizer.model',\n",
       " 'Llama-2-70b-8bit/added_tokens.json',\n",
       " 'Llama-2-70b-8bit/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(quantized_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed1d3896c9742b9a063c96ca77cf5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00015.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b09998fc55d48859470ca33364bfb18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00015.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa618074cda4965beebbc49330c2c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00015.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42975a545df14f0bb25d1754be5cc2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00015.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d307752790453da40065a74eea34cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 15 LFS files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968fce352ce44d6bbd4f3e1e625ca64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00015.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df43544dd7d44cfa278aa7d49ee20d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00015.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe53eebe3f34bdbbf162b9d3dce5065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00015.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae10444a5f6c44179ef9243d68644244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00015.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48177dcbb9b94722837f0192b06042ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00015.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8079d0fa34c94bf5a09f9240b095cd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00015.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0db7d7ec5a94e079133d25d6c5f371f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00015-of-00015.safetensors:   0%|          | 0.00/524M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30419b051a61408cb08d89fd0ceb0ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00015.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c585915ca9f48fcb2b52bb97db6a816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00015.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034a4824ebfd468cab54a09c5ff1a8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00015.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0893e00a5dfd47dab3f0d021ceca629f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00015.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/dwetzel/Llama-2-70b-8bit/commit/95add82b85c18238271f437ec7353661dc6bf860', commit_message='Upload LlamaForCausalLM', commit_description='', oid='95add82b85c18238271f437ec7353661dc6bf860', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(quantized_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91b092102fe4c37a9802d51973e1c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2303ac8b8740789efe4657f9a23826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/dwetzel/Llama-2-70b-chat-8bit-bnb-quant/commit/8d5277c88349851dfc66a0a92a76b9685636e245', commit_message='Upload tokenizer', commit_description='', oid='8d5277c88349851dfc66a0a92a76b9685636e245', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(quantized_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with the following kwargs: {'torch_dtype': 'auto', 'device_map': 'auto', 'cache_dir': None, 'force_download': False, 'proxies': None, 'resume_download': False, 'local_files_only': False, 'use_auth_token': None, 'revision': None, 'subfolder': '', '_commit_hash': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4167f8d2ac6d4cc786a201018215d350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define quantization config with static activation scales\n",
    "quantize_config = BaseQuantizeConfig(quant_method=\"fp8\", activation_scheme=\"dynamic\")\n",
    "# For dynamic activation scales, there is no need for calbration examples\n",
    "examples = []\n",
    "\n",
    "# Load the model, quantize, and save checkpoint\n",
    "model = AutoFP8ForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing weights: 100%|██████████| 454/454 [00:00<00:00, 2878.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): FP8DynamicLinear()\n",
      "          (k_proj): FP8DynamicLinear()\n",
      "          (v_proj): FP8DynamicLinear()\n",
      "          (o_proj): FP8DynamicLinear()\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): FP8DynamicLinear()\n",
      "          (up_proj): FP8DynamicLinear()\n",
      "          (down_proj): FP8DynamicLinear()\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n",
      "Saving the model to Meta-Llama-3-8B-Instruct-FP8-Dynamic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model.quantize(examples)\n",
    "model.save_quantized(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_fp8 import AutoFP8ForCausalLM, BaseQuantizeConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "quantized_model_name = \"Llama-2-70b-chat-8bit-fp8-quant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with the following kwargs: {'torch_dtype': 'auto', 'device_map': 'auto', 'cache_dir': None, 'force_download': False, 'proxies': None, 'resume_download': False, 'local_files_only': False, 'use_auth_token': None, 'revision': None, 'subfolder': '', '_commit_hash': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30de56d205e04176989e3854eefe87d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# Define quantization config with static activation scales\n",
    "quantize_config = BaseQuantizeConfig(quant_method=\"fp8\", activation_scheme=\"dynamic\")\n",
    "# For dynamic activation scales, there is no need for calbration examples\n",
    "examples = []\n",
    "\n",
    "# Load the model, quantize, and save checkpoint\n",
    "model = AutoFP8ForCausalLM.from_pretrained(model_name, quantize_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing weights:  16%|█▌        | 180/1126 [00:00<00:01, 641.55it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 448.00 MiB. GPU \u0001 has a total capacity of 21.96 GiB of which 443.06 MiB is free. Including non-PyTorch memory, this process has 21.52 GiB memory in use. Of the allocated memory 21.27 GiB is allocated by PyTorch, and 49.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39msave_quantized(quantized_model_name)\n",
      "File \u001b[0;32m~/llm-customization/notebooks/AutoFP8/auto_fp8/modeling.py:113\u001b[0m, in \u001b[0;36mAutoFP8ForCausalLM.quantize\u001b[0;34m(self, calibration_tokens)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquantize\u001b[39m(\u001b[38;5;28mself\u001b[39m, calibration_tokens: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    111\u001b[0m \n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# Always quantize the weights as they do not require calibration data\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[43mquantize_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantize_config\u001b[38;5;241m.\u001b[39mactivation_scheme \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    117\u001b[0m             calibration_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalibration tokens required for activation quantization\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/llm-customization/notebooks/AutoFP8/auto_fp8/quantize.py:237\u001b[0m, in \u001b[0;36mquantize_weights\u001b[0;34m(model, quantize_config)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(linear, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear)\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m quantize_config\u001b[38;5;241m.\u001b[39mignored_layers\n\u001b[1;32m    235\u001b[0m ):\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m quant_weight, weight_scale \u001b[38;5;241m=\u001b[39m \u001b[43mper_tensor_quantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m bias \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(linear\u001b[38;5;241m.\u001b[39mbias) \u001b[38;5;28;01mif\u001b[39;00m linear\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    239\u001b[0m quant_linear \u001b[38;5;241m=\u001b[39m FP8DynamicLinear(\n\u001b[1;32m    240\u001b[0m     weight\u001b[38;5;241m=\u001b[39mquant_weight, weight_scale\u001b[38;5;241m=\u001b[39mweight_scale, bias\u001b[38;5;241m=\u001b[39mbias\n\u001b[1;32m    241\u001b[0m )\n",
      "File \u001b[0;32m~/llm-customization/notebooks/AutoFP8/auto_fp8/quantize.py:56\u001b[0m, in \u001b[0;36mper_tensor_quantize\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m     52\u001b[0m scale \u001b[38;5;241m=\u001b[39m finfo\u001b[38;5;241m.\u001b[39mmax \u001b[38;5;241m/\u001b[39m amax\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-12\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# scale and clamp the tensor to bring it to\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# the representative range of float8 data type\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# (as default cast is unsaturated)\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m qweight \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Return both float8 data and the inverse scale (as float),\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# as both required as inputs to torch._scaled_mm\u001b[39;00m\n\u001b[1;32m     59\u001b[0m qweight \u001b[38;5;241m=\u001b[39m qweight\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat8_e4m3fn)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 448.00 MiB. GPU \u0001 has a total capacity of 21.96 GiB of which 443.06 MiB is free. Including non-PyTorch memory, this process has 21.52 GiB memory in use. Of the allocated memory 21.27 GiB is allocated by PyTorch, and 49.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model.quantize(examples)\n",
    "model.save_quantized(quantized_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TextGenerationPipeline\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from datasets import load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_dir = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "quantized_model_dir = \"Llama-2-70b-chat-8bit-GPTQ-quant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb47e69bb6a64055b244d9c351f93fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.91k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71447cd0f89c439d9a7f1a05ee73b5bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba2911591aa46e997699fbd8f5a10de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/200035 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sally had 13 peaches at her roadside fruit dish.  She went to the orchard and picked peaches to stock up. She picked 55 peaches. There are _____ peaches now.\n"
     ]
    }
   ],
   "source": [
    "orca_math = load_dataset(\"microsoft/orca-math-word-problems-200k\", split='train')\n",
    "\n",
    "orca_math_examples = orca_math.shuffle(seed=42).select(range(100))\n",
    "\n",
    "examples = orca_math_examples['question']\n",
    "examples.append(orca_math_examples['answer'])\n",
    "\n",
    "print(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir)\n",
    "tokenized_examples = [tokenizer(text) for text in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=8,  # quantize model to 4-bit\n",
    "    group_size=128,  # it is recommended to set the value to 128\n",
    "    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantize model, the examples should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\n",
    "model.quantize(tokenized_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-09 10:39:44 config.py:468] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-09 10:39:45 config.py:698] Defaulting to use mp for distributed inference\n",
      "INFO 07-09 10:39:45 llm_engine.py:169] Initializing an LLM engine (v0.5.1) with config: model='neuralmagic/Meta-Llama-3-70B-Instruct-FP8-KV', speculative_config=None, tokenizer='neuralmagic/Meta-Llama-3-70B-Instruct-FP8-KV', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=fp8, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=neuralmagic/Meta-Llama-3-70B-Instruct-FP8-KV, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-09 10:39:45 selector.py:163] Cannot use FlashAttention-2 backend for FP8 KV cache.\n",
      "INFO 07-09 10:39:45 selector.py:53] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m INFO 07-09 10:39:45 selector.py:163] Cannot use FlashAttention-2 backend for FP8 KV cache.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m INFO 07-09 10:39:45 selector.py:53] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m INFO 07-09 10:39:45 selector.py:163] Cannot use FlashAttention-2 backend for FP8 KV cache.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m INFO 07-09 10:39:45 selector.py:53] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m INFO 07-09 10:39:45 selector.py:163] Cannot use FlashAttention-2 backend for FP8 KV cache.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m INFO 07-09 10:39:45 selector.py:53] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m INFO 07-09 10:39:46 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m INFO 07-09 10:39:46 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m INFO 07-09 10:39:46 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 07-09 10:39:47 utils.py:741] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m INFO 07-09 10:39:47 utils.py:741] Found nccl from library libnccl.so.2\n",
      "INFO 07-09 10:39:47 utils.py:741] Found nccl from library libnccl.so.2\n",
      "INFO 07-09 10:39:47 pynccl.py:63] vLLM is using nccl==2.21.5\n",
      "INFO 07-09 10:39:47 utils.py:741] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m INFO 07-09 10:39:47 pynccl.py:63] vLLM is using nccl==2.21.5\n",
      "INFO 07-09 10:39:47 pynccl.py:63] vLLM is using nccl==2.21.5\n",
      "INFO 07-09 10:39:47 pynccl.py:63] vLLM is using nccl==2.21.5\n",
      "WARNING 07-09 10:39:47 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m WARNING 07-09 10:39:47 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 07-09 10:39:47 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 07-09 10:39:47 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 07-09 10:39:47 fp8.py:45] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m WARNING 07-09 10:39:47 fp8.py:45] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n",
      "WARNING 07-09 10:39:47 fp8.py:45] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n",
      "WARNING 07-09 10:39:47 fp8.py:45] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n",
      "INFO 07-09 10:39:47 selector.py:163] Cannot use FlashAttention-2 backend for FP8 KV cache.\n",
      "INFO 07-09 10:39:47 selector.py:53] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m INFO 07-09 10:39:47 selector.py:163] Cannot use FlashAttention-2 backend for FP8 KV cache.\n",
      "INFO 07-09 10:39:47 selector.py:163] Cannot use FlashAttention-2 backend for FP8 KV cache.\n",
      "INFO 07-09 10:39:47 selector.py:163] Cannot use FlashAttention-2 backend for FP8 KV cache.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m INFO 07-09 10:39:47 selector.py:53] Using XFormers backend.\n",
      "INFO 07-09 10:39:47 selector.py:53] Using XFormers backend.\n",
      "INFO 07-09 10:39:47 selector.py:53] Using XFormers backend.\n",
      "INFO 07-09 10:39:48 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m INFO 07-09 10:39:48 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m INFO 07-09 10:39:48 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m INFO 07-09 10:39:48 weight_utils.py:218] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9701486e5dc84808a0165bffd0afc71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00015.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d4a619de83439faa444e0cc1eaf013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00015.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af19661f80b3401c8d1df2e34db364b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00015.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5a49de76ba45f7a0451bd4a524c28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00015.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14410c741d5447f28834f9b6f71ee315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00015.safetensors:   0%|          | 0.00/4.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f567c2060e47e791b70a856f5be2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00015.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da908f67c1f460b910fd5f6fb701354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00015.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054d01aa59d04eaeb2a266079eb9e159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00015.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac361177723c4b848acdd92aa91cf8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00015.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9552c77e2c6e4189878affd599e1f01b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00015.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5eeb804c6349d18983c1194dfc1314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00015.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713088529e1844f88a7be4461e52d87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00015.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572c514c8a4748c59f1475fdce2654ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00015.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38b2670150947749ce89b54f18a9ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00015.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14dbd68a223c42c2a67ab140d52b7af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00015-of-00015.safetensors:   0%|          | 0.00/3.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a9fc9edb6c4ab6a157fede33db8a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/163k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-09 10:41:31 model_runner.py:255] Loading model weights took 16.9227 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m INFO 07-09 10:41:36 model_runner.py:255] Loading model weights took 16.9227 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m INFO 07-09 10:41:36 model_runner.py:255] Loading model weights took 16.9227 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m INFO 07-09 10:41:36 model_runner.py:255] Loading model weights took 16.9227 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method determine_num_available_blocks: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Ddesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`, Traceback (most recent call last):\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method determine_num_available_blocks: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Ddesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`, Traceback (most recent call last):\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method determine_num_available_blocks: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Ddesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`, Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/worker.py\", line 173, in determine_num_available_blocks\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/worker.py\", line 173, in determine_num_available_blocks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/worker.py\", line 173, in determine_num_available_blocks\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     self.model_runner.profile_run()\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     self.model_runner.profile_run()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     self.model_runner.profile_run()\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 874, in profile_run\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 874, in profile_run\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     self.execute_model(model_input, kv_caches, intermediate_tensors)\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     self.execute_model(model_input, kv_caches, intermediate_tensors)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 874, in profile_run\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     self.execute_model(model_input, kv_caches, intermediate_tensors)\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1243, in execute_model\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1243, in execute_model\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     hidden_or_intermediate_states = model_executable(\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     hidden_or_intermediate_states = model_executable(\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]                                     ^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]                                     ^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1243, in execute_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     hidden_or_intermediate_states = model_executable(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Ddesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mneuralmagic/Meta-Llama-3-70B-Instruct-FP8-KV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfp8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, my name is\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/entrypoints/llm.py:149\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is no need to pass vision-related arguments anymore.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    130\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    131\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    148\u001b[0m )\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/engine/llm_engine.py:414\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context)\u001b[0m\n\u001b[1;32m    411\u001b[0m     executor_class \u001b[38;5;241m=\u001b[39m GPUExecutor\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/engine/llm_engine.py:256\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, multimodal_config, speculative_config, decoding_config, observability_config, executor_class, log_stats, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m executor_class(\n\u001b[1;32m    244\u001b[0m     model_config\u001b[38;5;241m=\u001b[39mmodel_config,\n\u001b[1;32m    245\u001b[0m     cache_config\u001b[38;5;241m=\u001b[39mcache_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m     load_config\u001b[38;5;241m=\u001b[39mload_config,\n\u001b[1;32m    253\u001b[0m )\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39membedding_mode:\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_kv_caches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# If usage stat is enabled, collect relevant info.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_usage_stats_enabled():\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/engine/llm_engine.py:353\u001b[0m, in \u001b[0;36mLLMEngine._initialize_kv_caches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_initialize_kv_caches\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the KV cache in the worker(s).\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m    The workers will determine the number of blocks in both the GPU cache\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m    and the swap CPU cache.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     num_gpu_blocks, num_cpu_blocks \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 353\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetermine_num_available_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks_override \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m         num_gpu_blocks_override \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks_override\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/executor/distributed_gpu_executor.py:38\u001b[0m, in \u001b[0;36mDistributedGPUExecutor.determine_num_available_blocks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Determine the number of available KV blocks.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mThis invokes `determine_num_available_blocks` on each worker and takes\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    - tuple[num_gpu_blocks, num_cpu_blocks]\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Get the maximum number of blocks that can be allocated on GPU and CPU.\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m num_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdetermine_num_available_blocks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Since we use a shared centralized controller, we take the minimum\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# number of blocks across all workers to make sure all the memory\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# operators can be applied to all workers.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m num_gpu_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(b[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m num_blocks)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/executor/multiproc_gpu_executor.py:130\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._run_workers\u001b[0;34m(self, method, async_run_tensor_parallel_workers_only, max_concurrent_workers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m worker_outputs\n\u001b[1;32m    129\u001b[0m driver_worker_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker, method)\n\u001b[0;32m--> 130\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m \u001b[43mdriver_worker_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Get the results of the workers.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [driver_worker_output\n\u001b[1;32m    134\u001b[0m         ] \u001b[38;5;241m+\u001b[39m [output\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m worker_outputs]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/worker.py:173\u001b[0m, in \u001b[0;36mWorker.determine_num_available_blocks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Execute a forward pass with dummy inputs to profile the memory usage\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# of the model.\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofile_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Calculate the number of blocks that can be allocated with the\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# profiled peak memory.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/model_runner.py:874\u001b[0m, in \u001b[0;36mGPUModelRunnerBase.profile_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_first_rank:\n\u001b[1;32m    870\u001b[0m     intermediate_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmake_empty_intermediate_tensors(\n\u001b[1;32m    871\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    872\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    873\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/model_runner.py:1243\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, model_input, kv_caches, intermediate_tensors, num_steps)\u001b[0m\n\u001b[1;32m   1238\u001b[0m multi_modal_kwargs \u001b[38;5;241m=\u001b[39m model_input\u001b[38;5;241m.\u001b[39mmulti_modal_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m   1239\u001b[0m seqlen_agnostic_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinished_requests_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_input\u001b[38;5;241m.\u001b[39mfinished_requests_ids,\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest_ids_to_seq_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_input\u001b[38;5;241m.\u001b[39mrequest_ids_to_seq_ids,\n\u001b[1;32m   1242\u001b[0m } \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_seqlen_agnostic \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m-> 1243\u001b[0m hidden_or_intermediate_states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_executable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmulti_modal_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mseqlen_agnostic_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;66;03m# Compute the logits in the last pipeline stage.\u001b[39;00m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py:401\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    395\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m     intermediate_tensors: Optional[IntermediateTensors] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    400\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[torch\u001b[38;5;241m.\u001b[39mTensor, IntermediateTensors]:\n\u001b[0;32m--> 401\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_output\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py:306\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors, inputs_embeds)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_layer):\n\u001b[1;32m    305\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\n\u001b[0;32m--> 306\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m IntermediateTensors({\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: hidden_states,\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresidual\u001b[39m\u001b[38;5;124m\"\u001b[39m: residual\n\u001b[1;32m    318\u001b[0m     })\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py:230\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, positions, hidden_states, kv_cache, attn_metadata, residual)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(\n\u001b[1;32m    229\u001b[0m         hidden_states, residual)\n\u001b[0;32m--> 230\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    238\u001b[0m hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(\n\u001b[1;32m    239\u001b[0m     hidden_states, residual)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py:161\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, positions, hidden_states, kv_cache, attn_metadata)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    156\u001b[0m     positions: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m     attn_metadata: AttentionMetadata,\n\u001b[1;32m    160\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 161\u001b[0m     qkv, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqkv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39msplit([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_size], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    163\u001b[0m     q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(positions, q, k)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py:317\u001b[0m, in \u001b[0;36mColumnParallelLinear.forward\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Matrix multiply.\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m output_parallel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_output:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# All-gather across the partitions.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     output \u001b[38;5;241m=\u001b[39m tensor_model_parallel_all_gather(output_parallel)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/fp8.py:370\u001b[0m, in \u001b[0;36mFp8LinearMethod.apply\u001b[0;34m(self, layer, x, bias)\u001b[0m\n\u001b[1;32m    362\u001b[0m         qinput, x_scale \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mscaled_fp8_quant(x,\n\u001b[1;32m    363\u001b[0m                                                layer\u001b[38;5;241m.\u001b[39minput_scale,\n\u001b[1;32m    364\u001b[0m                                                batch_dim_padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m17\u001b[39m)\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;66;03m# Fused GEMM_DQ -- note we padded the input above because\u001b[39;00m\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;66;03m# torch._scaled_mm is more performant for matrices with\u001b[39;00m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;66;03m# batch dimension > 16. Note that this could change\u001b[39;00m\n\u001b[1;32m    369\u001b[0m         \u001b[38;5;66;03m# in the future.\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m         output, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scaled_mm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m            \u001b[49m\u001b[43mqinput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m            \u001b[49m\u001b[43mout_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscale_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscale_b\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnarrow(output, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Ddesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]                                     ^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 401, in forward\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 401, in forward\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     model_output = self.model(input_ids, positions, kv_caches,\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     model_output = self.model(input_ids, positions, kv_caches,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 401, in forward\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     model_output = self.model(input_ids, positions, kv_caches,\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 306, in forward\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 306, in forward\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     hidden_states, residual = layer(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     hidden_states, residual = layer(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]                               ^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]                               ^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 306, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     hidden_states, residual = layer(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]                               ^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 230, in forward\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 230, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     hidden_states = self.self_attn(\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     hidden_states = self.self_attn(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]                     ^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]                     ^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 230, in forward\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     hidden_states = self.self_attn(\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]                     ^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 161, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 161, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     qkv, _ = self.qkv_proj(hidden_states)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     qkv, _ = self.qkv_proj(hidden_states)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 161, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     qkv, _ = self.qkv_proj(hidden_states)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 317, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 317, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     output_parallel = self.quant_method.apply(self, input_, bias)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     output_parallel = self.quant_method.apply(self, input_, bias)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/fp8.py\", line 370, in apply\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/fp8.py\", line 370, in apply\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     output, _ = torch._scaled_mm(\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 317, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     output, _ = torch._scaled_mm(\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]                 ^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     output_parallel = self.quant_method.apply(self, input_, bias)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]                 ^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226] RuntimeError: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Ddesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36134)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226] \n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226] RuntimeError: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Ddesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`\n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/fp8.py\", line 370, in apply\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36133)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226] \n",
      "ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]     output, _ = torch._scaled_mm(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226]                 ^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226] RuntimeError: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Ddesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`\n",
      "\u001b[1;36m(VllmWorkerProcess pid=36135)\u001b[0;0m ERROR 07-09 10:41:38 multiproc_worker_utils.py:226] \n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "model = LLM(model=\"neuralmagic/Meta-Llama-3-70B-Instruct-FP8-KV\", kv_cache_dtype=\"fp8\", tensor_parallel_size=4)\n",
    "print(model.generate(\"Hello, my name is\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.generate(\"Hello, my name is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute in Shell\n",
    "#%%sh\n",
    "#python -m vllm.entrypoints.openai.api_server --model dwetzel/Llama-3-8B-Instruct_Orce_plus --tensor-parallel-size 4 --enable-chunked-prefill --dtype auto --quantization fp8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute in Shell - For Docker\n",
    "# %%sh\n",
    "# docker run --runtime nvidia --gpus all -v ~/.cache/huggingface:/root/.cache/huggingface --env \"HUGGING_FACE_HUB_TOKEN=...\" -p 8000:8000 --ipc=host vllm/vllm-openai:latest --model neuralmagic/Meta-Llama-3-70B-Instruct-FP8-KV --tensor-parallel-size 4 --kv-cache-dtype fp8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import  AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    CBLACKBG  = '\\33[40m'\n",
    "    CREDBG    = '\\33[41m'\n",
    "    CGREENBG  = '\\33[42m'\n",
    "    CYELLOWBG = '\\33[43m'\n",
    "    CBLUEBG   = '\\33[44m'\n",
    "    CVIOLETBG = '\\33[45m'\n",
    "    CBEIGEBG  = '\\33[46m'\n",
    "    CWHITEBG  = '\\33[47m'\n",
    "    CBLACK  = '\\33[30m'\n",
    "    CRED    = '\\33[31m'\n",
    "    CGREEN  = '\\33[32m'\n",
    "    CYELLOW = '\\33[33m'\n",
    "    CBLUE   = '\\33[34m'\n",
    "    CVIOLET = '\\33[35m'\n",
    "    CBEIGE  = '\\33[36m'\n",
    "    CWHITE  = '\\33[37m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  9 11:23:58 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA L4                      On  | 00000000:38:00.0 Off |                    0 |\n",
      "| N/A   49C    P0              28W /  72W |  19758MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA L4                      On  | 00000000:3A:00.0 Off |                    0 |\n",
      "| N/A   49C    P0              29W /  72W |  19756MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA L4                      On  | 00000000:3C:00.0 Off |                    0 |\n",
      "| N/A   47C    P0              27W /  72W |  19756MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA L4                      On  | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   48C    P0              28W /  72W |  19756MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     68094      C   python3                                   19740MiB |\n",
      "|    1   N/A  N/A     68306      C   python3                                   19738MiB |\n",
      "|    2   N/A  N/A     68307      C   python3                                   19738MiB |\n",
      "|    3   N/A  N/A     68308      C   python3                                   19738MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "model_name = \"neuralmagic/Meta-Llama-3-70B-Instruct-FP8-KV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\", role='assistant', function_call=None, tool_calls=[])\n"
     ]
    }
   ],
   "source": [
    "prompt = [{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=prompt\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prompt(prompt, completion, with_system=False):\n",
    "    print(\"=\"*30 + f\" Chat with  --- {model_name} ---  LLM using vLLM \" + \"=\"*30 + \"\\n\")\n",
    "    for idx, message in enumerate(prompt):\n",
    "        if prompt[idx]['role'] == 'user':\n",
    "            color = bcolors.CBLUE\n",
    "            print(color + f\"[ {prompt[idx]['role'].upper()} ]\" + bcolors.ENDC)\n",
    "            print(prompt[idx]['content']+ \"\\n\")\n",
    "        else: \n",
    "            if with_system:\n",
    "                color = bcolors.CVIOLET\n",
    "                print(color + f\"[ {prompt[idx]['role'].upper()} ]\" + bcolors.ENDC)\n",
    "                print(prompt[idx]['content']+ \"\\n\")\n",
    "\n",
    "    print(bcolors.OKGREEN + f\"[ {completion.choices[0].message.role.upper()} ]\" + bcolors.ENDC)\n",
    "    print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Chat with  --- neuralmagic/Meta-Llama-3-70B-Instruct-FP8-KV ---  LLM using vLLM ==============================\n",
      "\n",
      "\u001b[34m[ USER ]\u001b[0m\n",
      "Hello!\n",
      "\n",
      "\u001b[92m[ ASSISTANT ]\u001b[0m\n",
      "Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "print_prompt(prompt, completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Who are you and how can you help me?\\n\\n Also can you help me with the following question: \\n\\n What is the Distance from Pluto to the Sun?\"\n",
    "user_message = f\"###Question:\\n{instruction} \\n\\n ###Answer:\"\n",
    "\n",
    "question = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Your task is to help the user with the following question:\"},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## GEMMA 2 does not support the \"system\" role, so we will only use the \"user\" role.\n",
    "\n",
    "# system = \"You are a helpful assistant. Your task is to help the user with the following question:\"\n",
    "# instruction = \"Who are you and how can you help me?\\n\\n Also can you help me with the following question: \\n\\n What is the Distance from Pluto to the Sun?\"\n",
    "# user_message = f\"###SYSTEM:\\n {system} \\n\\n ###QUESTION:\\n{instruction} \\n\\n ###ANSWER:\"\n",
    "\n",
    "# question = [\n",
    "#     {\"role\": \"user\", \"content\": user_message},\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(prompt, stream, with_system=False):\n",
    "    print(\"=\"*30 + f\" Chat with  --- {model_name} ---  LLM using vLLM \" + \"=\"*30 + \"\\n\")\n",
    "    for idx, message in enumerate(prompt):\n",
    "        if prompt[idx]['role'] == 'user':\n",
    "            color = bcolors.CBLUE\n",
    "            print(color + f\"[ {prompt[idx]['role'].upper()} ]\" + bcolors.ENDC)\n",
    "            print(prompt[idx]['content']+ \"\\n\")\n",
    "        else: \n",
    "            if with_system:\n",
    "                color = bcolors.CVIOLET\n",
    "                print(color + f\"[ {prompt[idx]['role'].upper()} ]\" + bcolors.ENDC)\n",
    "                print(prompt[idx]['content']+ \"\\n\")\n",
    "\n",
    "    print(bcolors.OKGREEN + f\"[ ASSISTANT ]\" + bcolors.ENDC + \"\\n\")\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Chat with  --- neuralmagic/Meta-Llama-3-70B-Instruct-FP8-KV ---  LLM using vLLM ==============================\n",
      "\n",
      "\u001b[34m[ USER ]\u001b[0m\n",
      "###Question:\n",
      "Who are you and how can you help me?\n",
      "\n",
      " Also can you help me with the following question: \n",
      "\n",
      " What is the Distance from Pluto to the Sun? \n",
      "\n",
      " ###Answer:\n",
      "\n",
      "\u001b[92m[ ASSISTANT ]\u001b[0m\n",
      "\n",
      "Nice to meet you! I'm a helpful assistant, here to provide information and assist with a wide range of questions and topics. I'm a large language model trained on a massive dataset, which enables me to provide accurate and up-to-date answers to your questions.\n",
      "\n",
      "Now, let's get to your question!\n",
      "\n",
      "The average distance from Pluto to the Sun is approximately 39.5 astronomical units (AU). One astronomical unit is the average distance between the Earth and the Sun, which is about 93 million miles or 149.6 million kilometers. So, if we do the math, the average distance from Pluto to the Sun is roughly:\n",
      "\n",
      "39.5 AU x 93 million miles/AU = 3.67 billion miles (5.9 billion kilometers)\n",
      "\n",
      "However, it's essential to note that Pluto's orbit is elliptical, which means its distance from the Sun varies throughout the year. At its closest point (perihelion), Pluto is about 29.7 AU (2.66 billion miles or 4.28 billion kilometers) from the Sun, and at its farthest point (aphelion), it's around 49.3 AU (4.29 billion miles or 6.9 billion kilometers) away.\n",
      "\n",
      "I hope that helps! Do you have any more questions or topics you'd like to explore?"
     ]
    }
   ],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=question,\n",
    "    stream=True, \n",
    "    extra_body={\n",
    "        \"min_p\" : 0.05 # If it's set to 0.05, then it will allow tokens at least 1/20th as probable as the top token to be generated.\n",
    "    }\n",
    ")\n",
    "print_stream(question , stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
