{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_fp8 import AutoFP8ForCausalLM, BaseQuantizeConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_dir = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "quantized_model_dir = \"Meta-Llama-3-8B-Instruct-FP8-Dynamic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with the following kwargs: {'torch_dtype': 'auto', 'device_map': 'auto', 'cache_dir': None, 'force_download': False, 'proxies': None, 'resume_download': False, 'local_files_only': False, 'use_auth_token': None, 'revision': None, 'subfolder': '', '_commit_hash': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4167f8d2ac6d4cc786a201018215d350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define quantization config with static activation scales\n",
    "quantize_config = BaseQuantizeConfig(quant_method=\"fp8\", activation_scheme=\"dynamic\")\n",
    "# For dynamic activation scales, there is no need for calbration examples\n",
    "examples = []\n",
    "\n",
    "# Load the model, quantize, and save checkpoint\n",
    "model = AutoFP8ForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantizing weights: 100%|██████████| 454/454 [00:00<00:00, 2878.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): FP8DynamicLinear()\n",
      "          (k_proj): FP8DynamicLinear()\n",
      "          (v_proj): FP8DynamicLinear()\n",
      "          (o_proj): FP8DynamicLinear()\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): FP8DynamicLinear()\n",
      "          (up_proj): FP8DynamicLinear()\n",
      "          (down_proj): FP8DynamicLinear()\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n",
      "Saving the model to Meta-Llama-3-8B-Instruct-FP8-Dynamic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model.quantize(examples)\n",
    "model.save_quantized(quantized_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-08 15:06:24 config.py:698] Defaulting to use mp for distributed inference\n",
      "INFO 07-08 15:06:24 llm_engine.py:169] Initializing an LLM engine (v0.5.1) with config: model='Meta-Llama-3-8B-Instruct-FP8-Dynamic/', speculative_config=None, tokenizer='Meta-Llama-3-8B-Instruct-FP8-Dynamic/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=Meta-Llama-3-8B-Instruct-FP8-Dynamic/, use_v2_block_manager=False, enable_prefix_caching=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m INFO 07-08 15:06:24 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m INFO 07-08 15:06:24 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m INFO 07-08 15:06:25 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 07-08 15:06:25 utils.py:741] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m INFO 07-08 15:06:25 pynccl.py:63] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m INFO 07-08 15:06:25 utils.py:741] Found nccl from library libnccl.so.2\n",
      "INFO 07-08 15:06:25 utils.py:741] Found nccl from library libnccl.so.2\n",
      "INFO 07-08 15:06:25 utils.py:741] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m INFO 07-08 15:06:25 pynccl.py:63] vLLM is using nccl==2.21.5\n",
      "INFO 07-08 15:06:25 pynccl.py:63] vLLM is using nccl==2.21.5\n",
      "INFO 07-08 15:06:25 pynccl.py:63] vLLM is using nccl==2.21.5\n",
      "WARNING 07-08 15:06:25 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m WARNING 07-08 15:06:25 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 07-08 15:06:25 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 07-08 15:06:25 custom_all_reduce.py:118] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 07-08 15:06:25 fp8.py:45] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m WARNING 07-08 15:06:25 fp8.py:45] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n",
      "WARNING 07-08 15:06:25 fp8.py:45] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n",
      "WARNING 07-08 15:06:25 fp8.py:45] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\n",
      "INFO 07-08 15:06:26 model_runner.py:255] Loading model weights took 2.1168 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m INFO 07-08 15:06:26 model_runner.py:255] Loading model weights took 2.1168 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m INFO 07-08 15:06:26 model_runner.py:255] Loading model weights took 2.1168 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m INFO 07-08 15:06:26 model_runner.py:255] Loading model weights took 2.1168 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method determine_num_available_blocks: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Ddesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`, Traceback (most recent call last):\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method determine_num_available_blocks: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Ddesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`, Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method determine_num_available_blocks: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Ddesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`, Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/worker.py\", line 173, in determine_num_available_blocks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/worker.py\", line 173, in determine_num_available_blocks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     self.model_runner.profile_run()\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     self.model_runner.profile_run()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/worker.py\", line 173, in determine_num_available_blocks\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     self.model_runner.profile_run()\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 874, in profile_run\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 874, in profile_run\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     self.execute_model(model_input, kv_caches, intermediate_tensors)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     self.execute_model(model_input, kv_caches, intermediate_tensors)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 874, in profile_run\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     self.execute_model(model_input, kv_caches, intermediate_tensors)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1243, in execute_model\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1243, in execute_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     hidden_or_intermediate_states = model_executable(\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     hidden_or_intermediate_states = model_executable(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]                                     ^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]                                     ^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1243, in execute_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     hidden_or_intermediate_states = model_executable(\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]                                     ^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 401, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 401, in forward\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     model_output = self.model(input_ids, positions, kv_caches,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     model_output = self.model(input_ids, positions, kv_caches,\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 401, in forward\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     model_output = self.model(input_ids, positions, kv_caches,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 306, in forward\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 306, in forward\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     hidden_states, residual = layer(\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]                               ^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     hidden_states, residual = layer(\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 306, in forward\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]                               ^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     hidden_states, residual = layer(\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]                               ^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 230, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     hidden_states = self.self_attn(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]                     ^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 230, in forward\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 230, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     hidden_states = self.self_attn(\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     hidden_states = self.self_attn(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]                     ^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]                     ^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 161, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     qkv, _ = self.qkv_proj(hidden_states)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 161, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     qkv, _ = self.qkv_proj(hidden_states)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 161, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     qkv, _ = self.qkv_proj(hidden_states)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 317, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     output_parallel = self.quant_method.apply(self, input_, bias)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 317, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/fp8.py\", line 370, in apply\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     output_parallel = self.quant_method.apply(self, input_, bias)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     output, _ = torch._scaled_mm(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 317, in forward\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]                 ^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     output_parallel = self.quant_method.apply(self, input_, bias)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/fp8.py\", line 370, in apply\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226] RuntimeError: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Ddesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54139)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     output, _ = torch._scaled_mm(\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226] \n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]                 ^^^^^^^^^^^^^^^^^\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/fp8.py\", line 370, in apply\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226] RuntimeError: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Ddesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`\n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]     output, _ = torch._scaled_mm(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54138)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226] \n",
      "ERROR 07-08 15:06:27 multiproc_worker_utils.py:226]                 ^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226] RuntimeError: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Ddesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`\n",
      "\u001b[1;36m(VllmWorkerProcess pid=54140)\u001b[0;0m ERROR 07-08 15:06:27 multiproc_worker_utils.py:226] \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Ddesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMeta-Llama-3-8B-Instruct-FP8-Dynamic/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/entrypoints/llm.py:149\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is no need to pass vision-related arguments anymore.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    130\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    131\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    148\u001b[0m )\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/engine/llm_engine.py:414\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context)\u001b[0m\n\u001b[1;32m    411\u001b[0m     executor_class \u001b[38;5;241m=\u001b[39m GPUExecutor\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/engine/llm_engine.py:256\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, multimodal_config, speculative_config, decoding_config, observability_config, executor_class, log_stats, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m executor_class(\n\u001b[1;32m    244\u001b[0m     model_config\u001b[38;5;241m=\u001b[39mmodel_config,\n\u001b[1;32m    245\u001b[0m     cache_config\u001b[38;5;241m=\u001b[39mcache_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m     load_config\u001b[38;5;241m=\u001b[39mload_config,\n\u001b[1;32m    253\u001b[0m )\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39membedding_mode:\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_kv_caches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# If usage stat is enabled, collect relevant info.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_usage_stats_enabled():\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/engine/llm_engine.py:353\u001b[0m, in \u001b[0;36mLLMEngine._initialize_kv_caches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_initialize_kv_caches\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize the KV cache in the worker(s).\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m    The workers will determine the number of blocks in both the GPU cache\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m    and the swap CPU cache.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     num_gpu_blocks, num_cpu_blocks \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 353\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetermine_num_available_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks_override \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m         num_gpu_blocks_override \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks_override\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/executor/distributed_gpu_executor.py:38\u001b[0m, in \u001b[0;36mDistributedGPUExecutor.determine_num_available_blocks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Determine the number of available KV blocks.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mThis invokes `determine_num_available_blocks` on each worker and takes\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    - tuple[num_gpu_blocks, num_cpu_blocks]\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Get the maximum number of blocks that can be allocated on GPU and CPU.\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m num_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdetermine_num_available_blocks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Since we use a shared centralized controller, we take the minimum\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# number of blocks across all workers to make sure all the memory\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# operators can be applied to all workers.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m num_gpu_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(b[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m num_blocks)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/executor/multiproc_gpu_executor.py:130\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._run_workers\u001b[0;34m(self, method, async_run_tensor_parallel_workers_only, max_concurrent_workers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m worker_outputs\n\u001b[1;32m    129\u001b[0m driver_worker_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker, method)\n\u001b[0;32m--> 130\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m \u001b[43mdriver_worker_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Get the results of the workers.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [driver_worker_output\n\u001b[1;32m    134\u001b[0m         ] \u001b[38;5;241m+\u001b[39m [output\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m worker_outputs]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/worker.py:173\u001b[0m, in \u001b[0;36mWorker.determine_num_available_blocks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Execute a forward pass with dummy inputs to profile the memory usage\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# of the model.\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofile_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Calculate the number of blocks that can be allocated with the\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# profiled peak memory.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/model_runner.py:874\u001b[0m, in \u001b[0;36mGPUModelRunnerBase.profile_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_first_rank:\n\u001b[1;32m    870\u001b[0m     intermediate_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmake_empty_intermediate_tensors(\n\u001b[1;32m    871\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    872\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    873\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/model_runner.py:1243\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, model_input, kv_caches, intermediate_tensors, num_steps)\u001b[0m\n\u001b[1;32m   1238\u001b[0m multi_modal_kwargs \u001b[38;5;241m=\u001b[39m model_input\u001b[38;5;241m.\u001b[39mmulti_modal_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m   1239\u001b[0m seqlen_agnostic_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinished_requests_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_input\u001b[38;5;241m.\u001b[39mfinished_requests_ids,\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest_ids_to_seq_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_input\u001b[38;5;241m.\u001b[39mrequest_ids_to_seq_ids,\n\u001b[1;32m   1242\u001b[0m } \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_seqlen_agnostic \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m-> 1243\u001b[0m hidden_or_intermediate_states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_executable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_positions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmulti_modal_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mseqlen_agnostic_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;66;03m# Compute the logits in the last pipeline stage.\u001b[39;00m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py:401\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    395\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m     intermediate_tensors: Optional[IntermediateTensors] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    400\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[torch\u001b[38;5;241m.\u001b[39mTensor, IntermediateTensors]:\n\u001b[0;32m--> 401\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_output\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py:306\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors, inputs_embeds)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_layer):\n\u001b[1;32m    305\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\n\u001b[0;32m--> 306\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m IntermediateTensors({\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: hidden_states,\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresidual\u001b[39m\u001b[38;5;124m\"\u001b[39m: residual\n\u001b[1;32m    318\u001b[0m     })\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py:230\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, positions, hidden_states, kv_cache, attn_metadata, residual)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(\n\u001b[1;32m    229\u001b[0m         hidden_states, residual)\n\u001b[0;32m--> 230\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    238\u001b[0m hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(\n\u001b[1;32m    239\u001b[0m     hidden_states, residual)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/models/llama.py:161\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, positions, hidden_states, kv_cache, attn_metadata)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    156\u001b[0m     positions: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m     attn_metadata: AttentionMetadata,\n\u001b[1;32m    160\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 161\u001b[0m     qkv, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqkv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39msplit([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_size], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    163\u001b[0m     q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(positions, q, k)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py:317\u001b[0m, in \u001b[0;36mColumnParallelLinear.forward\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Matrix multiply.\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m output_parallel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_output:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# All-gather across the partitions.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     output \u001b[38;5;241m=\u001b[39m tensor_model_parallel_all_gather(output_parallel)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/fp8.py:370\u001b[0m, in \u001b[0;36mFp8LinearMethod.apply\u001b[0;34m(self, layer, x, bias)\u001b[0m\n\u001b[1;32m    362\u001b[0m         qinput, x_scale \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mscaled_fp8_quant(x,\n\u001b[1;32m    363\u001b[0m                                                layer\u001b[38;5;241m.\u001b[39minput_scale,\n\u001b[1;32m    364\u001b[0m                                                batch_dim_padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m17\u001b[39m)\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;66;03m# Fused GEMM_DQ -- note we padded the input above because\u001b[39;00m\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;66;03m# torch._scaled_mm is more performant for matrices with\u001b[39;00m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;66;03m# batch dimension > 16. Note that this could change\u001b[39;00m\n\u001b[1;32m    369\u001b[0m         \u001b[38;5;66;03m# in the future.\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m         output, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scaled_mm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m            \u001b[49m\u001b[43mqinput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m            \u001b[49m\u001b[43mout_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscale_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscale_b\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnarrow(output, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_NOT_SUPPORTED when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Ddesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)`"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "model = LLM(model=\"Meta-Llama-3-8B-Instruct-FP8-Dynamic/\", tensor_parallel_size=4, max_model_len=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.generate(\"Hello, my name is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute in Shell\n",
    "#%%sh\n",
    "#python -m vllm.entrypoints.openai.api_server --model dwetzel/Llama-3-8B-Instruct_Orce_plus --tensor-parallel-size 4 --enable-chunked-prefill --dtype auto --quantization fp8 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
