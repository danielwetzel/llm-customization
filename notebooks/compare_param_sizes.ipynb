{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T11:40:34.232029Z","iopub.status.busy":"2024-06-03T11:40:34.231563Z","iopub.status.idle":"2024-06-03T11:40:49.921593Z","shell.execute_reply":"2024-06-03T11:40:49.920655Z","shell.execute_reply.started":"2024-06-03T11:40:34.231990Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-07-31 19:02:09.916449: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-31 19:02:09.930038: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-31 19:02:09.934262: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-07-31 19:02:09.945476: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-07-31 19:02:10.790118: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["\n","import torch\n","from transformers import  AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n","import os\n","from codecarbon import EmissionsTracker\n","from time import time\n","import csv\n","from vllm import LLM, SamplingParams\n","from openai import OpenAI\n","\n","import wandb\n","import gc"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["env: WANDB_LOG_MODEL=true\n","env: WANDB_WATCH=all\n"]}],"source":["%env WANDB_LOG_MODEL=true\n","%env WANDB_WATCH=all"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 4 GPUs\n"]}],"source":["# Show active GPUs - pyTorch Stores the number of GPUs once Cuda ist firstly initialized. Therefore this needs to be executed on first startup\n","if torch.cuda.is_available():\n","    num_gpus = torch.cuda.device_count()\n","    print(f\"Found {num_gpus} GPUs\")\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T11:40:49.984360Z","iopub.status.busy":"2024-06-03T11:40:49.983352Z","iopub.status.idle":"2024-06-03T11:40:50.017338Z","shell.execute_reply":"2024-06-03T11:40:50.016585Z","shell.execute_reply.started":"2024-06-03T11:40:49.984314Z"},"trusted":true},"outputs":[],"source":["#model_name = \"meta-llama/CodeLlama-7b-Instruct-hf\"\n","model_name = \"meta-llama/CodeLlama-13b-Instruct-hf\"\n","#model_name = \"meta-llama/CodeLlama-34b-Instruct-hf\"\n","#model_name = \"meta-llama/CodeLlama-70b-Instruct-hf\""]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["class bcolors:\n","    HEADER = '\\033[95m'\n","    OKBLUE = '\\033[94m'\n","    OKCYAN = '\\033[96m'\n","    OKGREEN = '\\033[92m'\n","    WARNING = '\\033[93m'\n","    FAIL = '\\033[91m'\n","    ENDC = '\\033[0m'\n","    BOLD = '\\033[1m'\n","    UNDERLINE = '\\033[4m'\n","    CBLACKBG  = '\\33[40m'\n","    CREDBG    = '\\33[41m'\n","    CGREENBG  = '\\33[42m'\n","    CYELLOWBG = '\\33[43m'\n","    CBLUEBG   = '\\33[44m'\n","    CVIOLETBG = '\\33[45m'\n","    CBEIGEBG  = '\\33[46m'\n","    CWHITEBG  = '\\33[47m'\n","    CBLACK  = '\\33[30m'\n","    CRED    = '\\33[31m'\n","    CGREEN  = '\\33[32m'\n","    CYELLOW = '\\33[33m'\n","    CBLUE   = '\\33[34m'\n","    CVIOLET = '\\33[35m'\n","    CBEIGE  = '\\33[36m'\n","    CWHITE  = '\\33[37m'"]},{"cell_type":"markdown","metadata":{},"source":["# Preparing the Test Data"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T11:42:29.765981Z","iopub.status.busy":"2024-06-03T11:42:29.765640Z","iopub.status.idle":"2024-06-03T11:42:29.771881Z","shell.execute_reply":"2024-06-03T11:42:29.771019Z","shell.execute_reply.started":"2024-06-03T11:42:29.765956Z"},"trusted":true},"outputs":[],"source":["# List of words that should be used to create sentences.\n","\n","words = [\n","    \"apple\", \"book\", \"car\", \"dog\", \"elephant\", \"forest\", \"guitar\", \"house\", \n","    \"island\", \"jacket\", \"kangaroo\", \"lamp\", \"mountain\", \"notebook\", \"ocean\", \n","    \"pencil\", \"queen\", \"river\", \"star\", \"tree\", \"umbrella\", \"village\", \n","    \"window\", \"xylophone\", \"yacht\", \"zebra\", \"balloon\", \"camera\", \"desert\", \n","    \"engine\", \"flower\", \"garden\", \"honey\", \"iceberg\", \"jungle\", \"kite\", \n","    \"ladder\", \"moon\", \"nest\", \"octopus\", \"pirate\", \"quilt\", \"robot\", \"swan\", \n","    \"telescope\", \"unicorn\", \"violin\", \"whale\", \"x-ray\", \"laptop\"\n","]"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T11:42:29.775722Z","iopub.status.busy":"2024-06-03T11:42:29.775401Z","iopub.status.idle":"2024-06-03T11:42:29.786294Z","shell.execute_reply":"2024-06-03T11:42:29.785432Z","shell.execute_reply.started":"2024-06-03T11:42:29.775697Z"},"trusted":true},"outputs":[],"source":["# List of examples to be provided in the system prompt\n","\n","example_sentences = [\n","    \"Why did the bicycle fall over? Because it was two-tired!\",\n","    \"Why don't scientists trust atoms? Because they make up everything!\",\n","    \"Why did the scarecrow win an award? Because he was outstanding in his field!\",\n","    \"Why don't skeletons fight each other? They don't have the guts!\",\n","    \"Why did the computer go to the doctor? Because it had a virus!\",\n","    \"Why was the math book sad? Because it had too many problems!\",\n","    \"Why did the coffee file a police report? It got mugged!\",\n","    \"Why did the tomato turn red? Because it saw the salad dressing!\",\n","    \"Why don't eggs tell jokes? They might crack up!\",\n","    \"Why did the golfer bring two pairs of pants? In case he got a hole in one!\",\n","    \"Why do cows wear bells? Because their horns don't work!\",\n","    \"Why don't some couples go to the gym? Because some relationships don't work out!\",\n","    \"Why did the photo go to jail? It was framed!\",\n","    \"Why don't programmers like nature? It has too many bugs!\",\n","    \"Why did the bicycle stand up by itself? It was two-tired!\",\n","    \"Why did the music teacher need a ladder? To reach the high notes!\",\n","    \"Why did the cookie go to the doctor? Because it felt crummy!\",\n","    \"Why did the student eat his homework? Because his teacher told him it was a piece of cake!\",\n","    \"Why don't oysters donate to charity? Because they are shellfish!\",\n","    \"Why did the broom get a promotion? Because it swept the competition!\",\n","    \"Why don't we see elephants hiding in trees? Because they are so good at it!\",\n","    \"Why did the fish blush? Because it saw the ocean's bottom!\",\n","    \"Why did the barber win the race? Because he knew all the shortcuts!\",\n","    \"Why did the banana go to the doctor? Because it wasn't peeling well!\",\n","    \"Why don't dinosaurs talk? Because they are extinct!\",\n","    \"Why did the clock go back to school? To learn about time management!\",\n","    \"Why did the farmer win an award? Because he was outstanding in his field!\",\n","    \"Why did the astronaut break up with his girlfriend? He needed space!\",\n","    \"Why did the shoe go to the party alone? Because it didn't want to be a pair!\",\n","    \"Why did the tree go to the dentist? To get its roots checked!\",\n","    \"Why did the calendar go on a diet? It wanted to lose some days!\",\n","    \"Why did the stadium get hot? All the fans left!\",\n","    \"Why did the belt go to jail? It held up a pair of pants!\",\n","    \"Why did the cookie cry? Because its mom was a wafer too long!\",\n","    \"Why did the cow jump over the moon? To get to the milky way!\",\n","    \"Why did the skeleton go to the party alone? He had no body to go with!\",\n","    \"Why did the grape stop in the middle of the road? Because it ran out of juice!\",\n","    \"Why did the bee get married? Because he found his honey!\",\n","    \"Why did the soccer ball quit the team? It was tired of being kicked around!\",\n","    \"Why did the traffic light turn red? You would too if you had to change in the middle of the street!\",\n","    \"Why did the scarecrow become a successful neurosurgeon? He was outstanding in his field!\",\n","    \"Why did the chicken cross the playground? To get to the other slide!\",\n","    \"Why did the belt get a promotion? It was a cinch!\",\n","    \"Why did the teacher wear sunglasses? Because her students were so bright!\",\n","    \"Why did the clock get kicked out of class? It was tocking too much!\",\n","    \"Why did the frog call his insurance company? He had a jump in his car!\",\n","    \"Why did the keyboard get a speeding ticket? It had a problem with the space bar!\",\n","    \"Why did the painting go to art school? It wanted to brush up on its skills!\",\n","    \"Why did the scientist install a knocker on his door? He wanted to win the No-bell prize!\",\n","    \"Why did the tomato turn green? Because it was embarrassed to ketchup!\",\n","    \"Why did the pencil go to jail? It was caught in a sketchy situation!\",\n","    \"Why did the music note need a loan? It needed some major funding!\",\n","    \"Why did the fisherman put peanut butter into the sea? To go with the jellyfish!\",\n","    \"Why did the bicycle bring a map? Because it didn't want to get lost on its wheel-y big adventure!\",\n","    \"Why did the pizza go to the party? It wanted to slice up the dance floor!\",\n","    \"Why did the phone sit on a bench? It wanted to recharge its batteries!\",\n","    \"Why did the blanket get arrested? It was covering up a crime!\",\n","    \"Why did the chef go to jail? Because he beat the eggs and whipped the cream!\",\n","    \"Why did the sandwich go to the beach? To get a little bologna-sun!\",\n","    \"Why did the banker switch careers? He lost interest!\",\n","    \"Why did the shovel go to therapy? It had too much dirt on its mind!\",\n","    \"Why did the soccer player bring string to the game? So he could tie the score!\",\n","    \"Why did the alarm clock break up with the pillow? It couldn't handle the pressure!\",\n","    \"Why did the frog take the bus to work? His car got toad away!\",\n","    \"Why did the dentist become a gardener? He wanted to brush up on his roots!\",\n","    \"Why did the light bulb fail his test? He wasn't too bright!\",\n","    \"Why did the ocean break up with the shore? It needed some space to tide things over!\",\n","    \"Why did the shoe store close down? It lost its sole!\",\n","    \"Why did the banana go out with the prune? Because it couldn't find a date!\",\n","    \"Why did the carpenter become a musician? He wanted to nail every note!\"\n","]\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def prepare_prompts(word_list, runs=1, num_examples=1, multiply_by=1):\n","    system_message = f\"\"\"\n","    You are an AI assistant designed to write python code that prints out a number of short jokes.\n","    Generate python code that prints out {num_examples*multiply_by} short joke(s) that are exactly 2 short sentences long. \n","    Do not include any follow-up questions or explanations.\n","\n","    For example, a joke for the word \"yogurt\" would look like this:\n","    'Why did the yogurt go to the art gallery? Because it wanted to be cultured!'\n","\n","    Here are a few further examples: \n","    - 'Why did the bicycle fall over? Because it was two-tired!'\n","    - 'Why don't scientists trust atoms? Because they make up everything!'\n","\n","    \n","    Be as close as possible to the example jokes!\n","    Respond with only the code to print out the {num_examples*multiply_by} short joke(s). Do not include any introductory phrases.\n","\n","    \"\"\"\n","\n","    prompts = []\n","    \n","    for r in range(runs):\n","\n","        for word in word_list:\n","            user_message = \"Question: \" + f'Now, tell me {num_examples*multiply_by} short joke(s) for the word: \"{word}\"\\n' + \" Answer:\"\n","            \n","            messages = [\n","                {\"role\": \"system\", \"content\": system_message},\n","                {\"role\": \"user\", \"content\": user_message},\n","            ]\n","            prompt = tokenizer.apply_chat_template(\n","                messages, \n","                tokenize=False, \n","                add_generation_prompt=True\n","            )\n","\n","            prompts.append(prompt)\n","    \n","    \n","    return prompts"]},{"cell_type":"markdown","metadata":{},"source":["# vLLM"]},{"cell_type":"markdown","metadata":{},"source":["## Creating the Model"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Load the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.bos_token"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO 07-31 19:03:00 config.py:715] Defaulting to use mp for distributed inference\n","INFO 07-31 19:03:00 config.py:806] Chunked prefill is enabled with max_num_batched_tokens=512.\n","INFO 07-31 19:03:00 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='meta-llama/CodeLlama-13b-Instruct-hf', speculative_config=None, tokenizer='meta-llama/CodeLlama-13b-Instruct-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/CodeLlama-13b-Instruct-hf, use_v2_block_manager=False, enable_prefix_caching=False)\n"]},{"name":"stdout","output_type":"stream","text":["INFO 07-31 19:03:00 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n"]},{"ename":"RuntimeError","evalue":"The number of CUDA devices has changed since the first call to torch.cuda.device_count(). This is not allowed and may result in undefined behavior. Please check out https://github.com/vllm-project/vllm/issues/6056 to find the first call to torch.cuda.device_count() and defer it until the engine is up. Or you can set CUDA_VISIBLE_DEVICES to the GPUs you want to use.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create an LLM.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m          \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbfloat16\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m          \u001b[49m\u001b[43menable_chunked_prefill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m          \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/entrypoints/llm.py:155\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is no need to pass vision-related arguments anymore.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    135\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    136\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    154\u001b[0m )\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/engine/llm_engine.py:441\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    439\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/engine/llm_engine.py:251\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, multimodal_config, speculative_config, decoding_config, observability_config, prompt_adapter_config, executor_class, log_stats, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config_fields \u001b[38;5;241m=\u001b[39m _load_generation_config_dict(\n\u001b[1;32m    246\u001b[0m     model_config)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_processor \u001b[38;5;241m=\u001b[39m INPUT_REGISTRY\u001b[38;5;241m.\u001b[39mcreate_input_processor(\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config)\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultimodal_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultimodal_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeculative_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeculative_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_adapter_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_adapter_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39membedding_mode:\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/executor/distributed_gpu_executor.py:25\u001b[0m, in \u001b[0;36mDistributedGPUExecutor.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Updated by implementations that require additional args to be passed\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# to the _run_workers execute_model call\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_execute_model_run_workers_kwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/executor/executor_base.py:47\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, multimodal_config, speculative_config, prompt_adapter_config)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeculative_config \u001b[38;5;241m=\u001b[39m speculative_config\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_adapter_config \u001b[38;5;241m=\u001b[39m prompt_adapter_config\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/executor/multiproc_gpu_executor.py:66\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m tensor_parallel_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m cuda_device_count, (\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease set tensor_parallel_size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor_parallel_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto less than max local gpu count (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcuda_device_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m world_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m cuda_device_count, (\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease ensure that world_size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworld_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis less than than max local gpu count (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcuda_device_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m \u001b[43merror_on_invalid_device_count_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Multiprocessing-based executor does not support multi-node setting.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Since it only works for single node, we can use the loopback address\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# 127.0.0.1 for communication.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m distributed_init_method \u001b[38;5;241m=\u001b[39m get_distributed_init_method(\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m127.0.0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m, get_open_port())\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/utils.py:895\u001b[0m, in \u001b[0;36merror_on_invalid_device_count_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    893\u001b[0m current \u001b[38;5;241m=\u001b[39m cuda_device_count_stateless()\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remembered \u001b[38;5;241m>\u001b[39m current:\n\u001b[0;32m--> 895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    896\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of CUDA devices has changed since the first \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    897\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall to torch.cuda.device_count(). This is not allowed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    898\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand may result in undefined behavior. Please check out \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    899\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/vllm-project/vllm/issues/6056 to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfind the first call to torch.cuda.device_count() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    901\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand defer it until the engine is up. Or you can set \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    902\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES to the GPUs you want to use.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mRuntimeError\u001b[0m: The number of CUDA devices has changed since the first call to torch.cuda.device_count(). This is not allowed and may result in undefined behavior. Please check out https://github.com/vllm-project/vllm/issues/6056 to find the first call to torch.cuda.device_count() and defer it until the engine is up. Or you can set CUDA_VISIBLE_DEVICES to the GPUs you want to use."]}],"source":["# Create an LLM.\n","llm = LLM(model=model_name,\n","          tensor_parallel_size=2, \n","          dtype='bfloat16',\n","          enable_chunked_prefill=True,\n","          max_model_len=2048,\n","          gpu_memory_utilization=0.9,\n","          )"]},{"cell_type":"markdown","metadata":{},"source":["## Testing the Model"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def query_model_vllm(prompt_list, temperature=0.8, min_p=0.05, max_length=500):\n","\n","    # Create a sampling params object.\n","    sampling_params = SamplingParams(temperature=temperature, min_p=min_p, max_tokens=max_length)\n","\n","    # Start Timer for Inference\n","    start_time = time()\n","\n","    outputs = llm.generate(prompt_list, sampling_params)\n","\n","    # End Timer for Inference\n","    end_time = time()\n","\n","    ttime = end_time-start_time\n","\n","    return outputs, ttime"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["========== INFERENCE TEST with meta-llama/CodeLlama-7b-Instruct-hf==========\n","\n","\n","Starting Test with 150 Runs and 50 Prompts / Run. \n","\n","Total Prompts: 7500\n","\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdaniel-wetzel\u001b[0m (\u001b[33mllm-emissions\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.17.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/ubuntu/llm-customization/notebooks/wandb/run-20240731_183504-3fvi7cyx</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/llm-emissions/Inference_Params_Comp/runs/3fvi7cyx' target=\"_blank\">vLLM_meta-llama/CodeLlama-7b-Instruct-hf_4GPUs</a></strong> to <a href='https://wandb.ai/llm-emissions/Inference_Params_Comp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/llm-emissions/Inference_Params_Comp' target=\"_blank\">https://wandb.ai/llm-emissions/Inference_Params_Comp</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/llm-emissions/Inference_Params_Comp/runs/3fvi7cyx' target=\"_blank\">https://wandb.ai/llm-emissions/Inference_Params_Comp/runs/3fvi7cyx</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 7500/7500 [15:34<00:00,  8.02it/s, est. speed input: 2053.99 toks/s, output: 1059.52 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["=============== RESULTS for vLLM_meta-llama/CodeLlama-7b-Instruct-hf_4GPUs ===============\n","\n","\n","    Finished 150 Runs with 50 Prompts/Run.\n","\n","\n","    Total Time: 936.63s, AVG/Prompt: 124.88ms\n","\n","\n","    Average tokens per second: 1057.41\n","\n","\n","    Total Prompts: 7500\n","\n","    Total Input Tokens: 1920000, AVG/Prompt: 256.0\n","\n","    Total Output Tokens: 990403, AVG/Prompt: 132.05373333333333\n","\n","    --------------------------------------------------\n","\n","    Total Inference Emissions: 0.076kg CO₂eq\n","\n","\n","    Emissions / 1.000.000 Input Tokens: 0.039kg CO₂eq\n","\n","    Emissions / 1.000.000 Output Tokens: 0.076kg CO₂eq\n","\n","    Emissions / 10.000 Prompts: 0.101kg CO₂eq\n","\n","\n","    \n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0f5ccf1c6cf7407a8ab46d71be2d73ca","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AVG. Input Tokens</td><td>▁</td></tr><tr><td>AVG. Output Tokens</td><td>▁</td></tr><tr><td>AVG. Time / Prompt</td><td>▁</td></tr><tr><td>AVG. Tokens / Second</td><td>▁</td></tr><tr><td>Emissions / 1.000.000 Input Tokens</td><td>▁</td></tr><tr><td>Emissions / 1.000.000 Output Tokens</td><td>▁</td></tr><tr><td>Emissions / 10.000 Prompts</td><td>▁</td></tr><tr><td>Total Emissions</td><td>▁</td></tr><tr><td>Total Time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AVG. Input Tokens</td><td>256.0</td></tr><tr><td>AVG. Output Tokens</td><td>132.05373</td></tr><tr><td>AVG. Time / Prompt</td><td>124.884</td></tr><tr><td>AVG. Tokens / Second</td><td>1057.41115</td></tr><tr><td>Emissions / 1.000.000 Input Tokens</td><td>0.0394</td></tr><tr><td>Emissions / 1.000.000 Output Tokens</td><td>0.07638</td></tr><tr><td>Emissions / 10.000 Prompts</td><td>0.10086</td></tr><tr><td>Total Emissions</td><td>0.07564</td></tr><tr><td>Total Time</td><td>936.62999</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">vLLM_meta-llama/CodeLlama-7b-Instruct-hf_4GPUs</strong> at: <a href='https://wandb.ai/llm-emissions/Inference_Params_Comp/runs/3fvi7cyx' target=\"_blank\">https://wandb.ai/llm-emissions/Inference_Params_Comp/runs/3fvi7cyx</a><br/> View project at: <a href='https://wandb.ai/llm-emissions/Inference_Params_Comp' target=\"_blank\">https://wandb.ai/llm-emissions/Inference_Params_Comp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240731_183504-3fvi7cyx/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Results saved to emission_data/vLLM_meta-llama/CodeLlama-7b-Instruct-hf_4GPUs_emission_data.csv\n","\n","\n"]}],"source":["runs = 150\n","num_prompts = len(words)\n","total_prompts = runs * num_prompts\n","\n","\n","total_input_tok = 0\n","total_output_tok = 0\n","\n","print(\"=\"*10 + f\" INFERENCE TEST with {model_name}\" + \"=\"*10 + \n","\"\\n\\n\" + \n","f\"\"\"\n","Starting Test with {runs} Runs and {num_prompts} Prompts / Run. \\n\n","Total Prompts: {total_prompts}\\n\\n\n","\"\"\")\n","\n","name=f\"vLLM_{model_name}_4GPUs\"\n","\n","prompts = prepare_prompts(words, runs=runs, num_examples=5, multiply_by=1)\n","\n","\n","wandb.init(\n","    # set the wandb project where this run will be logged\n","    project=\"Inference_Params_Comp\",\n","\n","    # track hyperparameters and run metadata\n","    config={\n","    \"runs\": runs,\n","    \"num_prompts\": num_prompts,\n","    \"total_prompts\": total_prompts,\n","    \"framework\": 'vLLM',\n","    \"model\": model_name,\n","    \"num_gpus\": 4,\n","    },\n","\n","    name=name,\n",")\n","\n","tracker = EmissionsTracker(save_to_file=True, project_name=f\"{name}\", log_level=\"error\", pue = 1.22, output_file=f\"emissions_params.csv\")\n","tracker.start()\n","\n","\n","outputs, ttime = query_model_vllm(prompts, max_length=100*5)\n","\n","emissions: float = tracker.stop()\n","\n","\n","\n","for output in outputs: \n","\n","\n","    # Extracting information\n","    prompt = output.prompt\n","    generated_text = output.outputs[0].text\n","    input_tokens = output.prompt_token_ids\n","    output_tokens = output.outputs[0].token_ids\n","    num_input_tokens = len(input_tokens)\n","    num_output_tokens = len(output_tokens)\n","\n","    # Updating cumulative counts\n","    total_input_tok += num_input_tokens\n","    total_output_tok += num_output_tokens\n","\n","\n","# Calculate averages\n","avg_time_per_prompt = (ttime / total_prompts)*1000\n","avg_toks_per_sec = total_output_tok/ttime\n","avg_input_tokens = total_input_tok / total_prompts\n","avg_output_tokens = total_output_tok / total_prompts\n","\n","em_i = emissions/total_input_tok *1_000_000\n","em_o = emissions/total_output_tok *1_000_000\n","em_p = emissions/total_prompts *10_000\n","\n","print(\"=\"*15 + f\" RESULTS for {name} \" + \"=\"*15 + \n","    \"\\n\\n\" + \n","    f\"\"\"\n","    Finished {runs} Runs with {num_prompts} Prompts/Run.\\n\\n\n","    Total Time: {ttime:.2f}s, AVG/Prompt: {avg_time_per_prompt:.2f}ms\\n\\n\n","    Average tokens per second: {avg_toks_per_sec:.2f}\\n\\n\n","    Total Prompts: {total_prompts}\\n\n","    Total Input Tokens: {total_input_tok}, AVG/Prompt: {avg_input_tokens}\\n\n","    Total Output Tokens: {total_output_tok}, AVG/Prompt: {avg_output_tokens}\\n\n","    \"\"\" + \n","    \n","    \"-\"*50 + \"\\n\" +\n","    \n","    f\"\"\"\n","    Total Inference Emissions: {emissions:.3f}kg CO₂eq\\n\\n\n","    Emissions / 1.000.000 Input Tokens: {em_i:.3f}kg CO₂eq\\n\n","    Emissions / 1.000.000 Output Tokens: {em_o:.3f}kg CO₂eq\\n\n","    Emissions / 10.000 Prompts: {em_p:.3f}kg CO₂eq\\n\n","\n","    \"\"\"\n","    )\n","\n","wandb.log({\"Total Time\": ttime,\n","    \"AVG. Time / Prompt\": avg_time_per_prompt,\n","            \"AVG. Tokens / Second\": avg_toks_per_sec,\n","            \"AVG. Input Tokens\": avg_input_tokens,\n","            \"AVG. Output Tokens\": avg_output_tokens,\n","            \"Total Emissions\": emissions,\n","            \"Emissions / 1.000.000 Input Tokens\": em_i,\n","            \"Emissions / 1.000.000 Output Tokens\": em_o,\n","            \"Emissions / 10.000 Prompts\": em_p,\n","            })\n","\n","wandb.finish()\n","\n","# Save results to a CSV file\n","results = [\n","    [\"Runs\", runs],\n","    [\"Prompts / Run\", num_prompts],\n","    [\"Total Prompts\", total_prompts],\n","    [\"Total Time\", ttime], \n","    [\"AVG. Time / Prompt\", avg_time_per_prompt],\n","    [\"AVG. Tokens / Second\", avg_toks_per_sec],\n","    [\"Total Input Tokens\", total_input_tok],\n","    [\"AVG. Input Tokens / Prompt\", avg_input_tokens],\n","    [\"Total Output Tokens\", total_output_tok],\n","    [\"AVG. Output Tokens / Prompt\", avg_output_tokens],\n","    [\"Total Emissions\", emissions],\n","    [\"Emissions / 1.000.000 Input Tokens\", em_i],\n","    [\"Emissions / 1.000.000 Output Tokens\", em_o],\n","    [\"Emissions / 10.000 Prompts\", em_p]\n","]\n","\n","# Ensure the directory exists\n","output_file_path = f\"emission_data/{name}_emission_data.csv\"\n","os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n","\n","with open(output_file_path, 'w', newline='') as f:\n","    writer = csv.writer(f)\n","    writer.writerow([\"Metric\", \"Value\"])\n","    writer.writerows(results)\n","\n","print(f\"Results saved to {output_file_path}\\n\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["# Note: Idle Performance"]},{"cell_type":"markdown","metadata":{},"source":["- In idle each L4 GPU needs about 27W to store its maximum capacity in VRAM. \n","- In full idle with empty VRAM each L4 needs about 16W"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4880117,"sourceId":8230136,"sourceType":"datasetVersion"},{"modelInstanceId":28079,"sourceId":33547,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":28083,"sourceId":33551,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30716,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
