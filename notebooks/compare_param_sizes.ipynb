{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T11:40:34.232029Z","iopub.status.busy":"2024-06-03T11:40:34.231563Z","iopub.status.idle":"2024-06-03T11:40:49.921593Z","shell.execute_reply":"2024-06-03T11:40:49.920655Z","shell.execute_reply.started":"2024-06-03T11:40:34.231990Z"},"trusted":true},"outputs":[],"source":["\n","import torch\n","from transformers import  AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n","\n","from codecarbon import EmissionsTracker\n","from time import time\n","import csv\n","from vllm import LLM, SamplingParams\n","from openai import OpenAI\n","\n","import wandb\n","import gc"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["env: WANDB_LOG_MODEL=true\n","env: WANDB_WATCH=all\n"]}],"source":["%env WANDB_LOG_MODEL=true\n","%env WANDB_WATCH=all"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 2 GPUs\n"]}],"source":["# Show active GPUs - pyTorch Stores the number of GPUs once Cuda ist firstly initialized. Therefore this needs to be executed on first startup\n","if torch.cuda.is_available():\n","    num_gpus = torch.cuda.device_count()\n","    print(f\"Found {num_gpus} GPUs\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T11:40:49.984360Z","iopub.status.busy":"2024-06-03T11:40:49.983352Z","iopub.status.idle":"2024-06-03T11:40:50.017338Z","shell.execute_reply":"2024-06-03T11:40:50.016585Z","shell.execute_reply.started":"2024-06-03T11:40:49.984314Z"},"trusted":true},"outputs":[],"source":["#model_name = \"meta-llama/CodeLlama-7b-Instruct-hf\"\n","model_name = \"meta-llama/CodeLlama-13b-Instruct-hf\"\n","#model_name = \"meta-llama/CodeLlama-34b-Instruct-hf\"\n","#model_name = \"meta-llama/CodeLlama-70b-Instruct-hf\""]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["class bcolors:\n","    HEADER = '\\033[95m'\n","    OKBLUE = '\\033[94m'\n","    OKCYAN = '\\033[96m'\n","    OKGREEN = '\\033[92m'\n","    WARNING = '\\033[93m'\n","    FAIL = '\\033[91m'\n","    ENDC = '\\033[0m'\n","    BOLD = '\\033[1m'\n","    UNDERLINE = '\\033[4m'\n","    CBLACKBG  = '\\33[40m'\n","    CREDBG    = '\\33[41m'\n","    CGREENBG  = '\\33[42m'\n","    CYELLOWBG = '\\33[43m'\n","    CBLUEBG   = '\\33[44m'\n","    CVIOLETBG = '\\33[45m'\n","    CBEIGEBG  = '\\33[46m'\n","    CWHITEBG  = '\\33[47m'\n","    CBLACK  = '\\33[30m'\n","    CRED    = '\\33[31m'\n","    CGREEN  = '\\33[32m'\n","    CYELLOW = '\\33[33m'\n","    CBLUE   = '\\33[34m'\n","    CVIOLET = '\\33[35m'\n","    CBEIGE  = '\\33[36m'\n","    CWHITE  = '\\33[37m'"]},{"cell_type":"markdown","metadata":{},"source":["# Preparing the Test Data"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T11:42:29.765981Z","iopub.status.busy":"2024-06-03T11:42:29.765640Z","iopub.status.idle":"2024-06-03T11:42:29.771881Z","shell.execute_reply":"2024-06-03T11:42:29.771019Z","shell.execute_reply.started":"2024-06-03T11:42:29.765956Z"},"trusted":true},"outputs":[],"source":["# List of words that should be used to create sentences.\n","\n","words = [\n","    \"apple\", \"book\", \"car\", \"dog\", \"elephant\", \"forest\", \"guitar\", \"house\", \n","    \"island\", \"jacket\", \"kangaroo\", \"lamp\", \"mountain\", \"notebook\", \"ocean\", \n","    \"pencil\", \"queen\", \"river\", \"star\", \"tree\", \"umbrella\", \"village\", \n","    \"window\", \"xylophone\", \"yacht\", \"zebra\", \"balloon\", \"camera\", \"desert\", \n","    \"engine\", \"flower\", \"garden\", \"honey\", \"iceberg\", \"jungle\", \"kite\", \n","    \"ladder\", \"moon\", \"nest\", \"octopus\", \"pirate\", \"quilt\", \"robot\", \"swan\", \n","    \"telescope\", \"unicorn\", \"violin\", \"whale\", \"x-ray\", \"laptop\"\n","]"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T11:42:29.775722Z","iopub.status.busy":"2024-06-03T11:42:29.775401Z","iopub.status.idle":"2024-06-03T11:42:29.786294Z","shell.execute_reply":"2024-06-03T11:42:29.785432Z","shell.execute_reply.started":"2024-06-03T11:42:29.775697Z"},"trusted":true},"outputs":[],"source":["# List of examples to be provided in the system prompt\n","\n","example_sentences = [\n","    \"Why did the bicycle fall over? Because it was two-tired!\",\n","    \"Why don't scientists trust atoms? Because they make up everything!\",\n","    \"Why did the scarecrow win an award? Because he was outstanding in his field!\",\n","    \"Why don't skeletons fight each other? They don't have the guts!\",\n","    \"Why did the computer go to the doctor? Because it had a virus!\",\n","    \"Why was the math book sad? Because it had too many problems!\",\n","    \"Why did the coffee file a police report? It got mugged!\",\n","    \"Why did the tomato turn red? Because it saw the salad dressing!\",\n","    \"Why don't eggs tell jokes? They might crack up!\",\n","    \"Why did the golfer bring two pairs of pants? In case he got a hole in one!\",\n","    \"Why do cows wear bells? Because their horns don't work!\",\n","    \"Why don't some couples go to the gym? Because some relationships don't work out!\",\n","    \"Why did the photo go to jail? It was framed!\",\n","    \"Why don't programmers like nature? It has too many bugs!\",\n","    \"Why did the bicycle stand up by itself? It was two-tired!\",\n","    \"Why did the music teacher need a ladder? To reach the high notes!\",\n","    \"Why did the cookie go to the doctor? Because it felt crummy!\",\n","    \"Why did the student eat his homework? Because his teacher told him it was a piece of cake!\",\n","    \"Why don't oysters donate to charity? Because they are shellfish!\",\n","    \"Why did the broom get a promotion? Because it swept the competition!\",\n","    \"Why don't we see elephants hiding in trees? Because they are so good at it!\",\n","    \"Why did the fish blush? Because it saw the ocean's bottom!\",\n","    \"Why did the barber win the race? Because he knew all the shortcuts!\",\n","    \"Why did the banana go to the doctor? Because it wasn't peeling well!\",\n","    \"Why don't dinosaurs talk? Because they are extinct!\",\n","    \"Why did the clock go back to school? To learn about time management!\",\n","    \"Why did the farmer win an award? Because he was outstanding in his field!\",\n","    \"Why did the astronaut break up with his girlfriend? He needed space!\",\n","    \"Why did the shoe go to the party alone? Because it didn't want to be a pair!\",\n","    \"Why did the tree go to the dentist? To get its roots checked!\",\n","    \"Why did the calendar go on a diet? It wanted to lose some days!\",\n","    \"Why did the stadium get hot? All the fans left!\",\n","    \"Why did the belt go to jail? It held up a pair of pants!\",\n","    \"Why did the cookie cry? Because its mom was a wafer too long!\",\n","    \"Why did the cow jump over the moon? To get to the milky way!\",\n","    \"Why did the skeleton go to the party alone? He had no body to go with!\",\n","    \"Why did the grape stop in the middle of the road? Because it ran out of juice!\",\n","    \"Why did the bee get married? Because he found his honey!\",\n","    \"Why did the soccer ball quit the team? It was tired of being kicked around!\",\n","    \"Why did the traffic light turn red? You would too if you had to change in the middle of the street!\",\n","    \"Why did the scarecrow become a successful neurosurgeon? He was outstanding in his field!\",\n","    \"Why did the chicken cross the playground? To get to the other slide!\",\n","    \"Why did the belt get a promotion? It was a cinch!\",\n","    \"Why did the teacher wear sunglasses? Because her students were so bright!\",\n","    \"Why did the clock get kicked out of class? It was tocking too much!\",\n","    \"Why did the frog call his insurance company? He had a jump in his car!\",\n","    \"Why did the keyboard get a speeding ticket? It had a problem with the space bar!\",\n","    \"Why did the painting go to art school? It wanted to brush up on its skills!\",\n","    \"Why did the scientist install a knocker on his door? He wanted to win the No-bell prize!\",\n","    \"Why did the tomato turn green? Because it was embarrassed to ketchup!\",\n","    \"Why did the pencil go to jail? It was caught in a sketchy situation!\",\n","    \"Why did the music note need a loan? It needed some major funding!\",\n","    \"Why did the fisherman put peanut butter into the sea? To go with the jellyfish!\",\n","    \"Why did the bicycle bring a map? Because it didn't want to get lost on its wheel-y big adventure!\",\n","    \"Why did the pizza go to the party? It wanted to slice up the dance floor!\",\n","    \"Why did the phone sit on a bench? It wanted to recharge its batteries!\",\n","    \"Why did the blanket get arrested? It was covering up a crime!\",\n","    \"Why did the chef go to jail? Because he beat the eggs and whipped the cream!\",\n","    \"Why did the sandwich go to the beach? To get a little bologna-sun!\",\n","    \"Why did the banker switch careers? He lost interest!\",\n","    \"Why did the shovel go to therapy? It had too much dirt on its mind!\",\n","    \"Why did the soccer player bring string to the game? So he could tie the score!\",\n","    \"Why did the alarm clock break up with the pillow? It couldn't handle the pressure!\",\n","    \"Why did the frog take the bus to work? His car got toad away!\",\n","    \"Why did the dentist become a gardener? He wanted to brush up on his roots!\",\n","    \"Why did the light bulb fail his test? He wasn't too bright!\",\n","    \"Why did the ocean break up with the shore? It needed some space to tide things over!\",\n","    \"Why did the shoe store close down? It lost its sole!\",\n","    \"Why did the banana go out with the prune? Because it couldn't find a date!\",\n","    \"Why did the carpenter become a musician? He wanted to nail every note!\"\n","]\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def prepare_prompts(word_list, runs=1, num_examples=1, multiply_by=1):\n","    system_message = f\"\"\"\n","    You are an AI assistant designed to write python code that prints out a number of short jokes.\n","    Generate python code that prints out {num_examples*multiply_by} short joke(s) that are exactly 2 short sentences long. \n","    Do not include any follow-up questions or explanations.\n","\n","    For example, a joke for the word \"yogurt\" would look like this:\n","    'Why did the yogurt go to the art gallery? Because it wanted to be cultured!'\n","\n","    Here are a few further examples: \n","    - 'Why did the bicycle fall over? Because it was two-tired!'\n","    - 'Why don't scientists trust atoms? Because they make up everything!'\n","\n","    \n","    Be as close as possible to the example jokes!\n","    Respond with only the code to print out the {num_examples*multiply_by} short joke(s). Do not include any introductory phrases.\n","\n","    \"\"\"\n","\n","    prompts = []\n","    \n","    for r in range(runs):\n","\n","        for word in word_list:\n","            user_message = \"Question: \" + f'Now, tell me {num_examples*multiply_by} short joke(s) for the word: \"{word}\"\\n' + \" Answer:\"\n","            \n","            messages = [\n","                {\"role\": \"system\", \"content\": system_message},\n","                {\"role\": \"user\", \"content\": user_message},\n","            ]\n","            prompt = tokenizer.apply_chat_template(\n","                messages, \n","                tokenize=False, \n","                add_generation_prompt=True\n","            )\n","\n","            prompts.append(prompt)\n","    \n","    \n","    return prompts"]},{"cell_type":"markdown","metadata":{},"source":["# vLLM"]},{"cell_type":"markdown","metadata":{},"source":["## Creating the Model"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e2b7128b0f3748009ca3a6eb95a95c8a","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.59k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ef4f0e82d4ab4d03855068f761394a21","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6d948bc0b2f4150963423466fd8e052","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4c2a5519e41419b9beee8f757ffb050","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Load the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.bos_token"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fab8a4ec42d1458792f22721bf25fd65","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/589 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["INFO 07-31 19:28:23 config.py:715] Defaulting to use mp for distributed inference\n","INFO 07-31 19:28:23 config.py:806] Chunked prefill is enabled with max_num_batched_tokens=512.\n","INFO 07-31 19:28:23 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='meta-llama/CodeLlama-13b-Instruct-hf', speculative_config=None, tokenizer='meta-llama/CodeLlama-13b-Instruct-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/CodeLlama-13b-Instruct-hf, use_v2_block_manager=False, enable_prefix_caching=False)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9805e7cfe02e4d7a83c3271a603cdc59","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["INFO 07-31 19:28:23 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n","\u001b[1;36m(VllmWorkerProcess pid=34477)\u001b[0;0m INFO 07-31 19:28:23 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n","\u001b[1;36m(VllmWorkerProcess pid=34477)\u001b[0;0m ERROR 07-31 19:28:23 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method init_device: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method, Traceback (most recent call last):\n","\u001b[1;36m(VllmWorkerProcess pid=34477)\u001b[0;0m ERROR 07-31 19:28:23 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n","\u001b[1;36m(VllmWorkerProcess pid=34477)\u001b[0;0m ERROR 07-31 19:28:23 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n","\u001b[1;36m(VllmWorkerProcess pid=34477)\u001b[0;0m ERROR 07-31 19:28:23 multiproc_worker_utils.py:226]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n","\u001b[1;36m(VllmWorkerProcess pid=34477)\u001b[0;0m ERROR 07-31 19:28:23 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/vllm/worker/worker.py\", line 123, in init_device\n","\u001b[1;36m(VllmWorkerProcess pid=34477)\u001b[0;0m ERROR 07-31 19:28:23 multiproc_worker_utils.py:226]     torch.cuda.set_device(self.device)\n","\u001b[1;36m(VllmWorkerProcess pid=34477)\u001b[0;0m ERROR 07-31 19:28:23 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 399, in set_device\n","\u001b[1;36m(VllmWorkerProcess pid=34477)\u001b[0;0m ERROR 07-31 19:28:23 multiproc_worker_utils.py:226]     torch._C._cuda_setDevice(device)\n","\u001b[1;36m(VllmWorkerProcess pid=34477)\u001b[0;0m ERROR 07-31 19:28:23 multiproc_worker_utils.py:226]   File \"/opt/conda/envs/pytorch/lib/python3.11/site-packages/torch/cuda/__init__.py\", line 279, in _lazy_init\n","\u001b[1;36m(VllmWorkerProcess pid=34477)\u001b[0;0m ERROR 07-31 19:28:23 multiproc_worker_utils.py:226]     raise RuntimeError(\n","\u001b[1;36m(VllmWorkerProcess pid=34477)\u001b[0;0m ERROR 07-31 19:28:23 multiproc_worker_utils.py:226] RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n","\u001b[1;36m(VllmWorkerProcess pid=34477)\u001b[0;0m ERROR 07-31 19:28:23 multiproc_worker_utils.py:226] \n"]}],"source":["# Create an LLM.\n","llm = LLM(model=model_name,\n","          tensor_parallel_size=2, \n","          dtype='bfloat16',\n","          enable_chunked_prefill=True,\n","          max_model_len=2048,\n","          gpu_memory_utilization=0.9,\n","          )"]},{"cell_type":"markdown","metadata":{},"source":["## Testing the Model"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def query_model_vllm(prompt_list, temperature=0.8, min_p=0.05, max_length=500):\n","\n","    # Create a sampling params object.\n","    sampling_params = SamplingParams(temperature=temperature, min_p=min_p, max_tokens=max_length)\n","\n","    # Start Timer for Inference\n","    start_time = time()\n","\n","    outputs = llm.generate(prompt_list, sampling_params)\n","\n","    # End Timer for Inference\n","    end_time = time()\n","\n","    ttime = end_time-start_time\n","\n","    return outputs, ttime"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["========== INFERENCE TEST with meta-llama/CodeLlama-7b-Instruct-hf==========\n","\n","\n","Starting Test with 150 Runs and 50 Prompts / Run. \n","\n","Total Prompts: 7500\n","\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdaniel-wetzel\u001b[0m (\u001b[33mllm-emissions\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.17.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/ubuntu/llm-customization/notebooks/wandb/run-20240731_183504-3fvi7cyx</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/llm-emissions/Inference_Params_Comp/runs/3fvi7cyx' target=\"_blank\">vLLM_meta-llama/CodeLlama-7b-Instruct-hf_4GPUs</a></strong> to <a href='https://wandb.ai/llm-emissions/Inference_Params_Comp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/llm-emissions/Inference_Params_Comp' target=\"_blank\">https://wandb.ai/llm-emissions/Inference_Params_Comp</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/llm-emissions/Inference_Params_Comp/runs/3fvi7cyx' target=\"_blank\">https://wandb.ai/llm-emissions/Inference_Params_Comp/runs/3fvi7cyx</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 7500/7500 [15:34<00:00,  8.02it/s, est. speed input: 2053.99 toks/s, output: 1059.52 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["=============== RESULTS for vLLM_meta-llama/CodeLlama-7b-Instruct-hf_4GPUs ===============\n","\n","\n","    Finished 150 Runs with 50 Prompts/Run.\n","\n","\n","    Total Time: 936.63s, AVG/Prompt: 124.88ms\n","\n","\n","    Average tokens per second: 1057.41\n","\n","\n","    Total Prompts: 7500\n","\n","    Total Input Tokens: 1920000, AVG/Prompt: 256.0\n","\n","    Total Output Tokens: 990403, AVG/Prompt: 132.05373333333333\n","\n","    --------------------------------------------------\n","\n","    Total Inference Emissions: 0.076kg CO₂eq\n","\n","\n","    Emissions / 1.000.000 Input Tokens: 0.039kg CO₂eq\n","\n","    Emissions / 1.000.000 Output Tokens: 0.076kg CO₂eq\n","\n","    Emissions / 10.000 Prompts: 0.101kg CO₂eq\n","\n","\n","    \n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0f5ccf1c6cf7407a8ab46d71be2d73ca","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AVG. Input Tokens</td><td>▁</td></tr><tr><td>AVG. Output Tokens</td><td>▁</td></tr><tr><td>AVG. Time / Prompt</td><td>▁</td></tr><tr><td>AVG. Tokens / Second</td><td>▁</td></tr><tr><td>Emissions / 1.000.000 Input Tokens</td><td>▁</td></tr><tr><td>Emissions / 1.000.000 Output Tokens</td><td>▁</td></tr><tr><td>Emissions / 10.000 Prompts</td><td>▁</td></tr><tr><td>Total Emissions</td><td>▁</td></tr><tr><td>Total Time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AVG. Input Tokens</td><td>256.0</td></tr><tr><td>AVG. Output Tokens</td><td>132.05373</td></tr><tr><td>AVG. Time / Prompt</td><td>124.884</td></tr><tr><td>AVG. Tokens / Second</td><td>1057.41115</td></tr><tr><td>Emissions / 1.000.000 Input Tokens</td><td>0.0394</td></tr><tr><td>Emissions / 1.000.000 Output Tokens</td><td>0.07638</td></tr><tr><td>Emissions / 10.000 Prompts</td><td>0.10086</td></tr><tr><td>Total Emissions</td><td>0.07564</td></tr><tr><td>Total Time</td><td>936.62999</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">vLLM_meta-llama/CodeLlama-7b-Instruct-hf_4GPUs</strong> at: <a href='https://wandb.ai/llm-emissions/Inference_Params_Comp/runs/3fvi7cyx' target=\"_blank\">https://wandb.ai/llm-emissions/Inference_Params_Comp/runs/3fvi7cyx</a><br/> View project at: <a href='https://wandb.ai/llm-emissions/Inference_Params_Comp' target=\"_blank\">https://wandb.ai/llm-emissions/Inference_Params_Comp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240731_183504-3fvi7cyx/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Results saved to emission_data/vLLM_meta-llama/CodeLlama-7b-Instruct-hf_4GPUs_emission_data.csv\n","\n","\n"]}],"source":["runs = 150\n","num_prompts = len(words)\n","total_prompts = runs * num_prompts\n","\n","\n","total_input_tok = 0\n","total_output_tok = 0\n","\n","print(\"=\"*10 + f\" INFERENCE TEST with {model_name}\" + \"=\"*10 + \n","\"\\n\\n\" + \n","f\"\"\"\n","Starting Test with {runs} Runs and {num_prompts} Prompts / Run. \\n\n","Total Prompts: {total_prompts}\\n\\n\n","\"\"\")\n","\n","name=f\"vLLM_{model_name}_4GPUs\"\n","\n","prompts = prepare_prompts(words, runs=runs, num_examples=5, multiply_by=1)\n","\n","\n","wandb.init(\n","    # set the wandb project where this run will be logged\n","    project=\"Inference_Params_Comp\",\n","\n","    # track hyperparameters and run metadata\n","    config={\n","    \"runs\": runs,\n","    \"num_prompts\": num_prompts,\n","    \"total_prompts\": total_prompts,\n","    \"framework\": 'vLLM',\n","    \"model\": model_name,\n","    \"num_gpus\": 4,\n","    },\n","\n","    name=name,\n",")\n","\n","tracker = EmissionsTracker(save_to_file=True, project_name=f\"{name}\", log_level=\"error\", pue = 1.22, output_file=f\"emissions_params.csv\")\n","tracker.start()\n","\n","\n","outputs, ttime = query_model_vllm(prompts, max_length=100*5)\n","\n","emissions: float = tracker.stop()\n","\n","\n","\n","for output in outputs: \n","\n","\n","    # Extracting information\n","    prompt = output.prompt\n","    generated_text = output.outputs[0].text\n","    input_tokens = output.prompt_token_ids\n","    output_tokens = output.outputs[0].token_ids\n","    num_input_tokens = len(input_tokens)\n","    num_output_tokens = len(output_tokens)\n","\n","    # Updating cumulative counts\n","    total_input_tok += num_input_tokens\n","    total_output_tok += num_output_tokens\n","\n","\n","# Calculate averages\n","avg_time_per_prompt = (ttime / total_prompts)*1000\n","avg_toks_per_sec = total_output_tok/ttime\n","avg_input_tokens = total_input_tok / total_prompts\n","avg_output_tokens = total_output_tok / total_prompts\n","\n","em_i = emissions/total_input_tok *1_000_000\n","em_o = emissions/total_output_tok *1_000_000\n","em_p = emissions/total_prompts *10_000\n","\n","print(\"=\"*15 + f\" RESULTS for {name} \" + \"=\"*15 + \n","    \"\\n\\n\" + \n","    f\"\"\"\n","    Finished {runs} Runs with {num_prompts} Prompts/Run.\\n\\n\n","    Total Time: {ttime:.2f}s, AVG/Prompt: {avg_time_per_prompt:.2f}ms\\n\\n\n","    Average tokens per second: {avg_toks_per_sec:.2f}\\n\\n\n","    Total Prompts: {total_prompts}\\n\n","    Total Input Tokens: {total_input_tok}, AVG/Prompt: {avg_input_tokens}\\n\n","    Total Output Tokens: {total_output_tok}, AVG/Prompt: {avg_output_tokens}\\n\n","    \"\"\" + \n","    \n","    \"-\"*50 + \"\\n\" +\n","    \n","    f\"\"\"\n","    Total Inference Emissions: {emissions:.3f}kg CO₂eq\\n\\n\n","    Emissions / 1.000.000 Input Tokens: {em_i:.3f}kg CO₂eq\\n\n","    Emissions / 1.000.000 Output Tokens: {em_o:.3f}kg CO₂eq\\n\n","    Emissions / 10.000 Prompts: {em_p:.3f}kg CO₂eq\\n\n","\n","    \"\"\"\n","    )\n","\n","wandb.log({\"Total Time\": ttime,\n","    \"AVG. Time / Prompt\": avg_time_per_prompt,\n","            \"AVG. Tokens / Second\": avg_toks_per_sec,\n","            \"AVG. Input Tokens\": avg_input_tokens,\n","            \"AVG. Output Tokens\": avg_output_tokens,\n","            \"Total Emissions\": emissions,\n","            \"Emissions / 1.000.000 Input Tokens\": em_i,\n","            \"Emissions / 1.000.000 Output Tokens\": em_o,\n","            \"Emissions / 10.000 Prompts\": em_p,\n","            })\n","\n","wandb.finish()\n","\n","# Save results to a CSV file\n","results = [\n","    [\"Runs\", runs],\n","    [\"Prompts / Run\", num_prompts],\n","    [\"Total Prompts\", total_prompts],\n","    [\"Total Time\", ttime], \n","    [\"AVG. Time / Prompt\", avg_time_per_prompt],\n","    [\"AVG. Tokens / Second\", avg_toks_per_sec],\n","    [\"Total Input Tokens\", total_input_tok],\n","    [\"AVG. Input Tokens / Prompt\", avg_input_tokens],\n","    [\"Total Output Tokens\", total_output_tok],\n","    [\"AVG. Output Tokens / Prompt\", avg_output_tokens],\n","    [\"Total Emissions\", emissions],\n","    [\"Emissions / 1.000.000 Input Tokens\", em_i],\n","    [\"Emissions / 1.000.000 Output Tokens\", em_o],\n","    [\"Emissions / 10.000 Prompts\", em_p]\n","]\n","\n","# Ensure the directory exists\n","output_file_path = f\"emission_data/{name}_emission_data.csv\"\n","os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n","\n","with open(output_file_path, 'w', newline='') as f:\n","    writer = csv.writer(f)\n","    writer.writerow([\"Metric\", \"Value\"])\n","    writer.writerows(results)\n","\n","print(f\"Results saved to {output_file_path}\\n\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["# Note: Idle Performance"]},{"cell_type":"markdown","metadata":{},"source":["- In idle each L4 GPU needs about 27W to store its maximum capacity in VRAM. \n","- In full idle with empty VRAM each L4 needs about 16W"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4880117,"sourceId":8230136,"sourceType":"datasetVersion"},{"modelInstanceId":28079,"sourceId":33547,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":28083,"sourceId":33551,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30716,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
