{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T11:40:34.232029Z","iopub.status.busy":"2024-06-03T11:40:34.231563Z","iopub.status.idle":"2024-06-03T11:40:49.921593Z","shell.execute_reply":"2024-06-03T11:40:49.920655Z","shell.execute_reply.started":"2024-06-03T11:40:34.231990Z"},"trusted":true},"outputs":[],"source":["\n","import torch\n","from transformers import  AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n","import os\n","from codecarbon import EmissionsTracker\n","from time import time\n","import csv\n","from vllm import LLM, SamplingParams\n","from openai import OpenAI\n","\n","import wandb\n","import gc"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["env: WANDB_LOG_MODEL=true\n","env: WANDB_WATCH=all\n"]}],"source":["%env WANDB_LOG_MODEL=true\n","%env WANDB_WATCH=all"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T11:40:49.984360Z","iopub.status.busy":"2024-06-03T11:40:49.983352Z","iopub.status.idle":"2024-06-03T11:40:50.017338Z","shell.execute_reply":"2024-06-03T11:40:50.016585Z","shell.execute_reply.started":"2024-06-03T11:40:49.984314Z"},"trusted":true},"outputs":[],"source":["model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class bcolors:\n","    HEADER = '\\033[95m'\n","    OKBLUE = '\\033[94m'\n","    OKCYAN = '\\033[96m'\n","    OKGREEN = '\\033[92m'\n","    WARNING = '\\033[93m'\n","    FAIL = '\\033[91m'\n","    ENDC = '\\033[0m'\n","    BOLD = '\\033[1m'\n","    UNDERLINE = '\\033[4m'\n","    CBLACKBG  = '\\33[40m'\n","    CREDBG    = '\\33[41m'\n","    CGREENBG  = '\\33[42m'\n","    CYELLOWBG = '\\33[43m'\n","    CBLUEBG   = '\\33[44m'\n","    CVIOLETBG = '\\33[45m'\n","    CBEIGEBG  = '\\33[46m'\n","    CWHITEBG  = '\\33[47m'\n","    CBLACK  = '\\33[30m'\n","    CRED    = '\\33[31m'\n","    CGREEN  = '\\33[32m'\n","    CYELLOW = '\\33[33m'\n","    CBLUE   = '\\33[34m'\n","    CVIOLET = '\\33[35m'\n","    CBEIGE  = '\\33[36m'\n","    CWHITE  = '\\33[37m'"]},{"cell_type":"markdown","metadata":{},"source":["# Preparing the Test Data"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T11:42:29.765981Z","iopub.status.busy":"2024-06-03T11:42:29.765640Z","iopub.status.idle":"2024-06-03T11:42:29.771881Z","shell.execute_reply":"2024-06-03T11:42:29.771019Z","shell.execute_reply.started":"2024-06-03T11:42:29.765956Z"},"trusted":true},"outputs":[],"source":["# List of words that should be used to create sentences.\n","\n","words = [\n","    \"apple\", \"book\", \"car\", \"dog\", \"elephant\", \"forest\", \"guitar\", \"house\", \n","    \"island\", \"jacket\", \"kangaroo\", \"lamp\", \"mountain\", \"notebook\", \"ocean\", \n","    \"pencil\", \"queen\", \"river\", \"star\", \"tree\", \"umbrella\", \"village\", \n","    \"window\", \"xylophone\", \"yacht\", \"zebra\", \"balloon\", \"camera\", \"desert\", \n","    \"engine\", \"flower\", \"garden\", \"honey\", \"iceberg\", \"jungle\", \"kite\", \n","    \"ladder\", \"moon\", \"nest\", \"octopus\", \"pirate\", \"quilt\", \"robot\", \"swan\", \n","    \"telescope\", \"unicorn\", \"violin\", \"whale\", \"x-ray\", \"laptop\"\n","]"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-03T11:42:29.775722Z","iopub.status.busy":"2024-06-03T11:42:29.775401Z","iopub.status.idle":"2024-06-03T11:42:29.786294Z","shell.execute_reply":"2024-06-03T11:42:29.785432Z","shell.execute_reply.started":"2024-06-03T11:42:29.775697Z"},"trusted":true},"outputs":[],"source":["# List of examples to be provided in the system prompt\n","\n","example_sentences = [\n","    \"Why did the bicycle fall over? Because it was two-tired!\",\n","    \"Why don't scientists trust atoms? Because they make up everything!\",\n","    \"Why did the scarecrow win an award? Because he was outstanding in his field!\",\n","    \"Why don't skeletons fight each other? They don't have the guts!\",\n","    \"Why did the computer go to the doctor? Because it had a virus!\",\n","    \"Why was the math book sad? Because it had too many problems!\",\n","    \"Why did the coffee file a police report? It got mugged!\",\n","    \"Why did the tomato turn red? Because it saw the salad dressing!\",\n","    \"Why don't eggs tell jokes? They might crack up!\",\n","    \"Why did the golfer bring two pairs of pants? In case he got a hole in one!\",\n","    \"Why do cows wear bells? Because their horns don't work!\",\n","    \"Why don't some couples go to the gym? Because some relationships don't work out!\",\n","    \"Why did the photo go to jail? It was framed!\",\n","    \"Why don't programmers like nature? It has too many bugs!\",\n","    \"Why did the bicycle stand up by itself? It was two-tired!\",\n","    \"Why did the music teacher need a ladder? To reach the high notes!\",\n","    \"Why did the cookie go to the doctor? Because it felt crummy!\",\n","    \"Why did the student eat his homework? Because his teacher told him it was a piece of cake!\",\n","    \"Why don't oysters donate to charity? Because they are shellfish!\",\n","    \"Why did the broom get a promotion? Because it swept the competition!\",\n","    \"Why don't we see elephants hiding in trees? Because they are so good at it!\",\n","    \"Why did the fish blush? Because it saw the ocean's bottom!\",\n","    \"Why did the barber win the race? Because he knew all the shortcuts!\",\n","    \"Why did the banana go to the doctor? Because it wasn't peeling well!\",\n","    \"Why don't dinosaurs talk? Because they are extinct!\",\n","    \"Why did the clock go back to school? To learn about time management!\",\n","    \"Why did the farmer win an award? Because he was outstanding in his field!\",\n","    \"Why did the astronaut break up with his girlfriend? He needed space!\",\n","    \"Why did the shoe go to the party alone? Because it didn't want to be a pair!\",\n","    \"Why did the tree go to the dentist? To get its roots checked!\",\n","    \"Why did the calendar go on a diet? It wanted to lose some days!\",\n","    \"Why did the stadium get hot? All the fans left!\",\n","    \"Why did the belt go to jail? It held up a pair of pants!\",\n","    \"Why did the cookie cry? Because its mom was a wafer too long!\",\n","    \"Why did the cow jump over the moon? To get to the milky way!\",\n","    \"Why did the skeleton go to the party alone? He had no body to go with!\",\n","    \"Why did the grape stop in the middle of the road? Because it ran out of juice!\",\n","    \"Why did the bee get married? Because he found his honey!\",\n","    \"Why did the soccer ball quit the team? It was tired of being kicked around!\",\n","    \"Why did the traffic light turn red? You would too if you had to change in the middle of the street!\",\n","    \"Why did the scarecrow become a successful neurosurgeon? He was outstanding in his field!\",\n","    \"Why did the chicken cross the playground? To get to the other slide!\",\n","    \"Why did the belt get a promotion? It was a cinch!\",\n","    \"Why did the teacher wear sunglasses? Because her students were so bright!\",\n","    \"Why did the clock get kicked out of class? It was tocking too much!\",\n","    \"Why did the frog call his insurance company? He had a jump in his car!\",\n","    \"Why did the keyboard get a speeding ticket? It had a problem with the space bar!\",\n","    \"Why did the painting go to art school? It wanted to brush up on its skills!\",\n","    \"Why did the scientist install a knocker on his door? He wanted to win the No-bell prize!\",\n","    \"Why did the tomato turn green? Because it was embarrassed to ketchup!\",\n","    \"Why did the pencil go to jail? It was caught in a sketchy situation!\",\n","    \"Why did the music note need a loan? It needed some major funding!\",\n","    \"Why did the fisherman put peanut butter into the sea? To go with the jellyfish!\",\n","    \"Why did the bicycle bring a map? Because it didn't want to get lost on its wheel-y big adventure!\",\n","    \"Why did the pizza go to the party? It wanted to slice up the dance floor!\",\n","    \"Why did the phone sit on a bench? It wanted to recharge its batteries!\",\n","    \"Why did the blanket get arrested? It was covering up a crime!\",\n","    \"Why did the chef go to jail? Because he beat the eggs and whipped the cream!\",\n","    \"Why did the sandwich go to the beach? To get a little bologna-sun!\",\n","    \"Why did the banker switch careers? He lost interest!\",\n","    \"Why did the shovel go to therapy? It had too much dirt on its mind!\",\n","    \"Why did the soccer player bring string to the game? So he could tie the score!\",\n","    \"Why did the alarm clock break up with the pillow? It couldn't handle the pressure!\",\n","    \"Why did the frog take the bus to work? His car got toad away!\",\n","    \"Why did the dentist become a gardener? He wanted to brush up on his roots!\",\n","    \"Why did the light bulb fail his test? He wasn't too bright!\",\n","    \"Why did the ocean break up with the shore? It needed some space to tide things over!\",\n","    \"Why did the shoe store close down? It lost its sole!\",\n","    \"Why did the banana go out with the prune? Because it couldn't find a date!\",\n","    \"Why did the carpenter become a musician? He wanted to nail every note!\"\n","]\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def prepare_prompts(word_list, runs=1, num_examples=1, multiply_by=1):\n","    system_message = f\"\"\"\n","    You are an AI assistant designed to tell jokes for a given word.\n","    Generate {num_examples*multiply_by} short joke(s) that are exactly 2 short sentences long. Do not include any follow-up questions or explanations.\n","\n","    For example, a joke for the word \"yogurt\":\n","    'Why did the yogurt go to the art gallery? Because it wanted to be cultured!'\n","\n","    Here are a few further examples: \n","    - 'Why did the bicycle fall over? Because it was two-tired!'\n","    - 'Why don't scientists trust atoms? Because they make up everything!'\n","\n","    \n","    Be as close as possible to the example jokes!\n","    Respond with only the {num_examples*multiply_by} joke(s) itself. Do not include any introductory phrases.\n","\n","    \"\"\"\n","\n","    prompts = []\n","    \n","    for r in range(runs):\n","\n","        for word in word_list:\n","            user_message = \"Question: \" + f'Now, tell me {num_examples*multiply_by} short joke(s) for the word: \"{word}\"\\n' + \" Answer:\"\n","            \n","            messages = [\n","                {\"role\": \"system\", \"content\": system_message},\n","                {\"role\": \"user\", \"content\": user_message},\n","            ]\n","            prompt = tokenizer.apply_chat_template(\n","                messages, \n","                tokenize=False, \n","                add_generation_prompt=True\n","            )\n","\n","            prompts.append(prompt)\n","    \n","    \n","    return prompts"]},{"cell_type":"markdown","metadata":{},"source":["# vLLM"]},{"cell_type":"markdown","metadata":{},"source":["## Creating the Model"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["# Load the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.bos_token"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/envs/pytorch/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"name":"stderr","output_type":"stream","text":["2024-07-11 16:44:44,922\tINFO worker.py:1788 -- Started a local Ray instance.\n"]},{"name":"stdout","output_type":"stream","text":["INFO 07-11 16:44:45 config.py:623] Defaulting to use mp for distributed inference\n","INFO 07-11 16:44:45 config.py:707] Chunked prefill is enabled (EXPERIMENTAL).\n","INFO 07-11 16:44:45 llm_engine.py:161] Initializing an LLM engine (v0.5.0.post1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\n"]},{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1;36m(VllmWorkerProcess pid=58628)\u001b[0;0m INFO 07-11 16:44:49 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n","\u001b[1;36m(VllmWorkerProcess pid=58629)\u001b[0;0m INFO 07-11 16:44:49 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n","\u001b[1;36m(VllmWorkerProcess pid=58630)\u001b[0;0m INFO 07-11 16:44:49 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n","INFO 07-11 16:44:50 utils.py:637] Found nccl from library libnccl.so.2\n","INFO 07-11 16:44:50 pynccl.py:63] vLLM is using nccl==2.21.5\n","\u001b[1;36m(VllmWorkerProcess pid=58629)\u001b[0;0m INFO 07-11 16:44:50 utils.py:637] Found nccl from library libnccl.so.2\n","\u001b[1;36m(VllmWorkerProcess pid=58628)\u001b[0;0m INFO 07-11 16:44:50 utils.py:637] Found nccl from library libnccl.so.2\n","\u001b[1;36m(VllmWorkerProcess pid=58630)\u001b[0;0m INFO 07-11 16:44:50 utils.py:637] Found nccl from library libnccl.so.2\n","\u001b[1;36m(VllmWorkerProcess pid=58629)\u001b[0;0m INFO 07-11 16:44:50 pynccl.py:63] vLLM is using nccl==2.21.5\n","\u001b[1;36m(VllmWorkerProcess pid=58628)\u001b[0;0m INFO 07-11 16:44:50 pynccl.py:63] vLLM is using nccl==2.21.5\n","\u001b[1;36m(VllmWorkerProcess pid=58630)\u001b[0;0m INFO 07-11 16:44:50 pynccl.py:63] vLLM is using nccl==2.21.5\n","WARNING 07-11 16:44:50 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n","\u001b[1;36m(VllmWorkerProcess pid=58630)\u001b[0;0m WARNING 07-11 16:44:50 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n","\u001b[1;36m(VllmWorkerProcess pid=58629)\u001b[0;0m WARNING 07-11 16:44:50 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n","\u001b[1;36m(VllmWorkerProcess pid=58628)\u001b[0;0m WARNING 07-11 16:44:50 custom_all_reduce.py:166] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n","\u001b[1;36m(VllmWorkerProcess pid=58628)\u001b[0;0m INFO 07-11 16:44:50 weight_utils.py:218] Using model weights format ['*.safetensors']\n","\u001b[1;36m(VllmWorkerProcess pid=58629)\u001b[0;0m INFO 07-11 16:44:50 weight_utils.py:218] Using model weights format ['*.safetensors']\n","INFO 07-11 16:44:50 weight_utils.py:218] Using model weights format ['*.safetensors']\n","\u001b[1;36m(VllmWorkerProcess pid=58630)\u001b[0;0m INFO 07-11 16:44:50 weight_utils.py:218] Using model weights format ['*.safetensors']\n"]},{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/opt/conda/envs/pytorch/lib/python3.11/multiprocessing/resource_tracker.py\", line 239, in main\n","    cache[rtype].remove(name)\n","KeyError: '/psm_3edf60cb'\n","Traceback (most recent call last):\n","  File \"/opt/conda/envs/pytorch/lib/python3.11/multiprocessing/resource_tracker.py\", line 239, in main\n","    cache[rtype].remove(name)\n","KeyError: '/psm_3edf60cb'\n","Traceback (most recent call last):\n","  File \"/opt/conda/envs/pytorch/lib/python3.11/multiprocessing/resource_tracker.py\", line 239, in main\n","    cache[rtype].remove(name)\n","KeyError: '/psm_3edf60cb'\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1;36m(VllmWorkerProcess pid=58628)\u001b[0;0m INFO 07-11 16:44:51 model_runner.py:160] Loading model weights took 3.7417 GB\n","\u001b[1;36m(VllmWorkerProcess pid=58629)\u001b[0;0m INFO 07-11 16:44:52 model_runner.py:160] Loading model weights took 3.7417 GB\n","\u001b[1;36m(VllmWorkerProcess pid=58630)\u001b[0;0m INFO 07-11 16:44:52 model_runner.py:160] Loading model weights took 3.7417 GB\n","INFO 07-11 16:44:52 model_runner.py:160] Loading model weights took 3.7417 GB\n","INFO 07-11 16:44:53 distributed_gpu_executor.py:56] # GPU blocks: 30244, # CPU blocks: 8192\n","\u001b[1;36m(VllmWorkerProcess pid=58630)\u001b[0;0m INFO 07-11 16:44:58 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n","\u001b[1;36m(VllmWorkerProcess pid=58630)\u001b[0;0m INFO 07-11 16:44:58 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n","\u001b[1;36m(VllmWorkerProcess pid=58629)\u001b[0;0m INFO 07-11 16:44:59 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n","\u001b[1;36m(VllmWorkerProcess pid=58629)\u001b[0;0m INFO 07-11 16:44:59 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n","\u001b[1;36m(VllmWorkerProcess pid=58628)\u001b[0;0m INFO 07-11 16:44:59 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n","\u001b[1;36m(VllmWorkerProcess pid=58628)\u001b[0;0m INFO 07-11 16:44:59 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n","INFO 07-11 16:44:59 model_runner.py:889] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n","INFO 07-11 16:44:59 model_runner.py:893] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n","\u001b[1;36m(VllmWorkerProcess pid=58629)\u001b[0;0m INFO 07-11 16:45:13 model_runner.py:965] Graph capturing finished in 14 secs.\n","\u001b[1;36m(VllmWorkerProcess pid=58630)\u001b[0;0m INFO 07-11 16:45:13 model_runner.py:965] Graph capturing finished in 14 secs.\n","\u001b[1;36m(VllmWorkerProcess pid=58628)\u001b[0;0m INFO 07-11 16:45:13 model_runner.py:965] Graph capturing finished in 14 secs.\n","INFO 07-11 16:45:13 model_runner.py:965] Graph capturing finished in 14 secs.\n"]}],"source":["# Create an LLM.\n","llm = LLM(model=model_name,\n","          tensor_parallel_size=4, \n","          dtype='bfloat16',\n","          enable_chunked_prefill=True,\n","          )"]},{"cell_type":"markdown","metadata":{},"source":["## Testing the Model"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def query_model_vllm(prompt_list, temperature=0.8, min_p=0.05, max_length=48):\n","\n","    # Create a sampling params object.\n","    sampling_params = SamplingParams(temperature=temperature, min_p=min_p, max_tokens=max_length)\n","\n","    # Start Timer for Inference\n","    start_time = time()\n","\n","    outputs = llm.generate(prompt_list, sampling_params)\n","\n","    # End Timer for Inference\n","    end_time = time()\n","\n","    ttime = end_time-start_time\n","\n","    return outputs, ttime"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["========== INFERENCE TEST with 50 PROMPTS ==========\n","\n","\n","Starting Test with 150 Runs and 50 Prompts / Run. \n","\n","Total Prompts: 7500\n","\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdaniel-wetzel\u001b[0m (\u001b[33mllm-emissions\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.17.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/ubuntu/llm-customization/notebooks/wandb/run-20240711_164514-39huy0au</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/llm-emissions/Inference_Framework_Comparison/runs/39huy0au' target=\"_blank\">vLLM_Inference_7500_prompts</a></strong> to <a href='https://wandb.ai/llm-emissions/Inference_Framework_Comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/llm-emissions/Inference_Framework_Comparison' target=\"_blank\">https://wandb.ai/llm-emissions/Inference_Framework_Comparison</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/llm-emissions/Inference_Framework_Comparison/runs/39huy0au' target=\"_blank\">https://wandb.ai/llm-emissions/Inference_Framework_Comparison/runs/39huy0au</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|██████████| 7500/7500 [08:46<00:00, 14.24it/s, est. speed input: 2658.24 toks/s, output: 1019.30 toks/s]\n","/opt/conda/envs/pytorch/lib/python3.11/site-packages/codecarbon/output_methods/file.py:50: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  df = pd.concat([df, pd.DataFrame.from_records([dict(total.values)])])\n"]},{"name":"stdout","output_type":"stream","text":["=============== RESULTS for vLLM_Inference_7500_prompts ===============\n","\n","\n","    Finished 150 Runs with 50 Prompts/Run.\n","\n","\n","    Total Time: 529.64s, AVG/Prompt: 70.62ms\n","\n","\n","    Average tokens per second: 1013.53\n","\n","\n","    Total Prompts: 7500\n","\n","    Total Input Tokens: 1399950, AVG/Prompt: 186.66\n","\n","    Total Output Tokens: 536807, AVG/Prompt: 71.57426666666667\n","\n","    --------------------------------------------------\n","\n","    Total Inference Emissions: 0.041kg CO₂eq\n","\n","\n","    Emissions / 1.000.000 Input Tokens: 0.029kg CO₂eq\n","\n","    Emissions / 1.000.000 Output Tokens: 0.076kg CO₂eq\n","\n","    Emissions / 10.000 Prompts: 0.055kg CO₂eq\n","\n","\n","    \n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ff90496fc1094e0b8f8dd54402e1228a","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AVG. Input Tokens</td><td>▁</td></tr><tr><td>AVG. Output Tokens</td><td>▁</td></tr><tr><td>AVG. Time / Prompt</td><td>▁</td></tr><tr><td>AVG. Tokens / Second</td><td>▁</td></tr><tr><td>Emissions / 1.000.000 Input Tokens</td><td>▁</td></tr><tr><td>Emissions / 1.000.000 Output Tokens</td><td>▁</td></tr><tr><td>Emissions / 10.000 Prompts</td><td>▁</td></tr><tr><td>Total Emissions</td><td>▁</td></tr><tr><td>Total Time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AVG. Input Tokens</td><td>186.66</td></tr><tr><td>AVG. Output Tokens</td><td>71.57427</td></tr><tr><td>AVG. Time / Prompt</td><td>70.6189</td></tr><tr><td>AVG. Tokens / Second</td><td>1013.52848</td></tr><tr><td>Emissions / 1.000.000 Input Tokens</td><td>0.02921</td></tr><tr><td>Emissions / 1.000.000 Output Tokens</td><td>0.07619</td></tr><tr><td>Emissions / 10.000 Prompts</td><td>0.05453</td></tr><tr><td>Total Emissions</td><td>0.0409</td></tr><tr><td>Total Time</td><td>529.64175</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">vLLM_Inference_7500_prompts</strong> at: <a href='https://wandb.ai/llm-emissions/Inference_Framework_Comparison/runs/39huy0au' target=\"_blank\">https://wandb.ai/llm-emissions/Inference_Framework_Comparison/runs/39huy0au</a><br/> View project at: <a href='https://wandb.ai/llm-emissions/Inference_Framework_Comparison' target=\"_blank\">https://wandb.ai/llm-emissions/Inference_Framework_Comparison</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240711_164514-39huy0au/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Results saved to emission_data/vllm_meta-llama/Meta-Llama-3-8B-Instruct_emission_data.csv\n","\n","\n"]}],"source":["runs = 150\n","num_prompts = len(words)\n","total_prompts = runs * num_prompts\n","\n","\n","total_input_tok = 0\n","total_output_tok = 0\n","\n","print(\"=\"*10 + f\" INFERENCE TEST with {num_prompts} PROMPTS \" + \"=\"*10 + \n","\"\\n\\n\" + \n","f\"\"\"\n","Starting Test with {runs} Runs and {num_prompts} Prompts / Run. \\n\n","Total Prompts: {total_prompts}\\n\\n\n","\"\"\")\n","\n","name=f\"vLLM_Inference_{total_prompts}_prompts\"\n","\n","prompts = prepare_prompts(words, runs=runs, num_examples=5, multiply_by=1)\n","\n","\n","wandb.init(\n","    # set the wandb project where this run will be logged\n","    project=\"Inference_Framework_Comparison\",\n","\n","    # track hyperparameters and run metadata\n","    config={\n","    \"runs\": runs,\n","    \"num_prompts\": num_prompts,\n","    \"total_prompts\": total_prompts,\n","    \"framework\": 'vLLM',\n","    \"model\": model_name,\n","    },\n","\n","    name=name,\n",")\n","\n","tracker = EmissionsTracker(save_to_file=True, project_name=f\"{name}-llama3-8B\", log_level=\"error\", pue = 1.22, output_file=f\"emissions_vllm_transformers.csv\")\n","tracker.start()\n","\n","\n","outputs, ttime = query_model_vllm(prompts, max_length=48*5)\n","\n","emissions: float = tracker.stop()\n","\n","\n","\n","for output in outputs: \n","\n","\n","    # Extracting information\n","    prompt = output.prompt\n","    generated_text = output.outputs[0].text\n","    input_tokens = output.prompt_token_ids\n","    output_tokens = output.outputs[0].token_ids\n","    num_input_tokens = len(input_tokens)\n","    num_output_tokens = len(output_tokens)\n","\n","    # Updating cumulative counts\n","    total_input_tok += num_input_tokens\n","    total_output_tok += num_output_tokens\n","\n","\n","# Calculate averages\n","avg_time_per_prompt = (ttime / total_prompts)*1000\n","avg_toks_per_sec = total_output_tok/ttime\n","avg_input_tokens = total_input_tok / total_prompts\n","avg_output_tokens = total_output_tok / total_prompts\n","\n","em_i = emissions/total_input_tok *1_000_000\n","em_o = emissions/total_output_tok *1_000_000\n","em_p = emissions/total_prompts *10_000\n","\n","print(\"=\"*15 + f\" RESULTS for {name} \" + \"=\"*15 + \n","    \"\\n\\n\" + \n","    f\"\"\"\n","    Finished {runs} Runs with {num_prompts} Prompts/Run.\\n\\n\n","    Total Time: {ttime:.2f}s, AVG/Prompt: {avg_time_per_prompt:.2f}ms\\n\\n\n","    Average tokens per second: {avg_toks_per_sec:.2f}\\n\\n\n","    Total Prompts: {total_prompts}\\n\n","    Total Input Tokens: {total_input_tok}, AVG/Prompt: {avg_input_tokens}\\n\n","    Total Output Tokens: {total_output_tok}, AVG/Prompt: {avg_output_tokens}\\n\n","    \"\"\" + \n","    \n","    \"-\"*50 + \"\\n\" +\n","    \n","    f\"\"\"\n","    Total Inference Emissions: {emissions:.3f}kg CO₂eq\\n\\n\n","    Emissions / 1.000.000 Input Tokens: {em_i:.3f}kg CO₂eq\\n\n","    Emissions / 1.000.000 Output Tokens: {em_o:.3f}kg CO₂eq\\n\n","    Emissions / 10.000 Prompts: {em_p:.3f}kg CO₂eq\\n\n","\n","    \"\"\"\n","    )\n","\n","wandb.log({\"Total Time\": ttime,\n","    \"AVG. Time / Prompt\": avg_time_per_prompt,\n","            \"AVG. Tokens / Second\": avg_toks_per_sec,\n","            \"AVG. Input Tokens\": avg_input_tokens,\n","            \"AVG. Output Tokens\": avg_output_tokens,\n","            \"Total Emissions\": emissions,\n","            \"Emissions / 1.000.000 Input Tokens\": em_i,\n","            \"Emissions / 1.000.000 Output Tokens\": em_o,\n","            \"Emissions / 10.000 Prompts\": em_p,\n","            })\n","\n","wandb.finish()\n","\n","# Save results to a CSV file\n","results = [\n","    [\"Runs\", runs],\n","    [\"Prompts / Run\", num_prompts],\n","    [\"Total Prompts\", total_prompts],\n","    [\"Total Time\", ttime], \n","    [\"AVG. Time / Prompt\", avg_time_per_prompt],\n","    [\"AVG. Tokens / Second\", avg_toks_per_sec],\n","    [\"Total Input Tokens\", total_input_tok],\n","    [\"AVG. Input Tokens / Prompt\", avg_input_tokens],\n","    [\"Total Output Tokens\", total_output_tok],\n","    [\"AVG. Output Tokens / Prompt\", avg_output_tokens],\n","    [\"Total Emissions\", emissions],\n","    [\"Emissions / 1.000.000 Input Tokens\", em_i],\n","    [\"Emissions / 1.000.000 Output Tokens\", em_o],\n","    [\"Emissions / 10.000 Prompts\", em_p]\n","]\n","\n","# Ensure the directory exists\n","output_file_path = f\"emission_data/vllm_{model_name}_emission_data.csv\"\n","os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n","\n","with open(output_file_path, 'w', newline='') as f:\n","    writer = csv.writer(f)\n","    writer.writerow([\"Metric\", \"Value\"])\n","    writer.writerows(results)\n","\n","print(f\"Results saved to {output_file_path}\\n\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["# Note: Idle Performance"]},{"cell_type":"markdown","metadata":{},"source":["- In idle each L4 GPU needs about 27W to store its maximum capacity in VRAM. \n","- In full idle with empty VRAM each L4 needs about 16W"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/envs/pytorch/lib/python3.11/site-packages/codecarbon/output_methods/file.py:50: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  df = pd.concat([df, pd.DataFrame.from_records([dict(total.values)])])\n"]},{"data":{"text/plain":["0.0031655554134173997"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["import time\n","\n","tracker = EmissionsTracker(save_to_file=True, project_name=f\"idle_vllm_output\", log_level=\"error\", pue = 1.22, output_file=f\"emissions_output_tok_vllm.csv\")\n","tracker.start()\n","\n","\n","time.sleep(60)\n","\n","tracker.stop()"]},{"cell_type":"markdown","metadata":{},"source":["# Huggingface Transformers"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["# Load the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.bos_token"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"69cc7ffe19cb43b1b6bdf3915696bdf9","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Load the model with flash attention 2\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    #quantization_config=bnb_config,\n","    device_map=\"auto\",\n","    torch_dtype=torch.bfloat16,\n","    #attn_implementation=\"flash_attention_2\", #FlashAttention only supports Ampere GPUs or newer.\n",")\n","model.config.pad_token_id = tokenizer.pad_token_id"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Initialize the pipeline\n","text_generation_pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer, device_map=\"auto\", batch_size=64, num_workers=48)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["def query_model_pipeline(prompt_list, temperature=0.8, min_p=0.05, max_length=48):\n","\n","    # Start Timer for Inference\n","    start_time = time()\n","\n","    terminators = [\n","        tokenizer.eos_token_id,\n","        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n","    ]\n","\n","    outputs = text_generation_pipeline(\n","        prompt_list,\n","        do_sample=True,\n","        min_p=min_p,\n","        temperature=temperature,\n","        eos_token_id=terminators,\n","        max_new_tokens=max_length,\n","        return_full_text=False,\n","        #return_tensors=True,\n","        #return_text = True,\n","        pad_token_id=model.config.eos_token_id\n","    )\n","\n","    # End Timer for Inference\n","    end_time = time()\n","\n","    ttime = end_time-start_time\n","\n","    return outputs, ttime"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdaniel-wetzel\u001b[0m (\u001b[33mllm-emissions\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.17.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/ubuntu/llm-customization/notebooks/wandb/run-20240711_170112-fix05t7s</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/llm-emissions/Inference_Framework_Comparison/runs/fix05t7s' target=\"_blank\">transformers_pipeline_Inference_7500_prompts</a></strong> to <a href='https://wandb.ai/llm-emissions/Inference_Framework_Comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/llm-emissions/Inference_Framework_Comparison' target=\"_blank\">https://wandb.ai/llm-emissions/Inference_Framework_Comparison</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/llm-emissions/Inference_Framework_Comparison/runs/fix05t7s' target=\"_blank\">https://wandb.ai/llm-emissions/Inference_Framework_Comparison/runs/fix05t7s</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n"," --->>> Starting Test Run <<<--- \n","\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/envs/pytorch/lib/python3.11/site-packages/codecarbon/output_methods/file.py:50: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  df = pd.concat([df, pd.DataFrame.from_records([dict(total.values)])])\n"]},{"name":"stdout","output_type":"stream","text":["\n"," Finished in: 1505.43s!\n","\n","\n","=============== RESULTS for transformers_pipeline_Inference_7500_prompts ===============\n","\n","\n","    Finished 150 Runs with 50 Prompts/Run.\n","\n","\n","    Total Time: 1505.43s, AVG/Prompt: 200.72ms\n","\n","\n","    Average tokens per second: 180.00\n","\n","\n","    Total Prompts: 7500\n","\n","    Total Input Tokens: 1399950, AVG/Prompt: 186.66\n","\n","    Total Output Tokens: 270975, AVG/Prompt: 36.13\n","\n","    --------------------------------------------------\n","\n","    Total Inference Emissions: 0.096kg CO₂eq\n","\n","\n","    Emissions / 1.000.000 Input Tokens: 0.069kg CO₂eq\n","\n","    Emissions / 1.000.000 Output Tokens: 0.356kg CO₂eq\n","\n","    Emissions / 10.000 Prompts: 0.129kg CO₂eq\n","\n","\n","    \n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"45f7d554777e49e0a830fcd0229c0891","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AVG. Input Tokens</td><td>▁</td></tr><tr><td>AVG. Output Tokens</td><td>▁</td></tr><tr><td>AVG. Time / Prompt</td><td>▁</td></tr><tr><td>AVG. Tokens / Second</td><td>▁</td></tr><tr><td>Emissions / 1.000.000 Input Tokens</td><td>▁</td></tr><tr><td>Emissions / 1.000.000 Output Tokens</td><td>▁</td></tr><tr><td>Emissions / 10.000 Prompts</td><td>▁</td></tr><tr><td>Total Emissions</td><td>▁</td></tr><tr><td>Total Time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AVG. Input Tokens</td><td>186.66</td></tr><tr><td>AVG. Output Tokens</td><td>36.13</td></tr><tr><td>AVG. Time / Prompt</td><td>200.7243</td></tr><tr><td>AVG. Tokens / Second</td><td>179.99814</td></tr><tr><td>Emissions / 1.000.000 Input Tokens</td><td>0.06887</td></tr><tr><td>Emissions / 1.000.000 Output Tokens</td><td>0.35581</td></tr><tr><td>Emissions / 10.000 Prompts</td><td>0.12855</td></tr><tr><td>Total Emissions</td><td>0.09641</td></tr><tr><td>Total Time</td><td>1505.43226</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">transformers_pipeline_Inference_7500_prompts</strong> at: <a href='https://wandb.ai/llm-emissions/Inference_Framework_Comparison/runs/fix05t7s' target=\"_blank\">https://wandb.ai/llm-emissions/Inference_Framework_Comparison/runs/fix05t7s</a><br/> View project at: <a href='https://wandb.ai/llm-emissions/Inference_Framework_Comparison' target=\"_blank\">https://wandb.ai/llm-emissions/Inference_Framework_Comparison</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240711_170112-fix05t7s/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Results saved to emission_data/transformers_meta-llama/Meta-Llama-3-8B-Instruct_emission_data.csv\n","\n","\n"]}],"source":["total_input_tok = 0\n","total_output_tok = 0\n","\n","runs = 150\n","num_prompts = len(words)\n","total_prompts = runs * num_prompts\n","\n","name=f\"transformers_pipeline_Inference_{total_prompts}_prompts\"\n","\n","\n","prompts = prepare_prompts(words, runs=runs, num_examples=5, multiply_by=1)\n","\n","\n","wandb.init(\n","    # set the wandb project where this run will be logged\n","    project=\"Inference_Framework_Comparison\",\n","\n","    # track hyperparameters and run metadata\n","    config={\n","    \"runs\": runs,\n","    \"num_prompts\": num_prompts,\n","    \"total_prompts\": total_prompts,\n","    \"framework\": 'pyTorch - Transformers Pipeline',\n","    \"model\": model_name,\n","    },\n","\n","    name=name,\n",")\n","\n","print('\\n --->>> Starting Test Run <<<--- \\n')\n","\n","tracker = EmissionsTracker(save_to_file=True, project_name=f\"{name}-llama3-8B\", log_level=\"error\", pue = 1.22, output_file=f\"emissions_{name}.csv\")\n","tracker.start()\n","\n","\n","outputs, ttime = query_model_pipeline(prompts, )\n","\n","emissions: float = tracker.stop()\n","    \n","print(f'\\n Finished in: {ttime:.2f}s!\\n\\n')\n","\n","  \n","    \n","for idx, output in enumerate(outputs): \n","    \n","    # Extracting information\n","    prompt = prompts[idx]\n","    generated_text = output[0]['generated_text']\n","    \n","    input_tokens = tokenizer.encode(prompt)\n","    input_token_count = len(input_tokens)\n","    \n","    output_tokens = tokenizer.encode(generated_text)\n","    num_output_tokens = len(output_tokens)\n","\n","    total_input_tok += input_token_count\n","    total_output_tok += num_output_tokens\n","\n","\n","# Calculate averages\n","avg_time_per_prompt = (ttime / total_prompts)*1000\n","avg_toks_per_sec = total_output_tok/ttime\n","avg_input_tokens = total_input_tok / total_prompts\n","avg_output_tokens = total_output_tok / total_prompts\n","\n","em_i = emissions/total_input_tok *1_000_000\n","em_o = emissions/total_output_tok *1_000_000\n","em_p = emissions/total_prompts *10_000\n","\n","print(\"=\"*15 + f\" RESULTS for {name} \" + \"=\"*15 + \n","    \"\\n\\n\" + \n","    f\"\"\"\n","    Finished {runs} Runs with {num_prompts} Prompts/Run.\\n\\n\n","    Total Time: {ttime:.2f}s, AVG/Prompt: {avg_time_per_prompt:.2f}ms\\n\\n\n","    Average tokens per second: {avg_toks_per_sec:.2f}\\n\\n\n","    Total Prompts: {total_prompts}\\n\n","    Total Input Tokens: {total_input_tok}, AVG/Prompt: {avg_input_tokens}\\n\n","    Total Output Tokens: {total_output_tok}, AVG/Prompt: {avg_output_tokens}\\n\n","    \"\"\" + \n","    \n","    \"-\"*50 + \"\\n\" +\n","    \n","    f\"\"\"\n","    Total Inference Emissions: {emissions:.3f}kg CO₂eq\\n\\n\n","    Emissions / 1.000.000 Input Tokens: {em_i:.3f}kg CO₂eq\\n\n","    Emissions / 1.000.000 Output Tokens: {em_o:.3f}kg CO₂eq\\n\n","    Emissions / 10.000 Prompts: {em_p:.3f}kg CO₂eq\\n\n","\n","    \"\"\"\n","    )\n","\n","wandb.log({\"Total Time\": ttime,\n","    \"AVG. Time / Prompt\": avg_time_per_prompt,\n","              \"AVG. Tokens / Second\": avg_toks_per_sec,\n","              \"AVG. Input Tokens\": avg_input_tokens,\n","              \"AVG. Output Tokens\": avg_output_tokens,\n","              \"Total Emissions\": emissions,\n","              \"Emissions / 1.000.000 Input Tokens\": em_i,\n","              \"Emissions / 1.000.000 Output Tokens\": em_o,\n","              \"Emissions / 10.000 Prompts\": em_p,\n","              })\n","\n","wandb.finish()\n","\n","# Save results to a CSV file\n","results = [\n","    [\"Runs\", runs],\n","    [\"Prompts / Run\", num_prompts],\n","    [\"Total Prompts\", total_prompts],\n","    [\"Total Time\", ttime], \n","    [\"AVG. Time / Prompt\", avg_time_per_prompt],\n","    [\"AVG. Tokens / Second\", avg_toks_per_sec],\n","    [\"Total Input Tokens\", total_input_tok],\n","    [\"AVG. Input Tokens / Prompt\", avg_input_tokens],\n","    [\"Total Output Tokens\", total_output_tok],\n","    [\"AVG. Output Tokens / Prompt\", avg_output_tokens],\n","    [\"Total Emissions\", emissions],\n","    [\"Emissions / 1.000.000 Input Tokens\", em_i],\n","    [\"Emissions / 1.000.000 Output Tokens\", em_o],\n","    [\"Emissions / 10.000 Prompts\", em_p]\n","]\n","\n","# Ensure the directory exists\n","output_file_path = f\"emission_data/transformers_{model_name}_emission_data.csv\"\n","os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n","\n","with open(output_file_path, 'w', newline='') as f:\n","    writer = csv.writer(f)\n","    writer.writerow([\"Metric\", \"Value\"])\n","    writer.writerows(results)\n","\n","print(f\"Results saved to {output_file_path}\\n\\n\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4880117,"sourceId":8230136,"sourceType":"datasetVersion"},{"modelInstanceId":28079,"sourceId":33547,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":28083,"sourceId":33551,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30716,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
