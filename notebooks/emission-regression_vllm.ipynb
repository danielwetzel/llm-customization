{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the individual CSV files\n",
    "csv_files_output = {\n",
    "    '1': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_1_examples.csv',\n",
    "    '2': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_2_examples.csv',\n",
    "    '3': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_3_examples.csv',\n",
    "    '5': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_5_examples.csv',\n",
    "    '10': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_10_examples.csv',\n",
    "    '15': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_15_examples.csv',\n",
    "    '20': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_20_examples.csv',\n",
    "    '30': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_30_examples.csv',\n",
    "    '40': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_40_examples.csv',\n",
    "    '60': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_60_examples.csv',\n",
    "    '90': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_90_examples.csv'\n",
    "}\n",
    "\n",
    "# Read the emissions data\n",
    "emissions_data = pd.read_csv('emissions_output_tok_vllm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store metadata\n",
    "total_time = []\n",
    "time_per_prompt = []\n",
    "tok_per_sec = []\n",
    "parameters_output = []\n",
    "num_examples_output = []\n",
    "num_prompts_output = []\n",
    "total_emissions_output = []\n",
    "cpu_energy_output = []\n",
    "gpu_energy_output = []\n",
    "ram_energy_output = []\n",
    "total_energy_output = []\n",
    "total_output_tokens_output = []\n",
    "total_input_tokens_output = []\n",
    "avg_input_tokens_output = []\n",
    "avg_output_tokens_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and extract metadata from each CSV file\n",
    "for model, file in csv_files_output.items():\n",
    "    data = pd.read_csv(file)\n",
    "    time = data.loc[data['Metric'] == 'Total Time', 'Value'].values[0]\n",
    "    time_p_prompt = data.loc[data['Metric'] == 'AVG. Time / Prompt', 'Value'].values[0] / 1000 #Time is in ms\n",
    "    tok_p_sec = data.loc[data['Metric'] == 'AVG. Tokens / Second', 'Value'].values[0]\n",
    "    prompts = data.loc[data['Metric'] == 'Total Prompts', 'Value'].values[0]\n",
    "    output_tokens = data.loc[data['Metric'] == 'Total Output Tokens', 'Value'].values[0]\n",
    "    input_tokens = data.loc[data['Metric'] == 'Total Input Tokens', 'Value'].values[0]\n",
    "    avg_i_tok = data.loc[data['Metric'] == 'AVG. Input Tokens / Prompt', 'Value'].values[0]\n",
    "    avg_o_tok = data.loc[data['Metric'] == 'AVG. Output Tokens / Prompt', 'Value'].values[0]\n",
    "    total_time.append(float(time))\n",
    "    time_per_prompt.append(float(time_p_prompt))\n",
    "    tok_per_sec.append(float(tok_p_sec))\n",
    "    parameters_output.append(8)\n",
    "    num_examples_output.append(int(model))\n",
    "    num_prompts_output.append(int(prompts))\n",
    "    total_output_tokens_output.append(float(output_tokens))\n",
    "    total_input_tokens_output.append(float(input_tokens))\n",
    "    avg_input_tokens_output.append(float(avg_i_tok))\n",
    "    avg_output_tokens_output.append(float(avg_o_tok))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract emissions data\n",
    "for model in csv_files_output.keys():\n",
    "    model_emissions = emissions_data[emissions_data['project_name'].str.contains(\"vLLM_Inference_1000_prompts_output_tok_\" + model + \"_\")]\n",
    "    total_emissions_output.append(model_emissions['emissions'].values[0])\n",
    "    cpu_energy_output.append(model_emissions['cpu_energy'].values[0])\n",
    "    gpu_energy_output.append(model_emissions['gpu_energy'].values[0])\n",
    "    ram_energy_output.append(model_emissions['ram_energy'].values[0])\n",
    "    total_energy_output.append(model_emissions['energy_consumed'].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[186.66, 186.66, 186.66, 186.66, 186.66, 186.66, 186.66, 186.66, 186.66, 186.66, 186.66]\n",
      "[18.173, 21.523, 40.863, 69.858, 182.237, 288.208, 398.264, 594.771, 781.428, 1202.476, 1796.304]\n",
      "[18173.0, 21523.0, 40863.0, 69858.0, 182237.0, 288208.0, 398264.0, 594771.0, 781428.0, 1202476.0, 1796304.0]\n",
      "[0.0034967513216419, 0.0038288557217628, 0.0047209939368813, 0.0058326642843592, 0.0107183823003618, 0.0155925593968694, 0.0204828999682617, 0.0303342998617103, 0.0402502108596118, 0.0663906878731937, 0.1103408511731546]\n"
     ]
    }
   ],
   "source": [
    "print(avg_input_tokens_output)\n",
    "print(avg_output_tokens_output)\n",
    "print(total_output_tokens_output)\n",
    "print(total_emissions_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for regression and visualization\n",
    "total_time = np.array(total_time)\n",
    "time_per_prompt = np.array(time_per_prompt)\n",
    "tok_per_sec = np.array(tok_per_sec)\n",
    "parameters_output = np.array(parameters_output)\n",
    "num_examples_output = np.array(num_examples_output)\n",
    "num_prompts_output = np.array(num_prompts_output)\n",
    "total_output_tokens_output = np.array(total_output_tokens_output)\n",
    "total_input_tokens_output = np.array(total_input_tokens_output)\n",
    "avg_input_tokens_output = np.array(avg_input_tokens_output)\n",
    "avg_output_tokens_output = np.array(avg_output_tokens_output)\n",
    "total_emissions_output = np.array(total_emissions_output)\n",
    "cpu_energy_output = np.array(cpu_energy_output)\n",
    "gpu_energy_output = np.array(gpu_energy_output)\n",
    "ram_energy_output = np.array(ram_energy_output)\n",
    "total_energy_output = np.array(total_energy_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  49.20257998   51.66708994   63.47096586   80.3412075   144.78470874\n",
      "  211.3589766   276.60654068  408.54310036  541.17958927  882.30840707\n",
      " 1447.26580811]\n"
     ]
    }
   ],
   "source": [
    "print(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idle_gpu_power = 28*4 # 28W per GPU, 4 GPUs\n",
    "\n",
    "total_idle_gpu_energy = (idle_gpu_power/1000)*(total_time/3600) # Convert W into kw and s into h\n",
    "idle_gpu_energy_per_thousand_prompts = total_idle_gpu_energy / num_prompts_output * 10_000\n",
    "\n",
    "gpu_energy_without_idle = gpu_energy_output - total_idle_gpu_energy\n",
    "gpu_energy_without_idle_per_thousand_prompts = gpu_energy_without_idle / num_prompts_output * 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate emissions per 10,000 prompts\n",
    "emissions_per_thousand_prompts = {\n",
    "    'Total Emissions Output Tok': total_emissions_output / num_prompts_output * 10_000,\n",
    "    'CPU Energy Output Tok': cpu_energy_output / num_prompts_output * 10_000,\n",
    "    'GPU Energy Output Tok': gpu_energy_output / num_prompts_output * 10_000,\n",
    "    'GPU Energy Output Tok (without idle)': gpu_energy_without_idle_per_thousand_prompts,\n",
    "    'GPU Energy Output Tok (idle)': idle_gpu_energy_per_thousand_prompts,\n",
    "    'RAM Energy Output Tok': ram_energy_output / num_prompts_output * 10_000,\n",
    "    'Total Energy Output Tok': total_energy_output / num_prompts_output * 10_000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idle GPU Energy per 10.000 prompts: [0.03413982 0.03822099 0.04719003 0.05759958 0.10691888 0.15527704\n",
      " 0.20436698 0.30308119 0.40250862 0.66785381 1.11722544]\n",
      "Idle GPU Energy per 10.000 prompts: [0.01530747 0.01607421 0.01974652 0.02499504 0.04504413 0.06575613\n",
      " 0.08605537 0.1271023  0.16836698 0.27449595 0.45026047]\n",
      "GPU Energy without idle per 10.000 prompts: [0.01883235 0.02214679 0.02744351 0.03260454 0.06187475 0.08952092\n",
      " 0.11831161 0.17597889 0.23414164 0.39335786 0.66696497]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Idle GPU Energy per 10.000 prompts: {emissions_per_thousand_prompts['GPU Energy Output Tok']}\")\n",
    "print(f\"Idle GPU Energy per 10.000 prompts: {idle_gpu_energy_per_thousand_prompts}\")\n",
    "print(f\"GPU Energy without idle per 10.000 prompts: {gpu_energy_without_idle_per_thousand_prompts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total Emissions Output Tok': array([0.03496751, 0.03828856, 0.04720994, 0.05832664, 0.10718382,\n",
      "       0.15592559, 0.204829  , 0.303343  , 0.40250211, 0.66390688,\n",
      "       1.10340851]), 'CPU Energy Output Tok': array([0.00708905, 0.00744669, 0.00914967, 0.01157489, 0.02085703,\n",
      "       0.03044518, 0.03984242, 0.05884579, 0.07794874, 0.12708085,\n",
      "       0.20845374]), 'GPU Energy Output Tok': array([0.03413982, 0.03822099, 0.04719003, 0.05759958, 0.10691888,\n",
      "       0.15527704, 0.20436698, 0.30308119, 0.40250862, 0.66785381,\n",
      "       1.11722544]), 'GPU Energy Output Tok (without idle)': array([0.01883235, 0.02214679, 0.02744351, 0.03260454, 0.06187475,\n",
      "       0.08952092, 0.11831161, 0.17597889, 0.23414164, 0.39335786,\n",
      "       0.66696497]), 'GPU Energy Output Tok (idle)': array([0.01530747, 0.01607421, 0.01974652, 0.02499504, 0.04504413,\n",
      "       0.06575613, 0.08605537, 0.1271023 , 0.16836698, 0.27449595,\n",
      "       0.45026047]), 'RAM Energy Output Tok': array([0.01135794, 0.01191357, 0.01465821, 0.01854159, 0.03341531,\n",
      "       0.04877059, 0.06382809, 0.09426339, 0.12485606, 0.20349921,\n",
      "       0.33371082]), 'Total Energy Output Tok': array([0.05258682, 0.05758126, 0.07099791, 0.08771606, 0.16119122,\n",
      "       0.23449282, 0.3080375 , 0.45619037, 0.60531342, 0.99843388,\n",
      "       1.65939   ])}\n"
     ]
    }
   ],
   "source": [
    "print(emissions_per_thousand_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 8 8 8 8 8 8 8 8 8 8]\n"
     ]
    }
   ],
   "source": [
    "print(parameters_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform regression analysis\n",
    "def perform_regression(x, y):\n",
    "    x = x.reshape(-1, 1)\n",
    "    model = LinearRegression()\n",
    "    model.fit(x, y)\n",
    "    predicted = model.predict(x)\n",
    "    return model, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Emissions Output Tok - Intercept: -0.0007795195459148752, Coefficient: 0.0005799787512447661\n",
      "CPU Energy Output Tok - Intercept: 0.0005904601720052721, Coefficient: 0.00010979374634339546\n",
      "GPU Energy Output Tok - Intercept: -0.0027304291004494052, Coefficient: 0.0005866435892202829\n",
      "GPU Energy Output Tok (without idle) - Intercept: -0.003997035816304478, Coefficient: 0.00034948248541403213\n",
      "GPU Energy Output Tok (idle) - Intercept: 0.0012666067158550592, Coefficient: 0.00023716110380625076\n",
      "RAM Energy Output Tok - Intercept: 0.0009676678935606475, Coefficient: 0.00017577901084810286\n",
      "Total Energy Output Tok - Intercept: -0.0011723010348828056, Coefficient: 0.0008722163464117812\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "predictions = {}\n",
    "for name, y in emissions_per_thousand_prompts.items():\n",
    "    model, predicted = perform_regression(avg_output_tokens_output, y)\n",
    "    models[name] = model\n",
    "    predictions[name] = predicted\n",
    "    print(f\"{name} - Intercept: {model.intercept_}, Coefficient: {model.coef_[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test types and model types\n",
    "test_types = ['Output-tok-vllm']\n",
    "model_types = ['llama3']\n",
    "\n",
    "# Define the parameters\n",
    "parameters = np.concatenate([parameters_output])\n",
    "num_examples = np.concatenate([num_examples_output])\n",
    "num_prompts = np.concatenate([num_prompts_output])\n",
    "total_out_tok = np.concatenate([total_output_tokens_output])\n",
    "total_in_tok = np.concatenate([total_input_tokens_output])\n",
    "avg_out_tok = np.concatenate([avg_output_tokens_output])\n",
    "avg_in_tok = np.concatenate([avg_input_tokens_output])\n",
    "\n",
    "pred_emissions_per_10k_prompts = predictions['Total Emissions Output Tok']\n",
    "\n",
    "pred_cpu_energy_per_10k_prompts = predictions['CPU Energy Output Tok']\n",
    "\n",
    "pred_gpu_energy_per_10k_prompts = predictions['GPU Energy Output Tok']\n",
    "\n",
    "pred_ram_energy_per_10k_prompts = predictions['RAM Energy Output Tok']\n",
    "\n",
    "pred_total_energy_per_10k_prompts = predictions['Total Energy Output Tok']\n",
    "\n",
    "pred_idle_gpu_energy_per_10k_prompts = predictions['GPU Energy Output Tok (idle)']\n",
    "\n",
    "pred_non_idle_gpu_energy_per_10k_prompts = predictions['GPU Energy Output Tok (without idle)']\n",
    "\n",
    "actual_emissions_per_10k_prompts = emissions_per_thousand_prompts['Total Emissions Output Tok']\n",
    "\n",
    "actual_cpu_energy_per_10k_prompts = emissions_per_thousand_prompts['CPU Energy Output Tok']\n",
    "\n",
    "actual_gpu_energy_per_10k_prompts = emissions_per_thousand_prompts['GPU Energy Output Tok']\n",
    "\n",
    "actual_ram_energy_per_10k_prompts = emissions_per_thousand_prompts['RAM Energy Output Tok']\n",
    "\n",
    "actual_total_energy_per_10k_prompts = emissions_per_thousand_prompts['Total Energy Output Tok']\n",
    "\n",
    "actual_idle_gpu_energy_per_10k_prompts = emissions_per_thousand_prompts['GPU Energy Output Tok (idle)']\n",
    "\n",
    "actual_non_idle_gpu_energy_per_10k_prompts = emissions_per_thousand_prompts['GPU Energy Output Tok (without idle)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat test types and model types for each data point\n",
    "test_type_column = np.concatenate([\n",
    "    np.repeat(test_types[0], len(parameters_output))\n",
    "])\n",
    "\n",
    "model_type_column = np.concatenate([\n",
    "    np.repeat(model_types[0], len(parameters_output))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_type</th>\n",
       "      <th>model_type</th>\n",
       "      <th>parameters</th>\n",
       "      <th>num_examples</th>\n",
       "      <th>num_prompts</th>\n",
       "      <th>total_time</th>\n",
       "      <th>time_per_prompt</th>\n",
       "      <th>tok_per_sec</th>\n",
       "      <th>total_out_tok</th>\n",
       "      <th>total_in_tok</th>\n",
       "      <th>...</th>\n",
       "      <th>actual_ram_energy_per_10k_prompts</th>\n",
       "      <th>actual_idle_gpu_energy_per_10k_prompts</th>\n",
       "      <th>actual_non_idle_gpu_energy_per_10k_prompts</th>\n",
       "      <th>pred_emissions_per_10k_prompts</th>\n",
       "      <th>pred_total_energy_per_10k_prompts</th>\n",
       "      <th>pred_cpu_energy_per_10k_prompts</th>\n",
       "      <th>pred_gpu_energy_per_10k_prompts</th>\n",
       "      <th>pred_ram_energy_per_10k_prompts</th>\n",
       "      <th>pred_idle_gpu_energy_per_10k_prompts</th>\n",
       "      <th>pred_non_idle_gpu_energy_per_10k_prompts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>49.202580</td>\n",
       "      <td>0.049203</td>\n",
       "      <td>369.350551</td>\n",
       "      <td>18173.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011358</td>\n",
       "      <td>0.015307</td>\n",
       "      <td>0.018832</td>\n",
       "      <td>0.009760</td>\n",
       "      <td>0.014678</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>0.004162</td>\n",
       "      <td>0.005577</td>\n",
       "      <td>0.002354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>51.667090</td>\n",
       "      <td>0.051667</td>\n",
       "      <td>416.570781</td>\n",
       "      <td>21523.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011914</td>\n",
       "      <td>0.016074</td>\n",
       "      <td>0.022147</td>\n",
       "      <td>0.011703</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>0.009896</td>\n",
       "      <td>0.004751</td>\n",
       "      <td>0.006371</td>\n",
       "      <td>0.003525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>63.470966</td>\n",
       "      <td>0.063471</td>\n",
       "      <td>643.806179</td>\n",
       "      <td>40863.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014658</td>\n",
       "      <td>0.019747</td>\n",
       "      <td>0.027444</td>\n",
       "      <td>0.022920</td>\n",
       "      <td>0.034469</td>\n",
       "      <td>0.005077</td>\n",
       "      <td>0.021242</td>\n",
       "      <td>0.008151</td>\n",
       "      <td>0.010958</td>\n",
       "      <td>0.010284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>80.341208</td>\n",
       "      <td>0.080341</td>\n",
       "      <td>869.516431</td>\n",
       "      <td>69858.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018542</td>\n",
       "      <td>0.024995</td>\n",
       "      <td>0.032605</td>\n",
       "      <td>0.039737</td>\n",
       "      <td>0.059759</td>\n",
       "      <td>0.008260</td>\n",
       "      <td>0.038251</td>\n",
       "      <td>0.013247</td>\n",
       "      <td>0.017834</td>\n",
       "      <td>0.020417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>144.784709</td>\n",
       "      <td>0.144785</td>\n",
       "      <td>1258.675737</td>\n",
       "      <td>182237.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033415</td>\n",
       "      <td>0.045044</td>\n",
       "      <td>0.061875</td>\n",
       "      <td>0.104914</td>\n",
       "      <td>0.157778</td>\n",
       "      <td>0.020599</td>\n",
       "      <td>0.104178</td>\n",
       "      <td>0.033001</td>\n",
       "      <td>0.044486</td>\n",
       "      <td>0.059692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>1000</td>\n",
       "      <td>211.358977</td>\n",
       "      <td>0.211359</td>\n",
       "      <td>1363.594793</td>\n",
       "      <td>288208.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048771</td>\n",
       "      <td>0.065756</td>\n",
       "      <td>0.089521</td>\n",
       "      <td>0.166375</td>\n",
       "      <td>0.250207</td>\n",
       "      <td>0.032234</td>\n",
       "      <td>0.166345</td>\n",
       "      <td>0.051629</td>\n",
       "      <td>0.069618</td>\n",
       "      <td>0.096727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>1000</td>\n",
       "      <td>276.606541</td>\n",
       "      <td>0.276607</td>\n",
       "      <td>1439.821340</td>\n",
       "      <td>398264.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063828</td>\n",
       "      <td>0.086055</td>\n",
       "      <td>0.118312</td>\n",
       "      <td>0.230205</td>\n",
       "      <td>0.346200</td>\n",
       "      <td>0.044317</td>\n",
       "      <td>0.230909</td>\n",
       "      <td>0.070974</td>\n",
       "      <td>0.095719</td>\n",
       "      <td>0.135189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>1000</td>\n",
       "      <td>408.543100</td>\n",
       "      <td>0.408543</td>\n",
       "      <td>1455.834157</td>\n",
       "      <td>594771.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094263</td>\n",
       "      <td>0.127102</td>\n",
       "      <td>0.175979</td>\n",
       "      <td>0.344175</td>\n",
       "      <td>0.517597</td>\n",
       "      <td>0.065893</td>\n",
       "      <td>0.346188</td>\n",
       "      <td>0.105516</td>\n",
       "      <td>0.142323</td>\n",
       "      <td>0.203865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>1000</td>\n",
       "      <td>541.179589</td>\n",
       "      <td>0.541180</td>\n",
       "      <td>1443.934722</td>\n",
       "      <td>781428.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124856</td>\n",
       "      <td>0.168367</td>\n",
       "      <td>0.234142</td>\n",
       "      <td>0.452432</td>\n",
       "      <td>0.680402</td>\n",
       "      <td>0.086386</td>\n",
       "      <td>0.455689</td>\n",
       "      <td>0.138326</td>\n",
       "      <td>0.186591</td>\n",
       "      <td>0.269098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>60</td>\n",
       "      <td>1000</td>\n",
       "      <td>882.308407</td>\n",
       "      <td>0.882308</td>\n",
       "      <td>1362.874920</td>\n",
       "      <td>1202476.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203499</td>\n",
       "      <td>0.274496</td>\n",
       "      <td>0.393358</td>\n",
       "      <td>0.696631</td>\n",
       "      <td>1.047647</td>\n",
       "      <td>0.132615</td>\n",
       "      <td>0.702694</td>\n",
       "      <td>0.212338</td>\n",
       "      <td>0.286447</td>\n",
       "      <td>0.416247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>90</td>\n",
       "      <td>1000</td>\n",
       "      <td>1447.265808</td>\n",
       "      <td>1.447266</td>\n",
       "      <td>1241.170758</td>\n",
       "      <td>1796304.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333711</td>\n",
       "      <td>0.450260</td>\n",
       "      <td>0.666965</td>\n",
       "      <td>1.041039</td>\n",
       "      <td>1.565593</td>\n",
       "      <td>0.197813</td>\n",
       "      <td>1.051060</td>\n",
       "      <td>0.316720</td>\n",
       "      <td>0.427280</td>\n",
       "      <td>0.623780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          test_type model_type  parameters  num_examples  num_prompts  \\\n",
       "0   Output-tok-vllm     llama3           8             1         1000   \n",
       "1   Output-tok-vllm     llama3           8             2         1000   \n",
       "2   Output-tok-vllm     llama3           8             3         1000   \n",
       "3   Output-tok-vllm     llama3           8             5         1000   \n",
       "4   Output-tok-vllm     llama3           8            10         1000   \n",
       "5   Output-tok-vllm     llama3           8            15         1000   \n",
       "6   Output-tok-vllm     llama3           8            20         1000   \n",
       "7   Output-tok-vllm     llama3           8            30         1000   \n",
       "8   Output-tok-vllm     llama3           8            40         1000   \n",
       "9   Output-tok-vllm     llama3           8            60         1000   \n",
       "10  Output-tok-vllm     llama3           8            90         1000   \n",
       "\n",
       "     total_time  time_per_prompt  tok_per_sec  total_out_tok  total_in_tok  \\\n",
       "0     49.202580         0.049203   369.350551        18173.0      186660.0   \n",
       "1     51.667090         0.051667   416.570781        21523.0      186660.0   \n",
       "2     63.470966         0.063471   643.806179        40863.0      186660.0   \n",
       "3     80.341208         0.080341   869.516431        69858.0      186660.0   \n",
       "4    144.784709         0.144785  1258.675737       182237.0      186660.0   \n",
       "5    211.358977         0.211359  1363.594793       288208.0      186660.0   \n",
       "6    276.606541         0.276607  1439.821340       398264.0      186660.0   \n",
       "7    408.543100         0.408543  1455.834157       594771.0      186660.0   \n",
       "8    541.179589         0.541180  1443.934722       781428.0      186660.0   \n",
       "9    882.308407         0.882308  1362.874920      1202476.0      186660.0   \n",
       "10  1447.265808         1.447266  1241.170758      1796304.0      186660.0   \n",
       "\n",
       "    ...  actual_ram_energy_per_10k_prompts  \\\n",
       "0   ...                           0.011358   \n",
       "1   ...                           0.011914   \n",
       "2   ...                           0.014658   \n",
       "3   ...                           0.018542   \n",
       "4   ...                           0.033415   \n",
       "5   ...                           0.048771   \n",
       "6   ...                           0.063828   \n",
       "7   ...                           0.094263   \n",
       "8   ...                           0.124856   \n",
       "9   ...                           0.203499   \n",
       "10  ...                           0.333711   \n",
       "\n",
       "    actual_idle_gpu_energy_per_10k_prompts  \\\n",
       "0                                 0.015307   \n",
       "1                                 0.016074   \n",
       "2                                 0.019747   \n",
       "3                                 0.024995   \n",
       "4                                 0.045044   \n",
       "5                                 0.065756   \n",
       "6                                 0.086055   \n",
       "7                                 0.127102   \n",
       "8                                 0.168367   \n",
       "9                                 0.274496   \n",
       "10                                0.450260   \n",
       "\n",
       "    actual_non_idle_gpu_energy_per_10k_prompts  \\\n",
       "0                                     0.018832   \n",
       "1                                     0.022147   \n",
       "2                                     0.027444   \n",
       "3                                     0.032605   \n",
       "4                                     0.061875   \n",
       "5                                     0.089521   \n",
       "6                                     0.118312   \n",
       "7                                     0.175979   \n",
       "8                                     0.234142   \n",
       "9                                     0.393358   \n",
       "10                                    0.666965   \n",
       "\n",
       "    pred_emissions_per_10k_prompts  pred_total_energy_per_10k_prompts  \\\n",
       "0                         0.009760                           0.014678   \n",
       "1                         0.011703                           0.017600   \n",
       "2                         0.022920                           0.034469   \n",
       "3                         0.039737                           0.059759   \n",
       "4                         0.104914                           0.157778   \n",
       "5                         0.166375                           0.250207   \n",
       "6                         0.230205                           0.346200   \n",
       "7                         0.344175                           0.517597   \n",
       "8                         0.452432                           0.680402   \n",
       "9                         0.696631                           1.047647   \n",
       "10                        1.041039                           1.565593   \n",
       "\n",
       "    pred_cpu_energy_per_10k_prompts  pred_gpu_energy_per_10k_prompts  \\\n",
       "0                          0.002586                         0.007931   \n",
       "1                          0.002954                         0.009896   \n",
       "2                          0.005077                         0.021242   \n",
       "3                          0.008260                         0.038251   \n",
       "4                          0.020599                         0.104178   \n",
       "5                          0.032234                         0.166345   \n",
       "6                          0.044317                         0.230909   \n",
       "7                          0.065893                         0.346188   \n",
       "8                          0.086386                         0.455689   \n",
       "9                          0.132615                         0.702694   \n",
       "10                         0.197813                         1.051060   \n",
       "\n",
       "    pred_ram_energy_per_10k_prompts  pred_idle_gpu_energy_per_10k_prompts  \\\n",
       "0                          0.004162                              0.005577   \n",
       "1                          0.004751                              0.006371   \n",
       "2                          0.008151                              0.010958   \n",
       "3                          0.013247                              0.017834   \n",
       "4                          0.033001                              0.044486   \n",
       "5                          0.051629                              0.069618   \n",
       "6                          0.070974                              0.095719   \n",
       "7                          0.105516                              0.142323   \n",
       "8                          0.138326                              0.186591   \n",
       "9                          0.212338                              0.286447   \n",
       "10                         0.316720                              0.427280   \n",
       "\n",
       "    pred_non_idle_gpu_energy_per_10k_prompts  \n",
       "0                                   0.002354  \n",
       "1                                   0.003525  \n",
       "2                                   0.010284  \n",
       "3                                   0.020417  \n",
       "4                                   0.059692  \n",
       "5                                   0.096727  \n",
       "6                                   0.135189  \n",
       "7                                   0.203865  \n",
       "8                                   0.269098  \n",
       "9                                   0.416247  \n",
       "10                                  0.623780  \n",
       "\n",
       "[11 rows x 26 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataframe\n",
    "df = pd.DataFrame({\n",
    "    'test_type': test_type_column,\n",
    "    'model_type': model_type_column,\n",
    "    'parameters': parameters,\n",
    "    'num_examples': num_examples,\n",
    "    'num_prompts': num_prompts,\n",
    "    'total_time': total_time,\n",
    "    'time_per_prompt': time_per_prompt,\n",
    "    'tok_per_sec': tok_per_sec,\n",
    "    'total_out_tok': total_out_tok,\n",
    "    'total_in_tok': total_in_tok,\n",
    "    'avg_out_tok': avg_out_tok,\n",
    "    'avg_in_tok': avg_in_tok,\n",
    "    'actual_emissions_per_10k_prompts': actual_emissions_per_10k_prompts,\n",
    "    'actual_total_energy_per_10k_prompts': actual_total_energy_per_10k_prompts,\n",
    "    'actual_cpu_energy_per_10k_prompts': actual_cpu_energy_per_10k_prompts,\n",
    "    'actual_gpu_energy_per_10k_prompts': actual_gpu_energy_per_10k_prompts,\n",
    "    'actual_ram_energy_per_10k_prompts': actual_ram_energy_per_10k_prompts,\n",
    "    'actual_idle_gpu_energy_per_10k_prompts': actual_idle_gpu_energy_per_10k_prompts,\n",
    "    'actual_non_idle_gpu_energy_per_10k_prompts': actual_non_idle_gpu_energy_per_10k_prompts,\n",
    "    'pred_emissions_per_10k_prompts': pred_emissions_per_10k_prompts,\n",
    "    'pred_total_energy_per_10k_prompts': pred_total_energy_per_10k_prompts,\n",
    "    'pred_cpu_energy_per_10k_prompts': pred_cpu_energy_per_10k_prompts,\n",
    "    'pred_gpu_energy_per_10k_prompts': pred_gpu_energy_per_10k_prompts,\n",
    "    'pred_ram_energy_per_10k_prompts': pred_ram_energy_per_10k_prompts,\n",
    "    'pred_idle_gpu_energy_per_10k_prompts': pred_idle_gpu_energy_per_10k_prompts,\n",
    "    'pred_non_idle_gpu_energy_per_10k_prompts': pred_non_idle_gpu_energy_per_10k_prompts\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-ef073b3407d645ebafdadcdd4018b3ed.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-ef073b3407d645ebafdadcdd4018b3ed.vega-embed details,\n",
       "  #altair-viz-ef073b3407d645ebafdadcdd4018b3ed.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-ef073b3407d645ebafdadcdd4018b3ed\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-ef073b3407d645ebafdadcdd4018b3ed\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-ef073b3407d645ebafdadcdd4018b3ed\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"layer\": [{\"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok-vllm\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual Emissions for Output-tok-vllm per 10,000 Prompts\"}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok-vllm\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}}]}], \"data\": {\"name\": \"data-42ec5c93108dd8df7d3912276fb1596d\"}, \"height\": 700, \"resolve\": {\"scale\": {\"x\": \"independent\"}}, \"title\": \"Emissions per Ten-Thousand Prompts by Test Type\", \"width\": 1000, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-42ec5c93108dd8df7d3912276fb1596d\": [{\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 1, \"num_prompts\": 1000, \"total_time\": 49.202579975128174, \"time_per_prompt\": 0.049202579975128176, \"tok_per_sec\": 369.3505505033765, \"total_out_tok\": 18173.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 18.173, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.034967513216419, \"actual_total_energy_per_10k_prompts\": 0.052586817284723995, \"actual_cpu_energy_per_10k_prompts\": 0.0070890547809, \"actual_gpu_energy_per_10k_prompts\": 0.034139819367388996, \"actual_ram_energy_per_10k_prompts\": 0.011357943136433998, \"actual_idle_gpu_energy_per_10k_prompts\": 0.015307469325595431, \"actual_non_idle_gpu_energy_per_10k_prompts\": 0.01883235004179357, \"pred_emissions_per_10k_prompts\": 0.009760434300456259, \"pred_total_energy_per_10k_prompts\": 0.014678486628458491, \"pred_cpu_energy_per_10k_prompts\": 0.0025857419243037975, \"pred_gpu_energy_per_10k_prompts\": 0.007930644846450795, \"pred_ram_energy_per_10k_prompts\": 0.004162099857703221, \"pred_idle_gpu_energy_per_10k_prompts\": 0.005576535455326054, \"pred_non_idle_gpu_energy_per_10k_prompts\": 0.0023541093911247273}, {\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 2, \"num_prompts\": 1000, \"total_time\": 51.66708993911743, \"time_per_prompt\": 0.05166708993911743, \"tok_per_sec\": 416.5707808464131, \"total_out_tok\": 21523.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 21.523, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.038288557217628, \"actual_total_energy_per_10k_prompts\": 0.057581256923745, \"actual_cpu_energy_per_10k_prompts\": 0.0074466903529230005, \"actual_gpu_energy_per_10k_prompts\": 0.038220993743436994, \"actual_ram_energy_per_10k_prompts\": 0.011913572827385, \"actual_idle_gpu_energy_per_10k_prompts\": 0.016074205758836533, \"actual_non_idle_gpu_energy_per_10k_prompts\": 0.022146787984600465, \"pred_emissions_per_10k_prompts\": 0.011703363117126227, \"pred_total_energy_per_10k_prompts\": 0.01760041138893796, \"pred_cpu_energy_per_10k_prompts\": 0.0029535509745541727, \"pred_gpu_energy_per_10k_prompts\": 0.009895900870338743, \"pred_ram_energy_per_10k_prompts\": 0.004750959544044366, \"pred_idle_gpu_energy_per_10k_prompts\": 0.0063710251530769944, \"pred_non_idle_gpu_energy_per_10k_prompts\": 0.0035248757172617355}, {\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 3, \"num_prompts\": 1000, \"total_time\": 63.47096586227417, \"time_per_prompt\": 0.06347096586227417, \"tok_per_sec\": 643.8061788545764, \"total_out_tok\": 40863.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 40.863, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.047209939368813, \"actual_total_energy_per_10k_prompts\": 0.07099791284113799, \"actual_cpu_energy_per_10k_prompts\": 0.009149670522312, \"actual_gpu_energy_per_10k_prompts\": 0.047190034251997, \"actual_ram_energy_per_10k_prompts\": 0.014658208066829001, \"actual_idle_gpu_energy_per_10k_prompts\": 0.01974652271270752, \"actual_non_idle_gpu_energy_per_10k_prompts\": 0.027443511539289485, \"pred_emissions_per_10k_prompts\": 0.022920152166200004, \"pred_total_energy_per_10k_prompts\": 0.03446907552854181, \"pred_cpu_energy_per_10k_prompts\": 0.005076962028835441, \"pred_gpu_energy_per_10k_prompts\": 0.021241587885859014, \"pred_ram_energy_per_10k_prompts\": 0.008150525613846674, \"pred_idle_gpu_energy_per_10k_prompts\": 0.010957720900689884, \"pred_non_idle_gpu_energy_per_10k_prompts\": 0.010283866985169116}, {\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 5, \"num_prompts\": 1000, \"total_time\": 80.34120750427246, \"time_per_prompt\": 0.08034120750427245, \"tok_per_sec\": 869.5164308587847, \"total_out_tok\": 69858.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 69.858, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.058326642843592, \"actual_total_energy_per_10k_prompts\": 0.08771606064932101, \"actual_cpu_energy_per_10k_prompts\": 0.011574894417689, \"actual_gpu_energy_per_10k_prompts\": 0.057599580746293996, \"actual_ram_energy_per_10k_prompts\": 0.018541585485336, \"actual_idle_gpu_energy_per_10k_prompts\": 0.024995042334662546, \"actual_non_idle_gpu_energy_per_10k_prompts\": 0.032604538411631453, \"pred_emissions_per_10k_prompts\": 0.039736636058542, \"pred_total_energy_per_10k_prompts\": 0.05975898849275141, \"pred_cpu_energy_per_10k_prompts\": 0.008260431704062193, \"pred_gpu_energy_per_10k_prompts\": 0.03825131875530112, \"pred_ram_energy_per_10k_prompts\": 0.013247238033387418, \"pred_idle_gpu_energy_per_10k_prompts\": 0.017834207105552127, \"pred_non_idle_gpu_energy_per_10k_prompts\": 0.02041711164974898}, {\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 10, \"num_prompts\": 1000, \"total_time\": 144.78470873832703, \"time_per_prompt\": 0.14478470873832702, \"tok_per_sec\": 1258.6757371550984, \"total_out_tok\": 182237.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 182.237, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.10718382300361799, \"actual_total_energy_per_10k_prompts\": 0.16119122001283298, \"actual_cpu_energy_per_10k_prompts\": 0.020857030055324, \"actual_gpu_energy_per_10k_prompts\": 0.106918879312812, \"actual_ram_energy_per_10k_prompts\": 0.033415310644696, \"actual_idle_gpu_energy_per_10k_prompts\": 0.04504413160747952, \"actual_non_idle_gpu_energy_per_10k_prompts\": 0.06187474770533248, \"pred_emissions_per_10k_prompts\": 0.10491406814467757, \"pred_total_energy_per_10k_prompts\": 0.15777778928616096, \"pred_cpu_energy_per_10k_prompts\": 0.02059894312438663, \"pred_gpu_energy_per_10k_prompts\": 0.10417773866828728, \"pred_ram_energy_per_10k_prompts\": 0.03300110749348637, \"pred_idle_gpu_energy_per_10k_prompts\": 0.04448613479019478, \"pred_non_idle_gpu_energy_per_10k_prompts\": 0.0596916038780925}, {\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 15, \"num_prompts\": 1000, \"total_time\": 211.35897660255432, \"time_per_prompt\": 0.21135897660255432, \"tok_per_sec\": 1363.5947932410502, \"total_out_tok\": 288208.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 288.208, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.15592559396869402, \"actual_total_energy_per_10k_prompts\": 0.23449281821372298, \"actual_cpu_energy_per_10k_prompts\": 0.030445183376703003, \"actual_gpu_energy_per_10k_prompts\": 0.15527704416598, \"actual_ram_energy_per_10k_prompts\": 0.04877059067104, \"actual_idle_gpu_energy_per_10k_prompts\": 0.06575612605412802, \"actual_non_idle_gpu_energy_per_10k_prompts\": 0.08952091811185199, \"pred_emissions_per_10k_prompts\": 0.1663749963928367, \"pred_total_energy_per_10k_prompts\": 0.25020742773176385, \"pred_cpu_energy_per_10k_prompts\": 0.03223389621814259, \"pred_gpu_energy_per_10k_prompts\": 0.1663449464615499, \"pred_ram_energy_per_10k_prompts\": 0.051628585052070686, \"pred_idle_gpu_energy_per_10k_prompts\": 0.06961833412164699, \"pred_non_idle_gpu_energy_per_10k_prompts\": 0.0967266123399029}, {\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 20, \"num_prompts\": 1000, \"total_time\": 276.60654067993164, \"time_per_prompt\": 0.2766065406799316, \"tok_per_sec\": 1439.8213398028113, \"total_out_tok\": 398264.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 398.264, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.204828999682617, \"actual_total_energy_per_10k_prompts\": 0.308037495096015, \"actual_cpu_energy_per_10k_prompts\": 0.03984242121524201, \"actual_gpu_energy_per_10k_prompts\": 0.20436697916011898, \"actual_ram_energy_per_10k_prompts\": 0.063828094720653, \"actual_idle_gpu_energy_per_10k_prompts\": 0.08605536821153428, \"actual_non_idle_gpu_energy_per_10k_prompts\": 0.11831161094858471, \"pred_emissions_per_10k_prompts\": 0.23020513783983068, \"pred_total_energy_per_10k_prompts\": 0.34620006995245883, \"pred_cpu_energy_per_10k_prompts\": 0.04431735676571132, \"pred_gpu_energy_per_10k_prompts\": 0.23090859331677735, \"pred_ram_energy_per_10k_prompts\": 0.07097411986996949, \"pred_idle_gpu_energy_per_10k_prompts\": 0.09571933656214772, \"pred_non_idle_gpu_energy_per_10k_prompts\": 0.13518925675462962}, {\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 30, \"num_prompts\": 1000, \"total_time\": 408.54310035705566, \"time_per_prompt\": 0.40854310035705566, \"tok_per_sec\": 1455.8341567393652, \"total_out_tok\": 594771.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 594.771, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.303342998617103, \"actual_total_energy_per_10k_prompts\": 0.456190371449908, \"actual_cpu_energy_per_10k_prompts\": 0.058845788307587, \"actual_gpu_energy_per_10k_prompts\": 0.303081192631426, \"actual_ram_energy_per_10k_prompts\": 0.094263390510894, \"actual_idle_gpu_energy_per_10k_prompts\": 0.12710229788886176, \"actual_non_idle_gpu_energy_per_10k_prompts\": 0.17597889474256423, \"pred_emissions_per_10k_prompts\": 0.3441750223106859, \"pred_total_energy_per_10k_prompts\": 0.5175966875367988, \"pred_cpu_energy_per_10k_prompts\": 0.06589259647841293, \"pred_gpu_energy_per_10k_prompts\": 0.34618816510368744, \"pred_ram_energy_per_10k_prompts\": 0.10551592595469762, \"pred_idle_gpu_energy_per_10k_prompts\": 0.14232315358780262, \"pred_non_idle_gpu_energy_per_10k_prompts\": 0.20386501151588482}, {\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 40, \"num_prompts\": 1000, \"total_time\": 541.1795892715454, \"time_per_prompt\": 0.5411795892715454, \"tok_per_sec\": 1443.9347223938007, \"total_out_tok\": 781428.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 781.428, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.40250210859611796, \"actual_total_energy_per_10k_prompts\": 0.605313415067829, \"actual_cpu_energy_per_10k_prompts\": 0.077948735114004, \"actual_gpu_energy_per_10k_prompts\": 0.402508618673304, \"actual_ram_energy_per_10k_prompts\": 0.12485606128051999, \"actual_idle_gpu_energy_per_10k_prompts\": 0.16836698332892522, \"actual_non_idle_gpu_energy_per_10k_prompts\": 0.23414163534437876, \"pred_emissions_per_10k_prompts\": 0.45243211608178024, \"pred_total_energy_per_10k_prompts\": 0.6804019741089826, \"pred_cpu_energy_per_10k_prompts\": 0.0863863677896321, \"pred_gpu_energy_per_10k_prompts\": 0.45568929753677784, \"pred_ram_energy_per_10k_prompts\": 0.13832630878257196, \"pred_idle_gpu_energy_per_10k_prompts\": 0.18659093374096597, \"pred_non_idle_gpu_energy_per_10k_prompts\": 0.26909836379581187}, {\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 60, \"num_prompts\": 1000, \"total_time\": 882.3084070682526, \"time_per_prompt\": 0.8823084070682525, \"tok_per_sec\": 1362.8749203417487, \"total_out_tok\": 1202476.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 1202.476, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.663906878731937, \"actual_total_energy_per_10k_prompts\": 0.9984338751762941, \"actual_cpu_energy_per_10k_prompts\": 0.127080853323572, \"actual_gpu_energy_per_10k_prompts\": 0.667853812004844, \"actual_ram_energy_per_10k_prompts\": 0.203499209847878, \"actual_idle_gpu_energy_per_10k_prompts\": 0.2744959488656786, \"actual_non_idle_gpu_energy_per_10k_prompts\": 0.39335786313916543, \"pred_emissions_per_10k_prompts\": 0.6966310093358865, \"pred_total_energy_per_10k_prompts\": 1.0476469223329703, \"pred_cpu_energy_per_10k_prompts\": 0.1326148051000261, \"pred_gpu_energy_per_10k_prompts\": 0.7026944074907996, \"pred_ram_energy_per_10k_prompts\": 0.212337709742144, \"pred_idle_gpu_energy_per_10k_prompts\": 0.2864471421763803, \"pred_non_idle_gpu_energy_per_10k_prompts\": 0.41624726531441925}, {\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 90, \"num_prompts\": 1000, \"total_time\": 1447.2658081054688, \"time_per_prompt\": 1.4472658081054688, \"tok_per_sec\": 1241.1707579490435, \"total_out_tok\": 1796304.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 1796.304, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 1.103408511731546, \"actual_total_energy_per_10k_prompts\": 1.65939000116228, \"actual_cpu_energy_per_10k_prompts\": 0.20845373654544302, \"actual_gpu_energy_per_10k_prompts\": 1.117225443668528, \"actual_ram_energy_per_10k_prompts\": 0.333710820948308, \"actual_idle_gpu_energy_per_10k_prompts\": 0.45026047363281246, \"actual_non_idle_gpu_energy_per_10k_prompts\": 0.6669649700357155, \"pred_emissions_per_10k_prompts\": 1.0410386312300637, \"pred_total_energy_per_10k_prompts\": 1.5655934108899856, \"pred_cpu_energy_per_10k_prompts\": 0.19781340590363194, \"pred_gpu_energy_per_10k_prompts\": 1.0510597967903017, \"pred_ram_energy_per_10k_prompts\": 0.3167202081960512, \"pred_idle_gpu_energy_per_10k_prompts\": 0.42728004612743853, \"pred_non_idle_gpu_energy_per_10k_prompts\": 0.6237797506628631}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define chart width and height\n",
    "chart_width = 1000\n",
    "chart_height = 700\n",
    "\n",
    "x_title = 'Average Output Tokens per Prompt'\n",
    "x_data = 'avg_out_tok'\n",
    "test_type = 'Output-tok-vllm'\n",
    "\n",
    "scatter = alt.Chart(df).mark_circle(size=100).encode(\n",
    "    x=alt.X(x_data, title=x_title),\n",
    "    y=alt.Y('actual_emissions_per_10k_prompts', title='Actual Emissions per 10,000 Prompts'),\n",
    "    color = alt.Color('test_type:N', title='Test Type').sort(df['test_type'].unique()),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('parameters', title='Parameters (billions)'),\n",
    "        alt.Tooltip('actual_emissions_per_10k_prompts', title='Actual Emissions per 10,000 Prompts'),\n",
    "        alt.Tooltip('pred_emissions_per_10k_prompts', title='Predicted Emissions per 10,000 Prompts'),\n",
    "        alt.Tooltip('avg_out_tok', title='Average Output Tokens per Prompt'),\n",
    "        alt.Tooltip('avg_in_tok', title='Average Input Tokens per Prompt'),\n",
    "        alt.Tooltip('num_examples', title='Number of Examples'),\n",
    "        alt.Tooltip('num_prompts', title='Number of Prompts'),\n",
    "        alt.Tooltip('model_type', title='Model Type'),\n",
    "        alt.Tooltip('test_type', title='Test Type'),\n",
    "        alt.Tooltip('test_type', title='Test Type'),\n",
    "    ]\n",
    ").properties(\n",
    "    title=f'Actual Emissions for {test_type} per 10,000 Prompts',\n",
    "    width=chart_width,\n",
    "    height=chart_height\n",
    ")\n",
    "\n",
    "# Create line plots for predicted emissions\n",
    "line = alt.Chart(df).mark_line().encode(\n",
    "    x=alt.X(x_data, title=x_title),\n",
    "    y=alt.Y('pred_emissions_per_10k_prompts', title='Predicted Emissions per 10,000 Prompts'),\n",
    "    color = alt.Color('test_type:N', title='Test Type').sort(df['test_type'].unique()),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('parameters', title='Parameters (billions)'),\n",
    "        alt.Tooltip('actual_emissions_per_10k_prompts', title='Actual Emissions per 10,000 Prompts'),\n",
    "        alt.Tooltip('pred_emissions_per_10k_prompts', title='Predicted Emissions per 10,000 Prompts'),\n",
    "        alt.Tooltip('avg_out_tok', title='Average Output Tokens per Prompt'),\n",
    "        alt.Tooltip('avg_in_tok', title='Average Input Tokens per Prompt'),\n",
    "        alt.Tooltip('num_examples', title='Number of Examples'),\n",
    "        alt.Tooltip('num_prompts', title='Number of Prompts'),\n",
    "        alt.Tooltip('model_type', title='Model Type'),\n",
    "        alt.Tooltip('test_type', title='Test Type'),\n",
    "    ]\n",
    ").properties(\n",
    "    width=chart_width,\n",
    "    height=chart_height\n",
    ")\n",
    "\n",
    "# Create a combined chart with overlays for all emission types\n",
    "combined_chart = alt.layer(scatter+line).resolve_scale(\n",
    "    x='independent'\n",
    ").properties(\n",
    "    title='Emissions per Ten-Thousand Prompts by Test Type',\n",
    "    width=chart_width,  # Adjusted width for combined chart\n",
    "    height=chart_height  # Adjusted height for combined chart\n",
    ")\n",
    "\n",
    "combined_chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results in a CSV file\n",
    "df.to_csv('results/data/emission_regression_vllm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
