{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-4-turbo-2024-04-09         | score: 82.6  | 95% CI: (-1.7, 1.9)  | average #tokens: 662\n",
    "# gpt-4-0125-preview             | score: 78.0  | 95% CI: (-2.1, 2.4)  | average #tokens: 619\n",
    "# gemini-1.5-pro-api-preview     | score: 72.0  | 95% CI: (-2.1, 2.5)  | average #tokens: 676\n",
    "# yi-large                       | score: 63.7  | 95% CI: (-2.6, 2.4)  | average #tokens: 626   \n",
    "# claude-3-opus-20240229         | score: 60.4  | 95% CI: (-2.5, 2.5)  | average #tokens: 541\n",
    "# glm-4                          | score: 55.7  | 95% CI: (-2.4, 2.3)  | average #tokens: 622\n",
    "# gpt-4-0314                     | score: 50.0  | 95% CI:  (0.0, 0.0)  | average #tokens: 423\n",
    "# gemini-1.5-flash-api-preview   | score: 49.6  | 95% CI: (-2.2, 2.8)  | average #tokens: 642\n",
    "# claude-3-sonnet-20240229       | score: 46.8  | 95% CI: (-2.3, 2.7)  | average #tokens: 552\n",
    "# claude-3-haiku-20240307        | score: 41.5  | 95% CI: (-2.5, 2.5)  | average #tokens: 505\n",
    "# llama-3-70b-chat-hf            | score: 41.1  | 95% CI: (-2.0, 2.2)  | average #tokens: 583\n",
    "# gpt-4-0613                     | score: 37.9  | 95% CI: (-2.8, 2.4)  | average #tokens: 354\n",
    "# mistral-large-2402             | score: 37.7  | 95% CI: (-2.1, 2.6)  | average #tokens: 400\n",
    "# mixtral-8x22b-instruct-v0.1    | score: 36.4  | 95% CI: (-2.4, 2.6)  | average #tokens: 430\n",
    "# Qwen1.5-72B-Chat               | score: 36.1  | 95% CI: (-2.0, 2.7)  | average #tokens: 474\n",
    "# command-r-plus                 | score: 33.1  | 95% CI: (-2.8, 2.4)  | average #tokens: 541\n",
    "# mistral-medium                 | score: 31.9  | 95% CI: (-1.9, 2.2)  | average #tokens: 485\n",
    "# mistral-next                   | score: 27.4  | 95% CI: (-2.4, 2.4)  | average #tokens: 297\n",
    "# gpt-3.5-turbo-0613             | score: 24.8  | 95% CI: (-1.9, 2.3)  | average #tokens: 401\n",
    "# claude-2.0                     | score: 24.0  | 95% CI: (-1.8, 1.8)  | average #tokens: 295\n",
    "# dbrx-instruct                  | score: 23.9  | 95% CI: (-1.5, 1.5)  | average #tokens: 415\n",
    "# Mixtral-8x7B-Instruct-v0.1     | score: 23.4  | 95% CI: (-2.0, 1.9)  | average #tokens: 457\n",
    "# gpt-3.5-turbo-0125             | score: 23.3  | 95% CI: (-2.2, 1.9)  | average #tokens: 329\n",
    "# Yi-34B-Chat                    | score: 23.1  | 95% CI: (-1.6, 1.8)  | average #tokens: 611\n",
    "# Starling-LM-7B-beta            | score: 23.0  | 95% CI: (-1.8, 1.8)  | average #tokens: 530\n",
    "# claude-2.1                     | score: 22.8  | 95% CI: (-2.3, 1.8)  | average #tokens: 290\n",
    "# Snorkel-Mistral-PairRM-DPO     | score: 20.7  | 95% CI: (-1.8, 2.2)  | average #tokens: 564                       \n",
    "# llama-3-8b-chat-hf             | score: 20.6  | 95% CI: (-2.0, 1.9)  | average #tokens: 585                       \n",
    "# gpt-3.5-turbo-1106             | score: 18.9  | 95% CI: (-1.8, 1.6)  | average #tokens: 285                       \n",
    "# gpt-3.5-turbo-0301             | score: 18.1  | 95% CI: (-1.9, 2.1)  | average #tokens: 334                               \n",
    "# gemini-1.0-pro                 | score: 17.8  | 95% CI: (-1.2, 2.2)  | average #tokens: 322                               \n",
    "# snowflake-arctic-instruct      | score: 17.6  | 95% CI: (-1.8, 1.5)  | average #tokens: 365                                         \n",
    "# command-r                      | score: 17.0  | 95% CI: (-1.7, 1.8)  | average #tokens: 432                                         \n",
    "# phi-3-mini-128k-instruct       | score: 15.4  | 95% CI: (-1.4, 1.4)  | average #tokens: 609                                                    \n",
    "# tulu-2-dpo-70b                 | score: 15.0  | 95% CI: (-1.6, 1.3)  | average #tokens: 550                                                    \n",
    "# Starling-LM-7B-alpha           | score: 12.8  | 95% CI: (-1.6, 1.4)  | average #tokens: 483                                                    \n",
    "# mistral-7b-instruct            | score: 12.6  | 95% CI: (-1.7, 1.4)  | average #tokens: 541                                                                 \n",
    "# gemma-1.1-7b-it                | score: 12.1  | 95% CI: (-1.3, 1.3)  | average #tokens: 341                                                                 \n",
    "# Llama-2-70b-chat-hf            | score: 11.6  | 95% CI: (-1.5, 1.2)  | average #tokens: 595                                                                 \n",
    "# vicuna-33b-v1.3                | score:  8.6  | 95% CI: (-1.1, 1.1)  | average #tokens: 451                                                                 \n",
    "# gemma-7b-it                    | score:  7.5  | 95% CI: (-1.2, 1.3)  | average #tokens: 378                                                                                \n",
    "# Llama-2-7b-chat-hf             | score:  4.6  | 95% CI: (-0.8, 0.8)  | average #tokens: 561                                                                                \n",
    "# gemma-1.1-2b-it                | score:  3.4  | 95% CI: (-0.6, 0.8)  | average #tokens: 316                                                                                \n",
    "# gemma-2b-it                    | score:  3.0  | 95% CI: (-0.6, 0.6)  | average #tokens: 369"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "533.9444444444445\n"
     ]
    }
   ],
   "source": [
    "# AVG Answer Tokens\n",
    "# Data is taken from the Arena Hard Auto Git Repo\n",
    "models_data = {\n",
    "    \"gpt-4-turbo-2024-04-09\": 662,\n",
    "    \"gpt-4-0125-preview\": 619,\n",
    "    \"gpt-4-0314\": 423,\n",
    "    \"gpt-4-0613\": 354,\n",
    "    \"claude-3-opus-20240229\": 541,\n",
    "    \"claude-3-sonnet-20240229\": 552,\n",
    "    \"claude-3-haiku-20240307\": 505,\n",
    "    \"gemini-1.5-pro-api-preview\": 676,\n",
    "    \"gemini-1.5-flash-api-preview\": 642,\n",
    "    \"llama-3-70b-chat-hf\": 583,\n",
    "    \"llama-3-8b-chat-hf\": 585,\n",
    "    \"Llama-2-70b-chat-hf\": 595,\n",
    "    \"Llama-2-7b-chat-hf\": 561,\n",
    "    \"mistral-large-2402\": 400,\n",
    "    \"mistral-medium\": 485,\n",
    "    \"mistral-7b-instruct\": 541,\n",
    "    \"mixtral-8x22b-instruct-v0.1\": 430,\n",
    "    \"Mixtral-8x7B-Instruct-v0.1\": 457\n",
    "}\n",
    "\n",
    "# Calculate the average number of tokens\n",
    "average_tokens = sum(models_data.values()) / len(models_data)\n",
    "print(average_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_questions(question_file: str):\n",
    "    \"\"\"Load questions from a file.\"\"\"\n",
    "    questions = []\n",
    "    with open(question_file, \"r\") as ques_file:\n",
    "        for line in ques_file:\n",
    "            if line:\n",
    "                questions.append(json.loads(line))\n",
    "\n",
    "    question_list = [question[\"turns\"][0][\"content\"] for question in questions]\n",
    "    return question_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use ABC notation to write a melody in the style of a folk tune.\n"
     ]
    }
   ],
   "source": [
    "question_array = load_questions(\"question.jsonl\")\n",
    "print(question_array[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8470, 33047, 92060, 316, 5067, 261, 101104, 306, 290, 2713, 328, 261, 30578, 38203, 13]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "tokens = [tokenizer.encode(prompt) for prompt in question_array]\n",
    "print(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47461 Tokens in a total of 500 questions\n"
     ]
    }
   ],
   "source": [
    "num_input_tokens = sum([len(token) for token in tokens])\n",
    "num_questions = len(tokens)\n",
    "print(f\"{num_input_tokens} Tokens in a total of {num_questions} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================  Expected Costs (GPT-4o)  =========================\n",
      "\n",
      "Expected Input Tokens: \n",
      " 47644 Tokens in a total of 500 questions\n",
      "\n",
      "Expected Output Tokens: \n",
      " 266972.22222222225 Tokens in a total of 500 questions\n",
      "\n",
      "Max Output Tokens: \n",
      " 400000 Tokens in a total of 500 questions\n",
      "\n",
      "\n",
      "-------------------------  Resulting Costs  -------------------------\n",
      "\n",
      "Costs for the Input Tokens (0.005 per 1000 tokens): \n",
      " 0.23822000000000002 USD\n",
      "\n",
      "Costs for the Expected Output Tokens (0.015 per 1000 tokens): \n",
      " 4.004583333333334 USD\n",
      "\n",
      "Costs for the Max Output Tokens (0.015 per 1000 tokens): \n",
      " 5.999999999999999 USD\n",
      "\n",
      "\n",
      "=========================  Total Costs  =========================\n",
      "\n",
      "Total Costs for the Expected Output Tokens: \n",
      " 4.242803333333334 USD\n",
      "\n",
      "Total Costs for the Max Output Tokens: \n",
      " 6.238219999999999 USD\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# gpt-4o rates\n",
    "input_muliply = 0.005 / 1000\n",
    "output_muliply = 0.015 / 1000\n",
    "\n",
    "avg_output_tokens = num_questions * 550\n",
    "max_output_tokens = num_questions * 800 # based on the tokens for the other models 800 seems reasonable\n",
    "\n",
    "input_cost = num_input_tokens * input_muliply\n",
    "avg_output_cost = avg_output_tokens * output_muliply\n",
    "max_output_cost = max_output_tokens * output_muliply\n",
    "\n",
    "print(\"=\"*25 + \"  Expected Costs (GPT-4o)  \" + \"=\"*25 + \"\\n\")\n",
    "print(f\"Expected Input Tokens: \\n {num_input_tokens} Tokens in a total of {num_questions} questions\\n\")\n",
    "print(f\"Expected Output Tokens: \\n {avg_output_tokens} Tokens in a total of {num_questions} questions\\n\")\n",
    "print(f\"Max Output Tokens: \\n {max_output_tokens} Tokens in a total of {num_questions} questions\\n\\n\")\n",
    "print(\"-\"*25 + \"  Resulting Costs  \" + \"-\"*25 + \"\\n\")\n",
    "print(f\"Costs for the Input Tokens (0.005 per 1000 tokens): \\n {input_cost} USD\\n\")\n",
    "print(f\"Costs for the Expected Output Tokens (0.015 per 1000 tokens): \\n {avg_output_cost} USD\\n\")\n",
    "print(f\"Costs for the Max Output Tokens (0.015 per 1000 tokens): \\n {max_output_cost} USD\\n\\n\")\n",
    "print(\"=\"*25 + \"  Total Costs  \" + \"=\"*25 + \"\\n\")\n",
    "print(f\"Total Costs for the Expected Output Tokens: \\n {input_cost + avg_output_cost} USD\\n\")\n",
    "print(f\"Total Costs for the Max Output Tokens: \\n {input_cost + max_output_cost} USD\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
