{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the individual CSV files\n",
    "csv_files_output = {\n",
    "    '1': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_1_examples.csv',\n",
    "    '2': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_2_examples.csv',\n",
    "    '3': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_3_examples.csv',\n",
    "    '5': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_5_examples.csv',\n",
    "    '10': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_10_examples.csv',\n",
    "    '15': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_15_examples.csv',\n",
    "    '20': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_20_examples.csv',\n",
    "    '30': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_30_examples.csv',\n",
    "    '40': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_40_examples.csv',\n",
    "    '60': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_60_examples.csv',\n",
    "    '90': 'emission_data/vllm_output_tok_meta-llama/Meta-Llama-3-8B-Instruct_emission_data_90_examples.csv'\n",
    "}\n",
    "\n",
    "# Read the emissions data\n",
    "emissions_data = pd.read_csv('emissions_output_tok_vllm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store metadata\n",
    "total_time = []\n",
    "time_per_prompt = []\n",
    "tok_per_sec = []\n",
    "parameters_output = []\n",
    "num_examples_output = []\n",
    "num_prompts_output = []\n",
    "total_emissions_output = []\n",
    "cpu_energy_output = []\n",
    "gpu_energy_output = []\n",
    "ram_energy_output = []\n",
    "total_energy_output = []\n",
    "total_output_tokens_output = []\n",
    "total_input_tokens_output = []\n",
    "avg_input_tokens_output = []\n",
    "avg_output_tokens_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and extract metadata from each CSV file\n",
    "for model, file in csv_files_output.items():\n",
    "    data = pd.read_csv(file)\n",
    "    time = data.loc[data['Metric'] == 'Total Time', 'Value'].values[0]\n",
    "    time_p_prompt = data.loc[data['Metric'] == 'AVG. Time / Prompt', 'Value'].values[0] / 1000 #Time is in ms\n",
    "    tok_p_sec = data.loc[data['Metric'] == 'AVG. Tokens / Second', 'Value'].values[0]\n",
    "    prompts = data.loc[data['Metric'] == 'Total Prompts', 'Value'].values[0]\n",
    "    output_tokens = data.loc[data['Metric'] == 'Total Output Tokens', 'Value'].values[0]\n",
    "    input_tokens = data.loc[data['Metric'] == 'Total Input Tokens', 'Value'].values[0]\n",
    "    avg_i_tok = data.loc[data['Metric'] == 'AVG. Input Tokens / Prompt', 'Value'].values[0]\n",
    "    avg_o_tok = data.loc[data['Metric'] == 'AVG. Output Tokens / Prompt', 'Value'].values[0]\n",
    "    total_time.append(float(time))\n",
    "    time_per_prompt.append(float(time_p_prompt))\n",
    "    tok_per_sec.append(float(tok_p_sec))\n",
    "    parameters_output.append(8)\n",
    "    num_examples_output.append(int(model))\n",
    "    num_prompts_output.append(int(prompts))\n",
    "    total_output_tokens_output.append(float(output_tokens))\n",
    "    total_input_tokens_output.append(float(input_tokens))\n",
    "    avg_input_tokens_output.append(float(avg_i_tok))\n",
    "    avg_output_tokens_output.append(float(avg_o_tok))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract emissions data\n",
    "for model in csv_files_output.keys():\n",
    "    model_emissions = emissions_data[emissions_data['project_name'].str.contains(\"vLLM_Inference_1000_prompts_output_tok_\" + model + \"_\")]\n",
    "    total_emissions_output.append(model_emissions['emissions'].values[0])\n",
    "    cpu_energy_output.append(model_emissions['cpu_energy'].values[0])\n",
    "    gpu_energy_output.append(model_emissions['gpu_energy'].values[0])\n",
    "    ram_energy_output.append(model_emissions['ram_energy'].values[0])\n",
    "    total_energy_output.append(model_emissions['energy_consumed'].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[186.66, 186.66, 186.66, 186.66, 186.66, 186.66, 186.66, 186.66, 186.66, 186.66, 186.66]\n",
      "[18.173, 21.523, 40.863, 69.858, 182.237, 288.208, 398.264, 594.771, 781.428, 1202.476, 1796.304]\n",
      "[18173.0, 21523.0, 40863.0, 69858.0, 182237.0, 288208.0, 398264.0, 594771.0, 781428.0, 1202476.0, 1796304.0]\n",
      "[0.0034967513216419, 0.0038288557217628, 0.0047209939368813, 0.0058326642843592, 0.0107183823003618, 0.0155925593968694, 0.0204828999682617, 0.0303342998617103, 0.0402502108596118, 0.0663906878731937, 0.1103408511731546]\n"
     ]
    }
   ],
   "source": [
    "print(avg_input_tokens_output)\n",
    "print(avg_output_tokens_output)\n",
    "print(total_output_tokens_output)\n",
    "print(total_emissions_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for regression and visualization\n",
    "total_time = np.array(total_time)\n",
    "time_per_prompt = np.array(time_per_prompt)\n",
    "tok_per_sec = np.array(tok_per_sec)\n",
    "parameters_output = np.array(parameters_output)\n",
    "num_examples_output = np.array(num_examples_output)\n",
    "num_prompts_output = np.array(num_prompts_output)\n",
    "total_output_tokens_output = np.array(total_output_tokens_output)\n",
    "total_input_tokens_output = np.array(total_input_tokens_output)\n",
    "avg_input_tokens_output = np.array(avg_input_tokens_output)\n",
    "avg_output_tokens_output = np.array(avg_output_tokens_output)\n",
    "total_emissions_output = np.array(total_emissions_output)\n",
    "cpu_energy_output = np.array(cpu_energy_output)\n",
    "gpu_energy_output = np.array(gpu_energy_output)\n",
    "ram_energy_output = np.array(ram_energy_output)\n",
    "total_energy_output = np.array(total_energy_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  49.20257998   51.66708994   63.47096586   80.3412075   144.78470874\n",
      "  211.3589766   276.60654068  408.54310036  541.17958927  882.30840707\n",
      " 1447.26580811]\n"
     ]
    }
   ],
   "source": [
    "print(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate emissions per 10,000 prompts\n",
    "emissions_per_thousand_prompts = {\n",
    "    'Total Emissions Output Tok': total_emissions_output / num_prompts_output * 10_000,\n",
    "    'CPU Energy Output Tok': cpu_energy_output / num_prompts_output * 10_000,\n",
    "    'GPU Energy Output Tok': gpu_energy_output / num_prompts_output * 10_000,\n",
    "    'RAM Energy Output Tok': ram_energy_output / num_prompts_output * 10_000,\n",
    "    'Total Energy Output Tok': total_energy_output / num_prompts_output * 10_000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total Emissions Output Tok': array([0.03496751, 0.03828856, 0.04720994, 0.05832664, 0.10718382,\n",
      "       0.15592559, 0.204829  , 0.303343  , 0.40250211, 0.66390688,\n",
      "       1.10340851]), 'CPU Energy Output Tok': array([0.00708905, 0.00744669, 0.00914967, 0.01157489, 0.02085703,\n",
      "       0.03044518, 0.03984242, 0.05884579, 0.07794874, 0.12708085,\n",
      "       0.20845374]), 'GPU Energy Output Tok': array([0.03413982, 0.03822099, 0.04719003, 0.05759958, 0.10691888,\n",
      "       0.15527704, 0.20436698, 0.30308119, 0.40250862, 0.66785381,\n",
      "       1.11722544]), 'RAM Energy Output Tok': array([0.01135794, 0.01191357, 0.01465821, 0.01854159, 0.03341531,\n",
      "       0.04877059, 0.06382809, 0.09426339, 0.12485606, 0.20349921,\n",
      "       0.33371082]), 'Total Energy Output Tok': array([0.05258682, 0.05758126, 0.07099791, 0.08771606, 0.16119122,\n",
      "       0.23449282, 0.3080375 , 0.45619037, 0.60531342, 0.99843388,\n",
      "       1.65939   ])}\n"
     ]
    }
   ],
   "source": [
    "print(emissions_per_thousand_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 8 8 8 8 8 8 8 8 8 8]\n"
     ]
    }
   ],
   "source": [
    "print(parameters_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform regression analysis\n",
    "def perform_regression(x, y):\n",
    "    x = x.reshape(-1, 1)\n",
    "    model = LinearRegression()\n",
    "    model.fit(x, y)\n",
    "    predicted = model.predict(x)\n",
    "    return model, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Emissions Output Tok - Intercept: -0.0007795195459148752, Coefficient: 0.0005799787512447661\n",
      "CPU Energy Output Tok - Intercept: 0.0005904601720052721, Coefficient: 0.00010979374634339546\n",
      "GPU Energy Output Tok - Intercept: -0.0027304291004494052, Coefficient: 0.0005866435892202829\n",
      "RAM Energy Output Tok - Intercept: 0.0009676678935606475, Coefficient: 0.00017577901084810286\n",
      "Total Energy Output Tok - Intercept: -0.0011723010348828056, Coefficient: 0.0008722163464117812\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "predictions = {}\n",
    "for name, y in emissions_per_thousand_prompts.items():\n",
    "    model, predicted = perform_regression(avg_output_tokens_output, y)\n",
    "    models[name] = model\n",
    "    predictions[name] = predicted\n",
    "    print(f\"{name} - Intercept: {model.intercept_}, Coefficient: {model.coef_[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idle GPU Energy per 10,000 prompts: [0.01530747 0.01607421 0.01974652 0.02499504 0.04504413 0.06575613\n",
      " 0.08605537 0.1271023  0.16836698 0.27449595 0.45026047]\n"
     ]
    }
   ],
   "source": [
    "idle_gpu_power = 28*4 # 28W per GPU, 4 GPUs\n",
    "\n",
    "total_idle_gpu_energy = (idle_gpu_power/1000)*(total_time/3600) # Convert W into kw and s into h\n",
    "idle_gpu_energy_per_thousand_prompts = total_idle_gpu_energy / num_prompts_output * 10_000\n",
    "\n",
    "print(f\"Idle GPU Energy per 10,000 prompts: {idle_gpu_energy_per_thousand_prompts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test types and model types\n",
    "test_types = ['Output-tok-vllm']\n",
    "model_types = ['llama3']\n",
    "\n",
    "# Define the parameters\n",
    "parameters = np.concatenate([parameters_output])\n",
    "num_examples = np.concatenate([num_examples_output])\n",
    "num_prompts = np.concatenate([num_prompts_output])\n",
    "total_out_tok = np.concatenate([total_output_tokens_output])\n",
    "total_in_tok = np.concatenate([total_input_tokens_output])\n",
    "avg_out_tok = np.concatenate([avg_output_tokens_output])\n",
    "avg_in_tok = np.concatenate([avg_input_tokens_output])\n",
    "\n",
    "pred_emissions_per_10k_prompts = predictions['Total Emissions Output Tok']\n",
    "\n",
    "pred_cpu_energy_per_10k_prompts = predictions['CPU Energy Output Tok']\n",
    "\n",
    "pred_gpu_energy_per_10k_prompts = predictions['GPU Energy Output Tok']\n",
    "\n",
    "pred_ram_energy_per_10k_prompts = predictions['RAM Energy Output Tok']\n",
    "\n",
    "pred_total_energy_per_10k_prompts = predictions['Total Energy Output Tok']\n",
    "\n",
    "actual_emissions_per_10k_prompts = emissions_per_thousand_prompts['Total Emissions Output Tok']\n",
    "\n",
    "actual_cpu_energy_per_10k_prompts = emissions_per_thousand_prompts['CPU Energy Output Tok']\n",
    "\n",
    "actual_gpu_energy_per_10k_prompts = emissions_per_thousand_prompts['GPU Energy Output Tok']\n",
    "\n",
    "actual_ram_energy_per_10k_prompts = emissions_per_thousand_prompts['RAM Energy Output Tok']\n",
    "\n",
    "actual_total_energy_per_10k_prompts = emissions_per_thousand_prompts['Total Energy Output Tok']\n",
    "\n",
    "idle_gpu_energy_per_10k_prompts = idle_gpu_energy_per_thousand_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat test types and model types for each data point\n",
    "test_type_column = np.concatenate([\n",
    "    np.repeat(test_types[0], len(parameters_output))\n",
    "])\n",
    "\n",
    "model_type_column = np.concatenate([\n",
    "    np.repeat(model_types[0], len(parameters_output))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_type</th>\n",
       "      <th>model_type</th>\n",
       "      <th>parameters</th>\n",
       "      <th>num_examples</th>\n",
       "      <th>num_prompts</th>\n",
       "      <th>total_time</th>\n",
       "      <th>time_per_prompt</th>\n",
       "      <th>tok_per_sec</th>\n",
       "      <th>total_out_tok</th>\n",
       "      <th>total_in_tok</th>\n",
       "      <th>...</th>\n",
       "      <th>actual_total_energy_per_10k_prompts</th>\n",
       "      <th>actual_cpu_energy_per_10k_prompts</th>\n",
       "      <th>actual_gpu_energy_per_10k_prompts</th>\n",
       "      <th>actual_ram_energy_per_10k_prompts</th>\n",
       "      <th>pred_emissions_per_10k_prompts</th>\n",
       "      <th>pred_total_energy_per_10k_prompts</th>\n",
       "      <th>pred_cpu_energy_per_10k_prompts</th>\n",
       "      <th>pred_gpu_energy_per_10k_prompts</th>\n",
       "      <th>pred_ram_energy_per_10k_prompts</th>\n",
       "      <th>idle_gpu_energy_per_10k_prompts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>49.202580</td>\n",
       "      <td>0.049203</td>\n",
       "      <td>369.350551</td>\n",
       "      <td>18173.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052587</td>\n",
       "      <td>0.007089</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>0.011358</td>\n",
       "      <td>0.009760</td>\n",
       "      <td>0.014678</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>0.004162</td>\n",
       "      <td>0.015307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>51.667090</td>\n",
       "      <td>0.051667</td>\n",
       "      <td>416.570781</td>\n",
       "      <td>21523.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057581</td>\n",
       "      <td>0.007447</td>\n",
       "      <td>0.038221</td>\n",
       "      <td>0.011914</td>\n",
       "      <td>0.011703</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>0.009896</td>\n",
       "      <td>0.004751</td>\n",
       "      <td>0.016074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>63.470966</td>\n",
       "      <td>0.063471</td>\n",
       "      <td>643.806179</td>\n",
       "      <td>40863.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070998</td>\n",
       "      <td>0.009150</td>\n",
       "      <td>0.047190</td>\n",
       "      <td>0.014658</td>\n",
       "      <td>0.022920</td>\n",
       "      <td>0.034469</td>\n",
       "      <td>0.005077</td>\n",
       "      <td>0.021242</td>\n",
       "      <td>0.008151</td>\n",
       "      <td>0.019747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>80.341208</td>\n",
       "      <td>0.080341</td>\n",
       "      <td>869.516431</td>\n",
       "      <td>69858.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087716</td>\n",
       "      <td>0.011575</td>\n",
       "      <td>0.057600</td>\n",
       "      <td>0.018542</td>\n",
       "      <td>0.039737</td>\n",
       "      <td>0.059759</td>\n",
       "      <td>0.008260</td>\n",
       "      <td>0.038251</td>\n",
       "      <td>0.013247</td>\n",
       "      <td>0.024995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>144.784709</td>\n",
       "      <td>0.144785</td>\n",
       "      <td>1258.675737</td>\n",
       "      <td>182237.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161191</td>\n",
       "      <td>0.020857</td>\n",
       "      <td>0.106919</td>\n",
       "      <td>0.033415</td>\n",
       "      <td>0.104914</td>\n",
       "      <td>0.157778</td>\n",
       "      <td>0.020599</td>\n",
       "      <td>0.104178</td>\n",
       "      <td>0.033001</td>\n",
       "      <td>0.045044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>1000</td>\n",
       "      <td>211.358977</td>\n",
       "      <td>0.211359</td>\n",
       "      <td>1363.594793</td>\n",
       "      <td>288208.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234493</td>\n",
       "      <td>0.030445</td>\n",
       "      <td>0.155277</td>\n",
       "      <td>0.048771</td>\n",
       "      <td>0.166375</td>\n",
       "      <td>0.250207</td>\n",
       "      <td>0.032234</td>\n",
       "      <td>0.166345</td>\n",
       "      <td>0.051629</td>\n",
       "      <td>0.065756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>1000</td>\n",
       "      <td>276.606541</td>\n",
       "      <td>0.276607</td>\n",
       "      <td>1439.821340</td>\n",
       "      <td>398264.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308037</td>\n",
       "      <td>0.039842</td>\n",
       "      <td>0.204367</td>\n",
       "      <td>0.063828</td>\n",
       "      <td>0.230205</td>\n",
       "      <td>0.346200</td>\n",
       "      <td>0.044317</td>\n",
       "      <td>0.230909</td>\n",
       "      <td>0.070974</td>\n",
       "      <td>0.086055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>1000</td>\n",
       "      <td>408.543100</td>\n",
       "      <td>0.408543</td>\n",
       "      <td>1455.834157</td>\n",
       "      <td>594771.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456190</td>\n",
       "      <td>0.058846</td>\n",
       "      <td>0.303081</td>\n",
       "      <td>0.094263</td>\n",
       "      <td>0.344175</td>\n",
       "      <td>0.517597</td>\n",
       "      <td>0.065893</td>\n",
       "      <td>0.346188</td>\n",
       "      <td>0.105516</td>\n",
       "      <td>0.127102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>1000</td>\n",
       "      <td>541.179589</td>\n",
       "      <td>0.541180</td>\n",
       "      <td>1443.934722</td>\n",
       "      <td>781428.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605313</td>\n",
       "      <td>0.077949</td>\n",
       "      <td>0.402509</td>\n",
       "      <td>0.124856</td>\n",
       "      <td>0.452432</td>\n",
       "      <td>0.680402</td>\n",
       "      <td>0.086386</td>\n",
       "      <td>0.455689</td>\n",
       "      <td>0.138326</td>\n",
       "      <td>0.168367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>60</td>\n",
       "      <td>1000</td>\n",
       "      <td>882.308407</td>\n",
       "      <td>0.882308</td>\n",
       "      <td>1362.874920</td>\n",
       "      <td>1202476.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998434</td>\n",
       "      <td>0.127081</td>\n",
       "      <td>0.667854</td>\n",
       "      <td>0.203499</td>\n",
       "      <td>0.696631</td>\n",
       "      <td>1.047647</td>\n",
       "      <td>0.132615</td>\n",
       "      <td>0.702694</td>\n",
       "      <td>0.212338</td>\n",
       "      <td>0.274496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Output-tok-vllm</td>\n",
       "      <td>llama3</td>\n",
       "      <td>8</td>\n",
       "      <td>90</td>\n",
       "      <td>1000</td>\n",
       "      <td>1447.265808</td>\n",
       "      <td>1.447266</td>\n",
       "      <td>1241.170758</td>\n",
       "      <td>1796304.0</td>\n",
       "      <td>186660.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.659390</td>\n",
       "      <td>0.208454</td>\n",
       "      <td>1.117225</td>\n",
       "      <td>0.333711</td>\n",
       "      <td>1.041039</td>\n",
       "      <td>1.565593</td>\n",
       "      <td>0.197813</td>\n",
       "      <td>1.051060</td>\n",
       "      <td>0.316720</td>\n",
       "      <td>0.450260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          test_type model_type  parameters  num_examples  num_prompts  \\\n",
       "0   Output-tok-vllm     llama3           8             1         1000   \n",
       "1   Output-tok-vllm     llama3           8             2         1000   \n",
       "2   Output-tok-vllm     llama3           8             3         1000   \n",
       "3   Output-tok-vllm     llama3           8             5         1000   \n",
       "4   Output-tok-vllm     llama3           8            10         1000   \n",
       "5   Output-tok-vllm     llama3           8            15         1000   \n",
       "6   Output-tok-vllm     llama3           8            20         1000   \n",
       "7   Output-tok-vllm     llama3           8            30         1000   \n",
       "8   Output-tok-vllm     llama3           8            40         1000   \n",
       "9   Output-tok-vllm     llama3           8            60         1000   \n",
       "10  Output-tok-vllm     llama3           8            90         1000   \n",
       "\n",
       "     total_time  time_per_prompt  tok_per_sec  total_out_tok  total_in_tok  \\\n",
       "0     49.202580         0.049203   369.350551        18173.0      186660.0   \n",
       "1     51.667090         0.051667   416.570781        21523.0      186660.0   \n",
       "2     63.470966         0.063471   643.806179        40863.0      186660.0   \n",
       "3     80.341208         0.080341   869.516431        69858.0      186660.0   \n",
       "4    144.784709         0.144785  1258.675737       182237.0      186660.0   \n",
       "5    211.358977         0.211359  1363.594793       288208.0      186660.0   \n",
       "6    276.606541         0.276607  1439.821340       398264.0      186660.0   \n",
       "7    408.543100         0.408543  1455.834157       594771.0      186660.0   \n",
       "8    541.179589         0.541180  1443.934722       781428.0      186660.0   \n",
       "9    882.308407         0.882308  1362.874920      1202476.0      186660.0   \n",
       "10  1447.265808         1.447266  1241.170758      1796304.0      186660.0   \n",
       "\n",
       "    ...  actual_total_energy_per_10k_prompts  \\\n",
       "0   ...                             0.052587   \n",
       "1   ...                             0.057581   \n",
       "2   ...                             0.070998   \n",
       "3   ...                             0.087716   \n",
       "4   ...                             0.161191   \n",
       "5   ...                             0.234493   \n",
       "6   ...                             0.308037   \n",
       "7   ...                             0.456190   \n",
       "8   ...                             0.605313   \n",
       "9   ...                             0.998434   \n",
       "10  ...                             1.659390   \n",
       "\n",
       "    actual_cpu_energy_per_10k_prompts  actual_gpu_energy_per_10k_prompts  \\\n",
       "0                            0.007089                           0.034140   \n",
       "1                            0.007447                           0.038221   \n",
       "2                            0.009150                           0.047190   \n",
       "3                            0.011575                           0.057600   \n",
       "4                            0.020857                           0.106919   \n",
       "5                            0.030445                           0.155277   \n",
       "6                            0.039842                           0.204367   \n",
       "7                            0.058846                           0.303081   \n",
       "8                            0.077949                           0.402509   \n",
       "9                            0.127081                           0.667854   \n",
       "10                           0.208454                           1.117225   \n",
       "\n",
       "    actual_ram_energy_per_10k_prompts  pred_emissions_per_10k_prompts  \\\n",
       "0                            0.011358                        0.009760   \n",
       "1                            0.011914                        0.011703   \n",
       "2                            0.014658                        0.022920   \n",
       "3                            0.018542                        0.039737   \n",
       "4                            0.033415                        0.104914   \n",
       "5                            0.048771                        0.166375   \n",
       "6                            0.063828                        0.230205   \n",
       "7                            0.094263                        0.344175   \n",
       "8                            0.124856                        0.452432   \n",
       "9                            0.203499                        0.696631   \n",
       "10                           0.333711                        1.041039   \n",
       "\n",
       "    pred_total_energy_per_10k_prompts  pred_cpu_energy_per_10k_prompts  \\\n",
       "0                            0.014678                         0.002586   \n",
       "1                            0.017600                         0.002954   \n",
       "2                            0.034469                         0.005077   \n",
       "3                            0.059759                         0.008260   \n",
       "4                            0.157778                         0.020599   \n",
       "5                            0.250207                         0.032234   \n",
       "6                            0.346200                         0.044317   \n",
       "7                            0.517597                         0.065893   \n",
       "8                            0.680402                         0.086386   \n",
       "9                            1.047647                         0.132615   \n",
       "10                           1.565593                         0.197813   \n",
       "\n",
       "    pred_gpu_energy_per_10k_prompts  pred_ram_energy_per_10k_prompts  \\\n",
       "0                          0.007931                         0.004162   \n",
       "1                          0.009896                         0.004751   \n",
       "2                          0.021242                         0.008151   \n",
       "3                          0.038251                         0.013247   \n",
       "4                          0.104178                         0.033001   \n",
       "5                          0.166345                         0.051629   \n",
       "6                          0.230909                         0.070974   \n",
       "7                          0.346188                         0.105516   \n",
       "8                          0.455689                         0.138326   \n",
       "9                          0.702694                         0.212338   \n",
       "10                         1.051060                         0.316720   \n",
       "\n",
       "    idle_gpu_energy_per_10k_prompts  \n",
       "0                          0.015307  \n",
       "1                          0.016074  \n",
       "2                          0.019747  \n",
       "3                          0.024995  \n",
       "4                          0.045044  \n",
       "5                          0.065756  \n",
       "6                          0.086055  \n",
       "7                          0.127102  \n",
       "8                          0.168367  \n",
       "9                          0.274496  \n",
       "10                         0.450260  \n",
       "\n",
       "[11 rows x 23 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the dataframe\n",
    "df = pd.DataFrame({\n",
    "    'test_type': test_type_column,\n",
    "    'model_type': model_type_column,\n",
    "    'parameters': parameters,\n",
    "    'num_examples': num_examples,\n",
    "    'num_prompts': num_prompts,\n",
    "    'total_time': total_time,\n",
    "    'time_per_prompt': time_per_prompt,\n",
    "    'tok_per_sec': tok_per_sec,\n",
    "    'total_out_tok': total_out_tok,\n",
    "    'total_in_tok': total_in_tok,\n",
    "    'avg_out_tok': avg_out_tok,\n",
    "    'avg_in_tok': avg_in_tok,\n",
    "    'actual_emissions_per_10k_prompts': actual_emissions_per_10k_prompts,\n",
    "    'actual_total_energy_per_10k_prompts': actual_total_energy_per_10k_prompts,\n",
    "    'actual_cpu_energy_per_10k_prompts': actual_cpu_energy_per_10k_prompts,\n",
    "    'actual_gpu_energy_per_10k_prompts': actual_gpu_energy_per_10k_prompts,\n",
    "    'actual_ram_energy_per_10k_prompts': actual_ram_energy_per_10k_prompts,\n",
    "    'pred_emissions_per_10k_prompts': pred_emissions_per_10k_prompts,\n",
    "    'pred_total_energy_per_10k_prompts': pred_total_energy_per_10k_prompts,\n",
    "    'pred_cpu_energy_per_10k_prompts': pred_cpu_energy_per_10k_prompts,\n",
    "    'pred_gpu_energy_per_10k_prompts': pred_gpu_energy_per_10k_prompts,\n",
    "    'pred_ram_energy_per_10k_prompts': pred_ram_energy_per_10k_prompts,\n",
    "    'idle_gpu_energy_per_10k_prompts': idle_gpu_energy_per_10k_prompts\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-2f59bbdb5fcd4f2483fda79fc49bb27e.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-2f59bbdb5fcd4f2483fda79fc49bb27e.vega-embed details,\n",
       "  #altair-viz-2f59bbdb5fcd4f2483fda79fc49bb27e.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-2f59bbdb5fcd4f2483fda79fc49bb27e\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-2f59bbdb5fcd4f2483fda79fc49bb27e\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-2f59bbdb5fcd4f2483fda79fc49bb27e\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"layer\": [{\"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok-vllm\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual Emissions for Output-tok-vllm per 10,000 Prompts\"}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok-vllm\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}}]}], \"data\": {\"name\": \"data-9d51211465e35ac8feebd364f60f3744\"}, \"height\": 700, \"resolve\": {\"scale\": {\"x\": \"independent\"}}, \"title\": \"Emissions per Ten-Thousand Prompts by Test Type\", \"width\": 1300, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-9d51211465e35ac8feebd364f60f3744\": [{\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 1, \"num_prompts\": 1000, \"total_time\": 49.202579975128174, \"time_per_prompt\": 0.049202579975128176, \"tok_per_sec\": 369.3505505033765, \"total_out_tok\": 18173.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 18.173, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.034967513216419, \"actual_total_energy_per_10k_prompts\": 0.052586817284723995, \"actual_cpu_energy_per_10k_prompts\": 0.0070890547809, \"actual_gpu_energy_per_10k_prompts\": 0.034139819367388996, \"actual_ram_energy_per_10k_prompts\": 0.011357943136433998, \"pred_emissions_per_10k_prompts\": 0.009760434300456259, \"pred_total_energy_per_10k_prompts\": 0.014678486628458491, \"pred_cpu_energy_per_10k_prompts\": 0.0025857419243037975, \"pred_gpu_energy_per_10k_prompts\": 0.007930644846450795, \"pred_ram_energy_per_10k_prompts\": 0.004162099857703221, \"idle_gpu_energy_per_10k_prompts\": 0.015307469325595431}, {\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 2, \"num_prompts\": 1000, \"total_time\": 51.66708993911743, \"time_per_prompt\": 0.05166708993911743, \"tok_per_sec\": 416.5707808464131, \"total_out_tok\": 21523.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 21.523, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.038288557217628, \"actual_total_energy_per_10k_prompts\": 0.057581256923745, \"actual_cpu_energy_per_10k_prompts\": 0.0074466903529230005, \"actual_gpu_energy_per_10k_prompts\": 0.038220993743436994, \"actual_ram_energy_per_10k_prompts\": 0.011913572827385, \"pred_emissions_per_10k_prompts\": 0.011703363117126227, \"pred_total_energy_per_10k_prompts\": 0.01760041138893796, \"pred_cpu_energy_per_10k_prompts\": 0.0029535509745541727, \"pred_gpu_energy_per_10k_prompts\": 0.009895900870338743, \"pred_ram_energy_per_10k_prompts\": 0.004750959544044366, \"idle_gpu_energy_per_10k_prompts\": 0.016074205758836533}, {\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 3, \"num_prompts\": 1000, \"total_time\": 63.47096586227417, \"time_per_prompt\": 0.06347096586227417, \"tok_per_sec\": 643.8061788545764, \"total_out_tok\": 40863.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 40.863, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.047209939368813, \"actual_total_energy_per_10k_prompts\": 0.07099791284113799, \"actual_cpu_energy_per_10k_prompts\": 0.009149670522312, \"actual_gpu_energy_per_10k_prompts\": 0.047190034251997, \"actual_ram_energy_per_10k_prompts\": 0.014658208066829001, \"pred_emissions_per_10k_prompts\": 0.022920152166200004, \"pred_total_energy_per_10k_prompts\": 0.03446907552854181, \"pred_cpu_energy_per_10k_prompts\": 0.005076962028835441, \"pred_gpu_energy_per_10k_prompts\": 0.021241587885859014, \"pred_ram_energy_per_10k_prompts\": 0.008150525613846674, \"idle_gpu_energy_per_10k_prompts\": 0.01974652271270752}, {\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 5, \"num_prompts\": 1000, \"total_time\": 80.34120750427246, \"time_per_prompt\": 0.08034120750427245, \"tok_per_sec\": 869.5164308587847, \"total_out_tok\": 69858.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 69.858, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.058326642843592, \"actual_total_energy_per_10k_prompts\": 0.08771606064932101, \"actual_cpu_energy_per_10k_prompts\": 0.011574894417689, \"actual_gpu_energy_per_10k_prompts\": 0.057599580746293996, \"actual_ram_energy_per_10k_prompts\": 0.018541585485336, \"pred_emissions_per_10k_prompts\": 0.039736636058542, \"pred_total_energy_per_10k_prompts\": 0.05975898849275141, \"pred_cpu_energy_per_10k_prompts\": 0.008260431704062193, \"pred_gpu_energy_per_10k_prompts\": 0.03825131875530112, \"pred_ram_energy_per_10k_prompts\": 0.013247238033387418, \"idle_gpu_energy_per_10k_prompts\": 0.024995042334662546}, {\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 10, \"num_prompts\": 1000, \"total_time\": 144.78470873832703, \"time_per_prompt\": 0.14478470873832702, \"tok_per_sec\": 1258.6757371550984, \"total_out_tok\": 182237.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 182.237, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.10718382300361799, \"actual_total_energy_per_10k_prompts\": 0.16119122001283298, \"actual_cpu_energy_per_10k_prompts\": 0.020857030055324, \"actual_gpu_energy_per_10k_prompts\": 0.106918879312812, \"actual_ram_energy_per_10k_prompts\": 0.033415310644696, \"pred_emissions_per_10k_prompts\": 0.10491406814467757, \"pred_total_energy_per_10k_prompts\": 0.15777778928616096, \"pred_cpu_energy_per_10k_prompts\": 0.02059894312438663, \"pred_gpu_energy_per_10k_prompts\": 0.10417773866828728, \"pred_ram_energy_per_10k_prompts\": 0.03300110749348637, \"idle_gpu_energy_per_10k_prompts\": 0.04504413160747952}, {\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 15, \"num_prompts\": 1000, \"total_time\": 211.35897660255432, \"time_per_prompt\": 0.21135897660255432, \"tok_per_sec\": 1363.5947932410502, \"total_out_tok\": 288208.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 288.208, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.15592559396869402, \"actual_total_energy_per_10k_prompts\": 0.23449281821372298, \"actual_cpu_energy_per_10k_prompts\": 0.030445183376703003, \"actual_gpu_energy_per_10k_prompts\": 0.15527704416598, \"actual_ram_energy_per_10k_prompts\": 0.04877059067104, \"pred_emissions_per_10k_prompts\": 0.1663749963928367, \"pred_total_energy_per_10k_prompts\": 0.25020742773176385, \"pred_cpu_energy_per_10k_prompts\": 0.03223389621814259, \"pred_gpu_energy_per_10k_prompts\": 0.1663449464615499, \"pred_ram_energy_per_10k_prompts\": 0.051628585052070686, \"idle_gpu_energy_per_10k_prompts\": 0.06575612605412802}, {\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 20, \"num_prompts\": 1000, \"total_time\": 276.60654067993164, \"time_per_prompt\": 0.2766065406799316, \"tok_per_sec\": 1439.8213398028113, \"total_out_tok\": 398264.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 398.264, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.204828999682617, \"actual_total_energy_per_10k_prompts\": 0.308037495096015, \"actual_cpu_energy_per_10k_prompts\": 0.03984242121524201, \"actual_gpu_energy_per_10k_prompts\": 0.20436697916011898, \"actual_ram_energy_per_10k_prompts\": 0.063828094720653, \"pred_emissions_per_10k_prompts\": 0.23020513783983068, \"pred_total_energy_per_10k_prompts\": 0.34620006995245883, \"pred_cpu_energy_per_10k_prompts\": 0.04431735676571132, \"pred_gpu_energy_per_10k_prompts\": 0.23090859331677735, \"pred_ram_energy_per_10k_prompts\": 0.07097411986996949, \"idle_gpu_energy_per_10k_prompts\": 0.08605536821153428}, {\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 30, \"num_prompts\": 1000, \"total_time\": 408.54310035705566, \"time_per_prompt\": 0.40854310035705566, \"tok_per_sec\": 1455.8341567393652, \"total_out_tok\": 594771.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 594.771, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.303342998617103, \"actual_total_energy_per_10k_prompts\": 0.456190371449908, \"actual_cpu_energy_per_10k_prompts\": 0.058845788307587, \"actual_gpu_energy_per_10k_prompts\": 0.303081192631426, \"actual_ram_energy_per_10k_prompts\": 0.094263390510894, \"pred_emissions_per_10k_prompts\": 0.3441750223106859, \"pred_total_energy_per_10k_prompts\": 0.5175966875367988, \"pred_cpu_energy_per_10k_prompts\": 0.06589259647841293, \"pred_gpu_energy_per_10k_prompts\": 0.34618816510368744, \"pred_ram_energy_per_10k_prompts\": 0.10551592595469762, \"idle_gpu_energy_per_10k_prompts\": 0.12710229788886176}, {\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 40, \"num_prompts\": 1000, \"total_time\": 541.1795892715454, \"time_per_prompt\": 0.5411795892715454, \"tok_per_sec\": 1443.9347223938007, \"total_out_tok\": 781428.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 781.428, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.40250210859611796, \"actual_total_energy_per_10k_prompts\": 0.605313415067829, \"actual_cpu_energy_per_10k_prompts\": 0.077948735114004, \"actual_gpu_energy_per_10k_prompts\": 0.402508618673304, \"actual_ram_energy_per_10k_prompts\": 0.12485606128051999, \"pred_emissions_per_10k_prompts\": 0.45243211608178024, \"pred_total_energy_per_10k_prompts\": 0.6804019741089826, \"pred_cpu_energy_per_10k_prompts\": 0.0863863677896321, \"pred_gpu_energy_per_10k_prompts\": 0.45568929753677784, \"pred_ram_energy_per_10k_prompts\": 0.13832630878257196, \"idle_gpu_energy_per_10k_prompts\": 0.16836698332892522}, {\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 60, \"num_prompts\": 1000, \"total_time\": 882.3084070682526, \"time_per_prompt\": 0.8823084070682525, \"tok_per_sec\": 1362.8749203417487, \"total_out_tok\": 1202476.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 1202.476, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.663906878731937, \"actual_total_energy_per_10k_prompts\": 0.9984338751762941, \"actual_cpu_energy_per_10k_prompts\": 0.127080853323572, \"actual_gpu_energy_per_10k_prompts\": 0.667853812004844, \"actual_ram_energy_per_10k_prompts\": 0.203499209847878, \"pred_emissions_per_10k_prompts\": 0.6966310093358865, \"pred_total_energy_per_10k_prompts\": 1.0476469223329703, \"pred_cpu_energy_per_10k_prompts\": 0.1326148051000261, \"pred_gpu_energy_per_10k_prompts\": 0.7026944074907996, \"pred_ram_energy_per_10k_prompts\": 0.212337709742144, \"idle_gpu_energy_per_10k_prompts\": 0.2744959488656786}, {\"test_type\": \"Output-tok-vllm\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 90, \"num_prompts\": 1000, \"total_time\": 1447.2658081054688, \"time_per_prompt\": 1.4472658081054688, \"tok_per_sec\": 1241.1707579490435, \"total_out_tok\": 1796304.0, \"total_in_tok\": 186660.0, \"avg_out_tok\": 1796.304, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 1.103408511731546, \"actual_total_energy_per_10k_prompts\": 1.65939000116228, \"actual_cpu_energy_per_10k_prompts\": 0.20845373654544302, \"actual_gpu_energy_per_10k_prompts\": 1.117225443668528, \"actual_ram_energy_per_10k_prompts\": 0.333710820948308, \"pred_emissions_per_10k_prompts\": 1.0410386312300637, \"pred_total_energy_per_10k_prompts\": 1.5655934108899856, \"pred_cpu_energy_per_10k_prompts\": 0.19781340590363194, \"pred_gpu_energy_per_10k_prompts\": 1.0510597967903017, \"pred_ram_energy_per_10k_prompts\": 0.3167202081960512, \"idle_gpu_energy_per_10k_prompts\": 0.45026047363281246}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define chart width and height\n",
    "chart_width = 600\n",
    "chart_height = 350\n",
    "\n",
    "x_title = 'Average Output Tokens per Prompt'\n",
    "x_data = 'avg_out_tok'\n",
    "test_type = 'Output-tok-vllm'\n",
    "\n",
    "scatter = alt.Chart(df).mark_circle(size=100).encode(\n",
    "    x=alt.X(x_data, title=x_title),\n",
    "    y=alt.Y('actual_emissions_per_10k_prompts', title='Actual Emissions per 10,000 Prompts'),\n",
    "    color = alt.Color('test_type:N', title='Test Type').sort(df['test_type'].unique()),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('parameters', title='Parameters (billions)'),\n",
    "        alt.Tooltip('actual_emissions_per_10k_prompts', title='Actual Emissions per 10,000 Prompts'),\n",
    "        alt.Tooltip('pred_emissions_per_10k_prompts', title='Predicted Emissions per 10,000 Prompts'),\n",
    "        alt.Tooltip('avg_out_tok', title='Average Output Tokens per Prompt'),\n",
    "        alt.Tooltip('avg_in_tok', title='Average Input Tokens per Prompt'),\n",
    "        alt.Tooltip('num_examples', title='Number of Examples'),\n",
    "        alt.Tooltip('num_prompts', title='Number of Prompts'),\n",
    "        alt.Tooltip('model_type', title='Model Type'),\n",
    "        alt.Tooltip('test_type', title='Test Type'),\n",
    "        alt.Tooltip('test_type', title='Test Type'),\n",
    "    ]\n",
    ").properties(\n",
    "    title=f'Actual Emissions for {test_type} per 10,000 Prompts',\n",
    "    width=chart_width,\n",
    "    height=chart_height\n",
    ")\n",
    "\n",
    "# Create line plots for predicted emissions\n",
    "line = alt.Chart(df).mark_line().encode(\n",
    "    x=alt.X(x_data, title=x_title),\n",
    "    y=alt.Y('pred_emissions_per_10k_prompts', title='Predicted Emissions per 10,000 Prompts'),\n",
    "    color = alt.Color('test_type:N', title='Test Type').sort(df['test_type'].unique()),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('parameters', title='Parameters (billions)'),\n",
    "        alt.Tooltip('actual_emissions_per_10k_prompts', title='Actual Emissions per 10,000 Prompts'),\n",
    "        alt.Tooltip('pred_emissions_per_10k_prompts', title='Predicted Emissions per 10,000 Prompts'),\n",
    "        alt.Tooltip('avg_out_tok', title='Average Output Tokens per Prompt'),\n",
    "        alt.Tooltip('avg_in_tok', title='Average Input Tokens per Prompt'),\n",
    "        alt.Tooltip('num_examples', title='Number of Examples'),\n",
    "        alt.Tooltip('num_prompts', title='Number of Prompts'),\n",
    "        alt.Tooltip('model_type', title='Model Type'),\n",
    "        alt.Tooltip('test_type', title='Test Type'),\n",
    "    ]\n",
    ").properties(\n",
    "    width=chart_width,\n",
    "    height=chart_height\n",
    ")\n",
    "\n",
    "# Create a combined chart with overlays for all emission types\n",
    "combined_chart = alt.layer(scatter+line).resolve_scale(\n",
    "    x='independent'\n",
    ").properties(\n",
    "    title='Emissions per Ten-Thousand Prompts by Test Type',\n",
    "    width=1300,  # Adjusted width for combined chart\n",
    "    height=700  # Adjusted height for combined chart\n",
    ")\n",
    "\n",
    "combined_chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chart width and height\n",
    "chart_width = 600\n",
    "chart_height = 350\n",
    "\n",
    "llama2_note = 'Note: Emissions normalized to number of output tokens for Llama2 because the Llama2 and Llama3 output differed drastically'\n",
    "\n",
    "# Function to create charts for each test type\n",
    "def create_chart(df, test_type, color, remove_x_title=False):\n",
    "    chart_data = df[df['test_type'] == test_type]\n",
    "\n",
    "    if test_type == 'Output-tok': \n",
    "        x_title = 'Average Output Tokens per Prompt'\n",
    "        x_data = 'avg_out_tok'\n",
    "    elif test_type == 'Input-tok':\n",
    "        x_title = 'Average Input Tokens per Prompt'\n",
    "        x_data = 'avg_in_tok'\n",
    "    elif test_type == 'Llama2 Params':\n",
    "        x_title = 'Parameters (billions)'\n",
    "        x_data = 'parameters'\n",
    "    elif test_type == 'Llama3 Params':\n",
    "        x_title = 'Parameters (billions)'\n",
    "        x_data = 'parameters'\n",
    "    \n",
    "    scatter = alt.Chart(chart_data).mark_circle(size=100).encode(\n",
    "        x=alt.X(x_data, title=x_title),\n",
    "        y=alt.Y('actual_emissions_per_10k_prompts', title='Actual Emissions per 10,000 Prompts'),\n",
    "        color = alt.Color('test_type:N', title='Test Type').sort(df['test_type'].unique()),\n",
    "        tooltip=[\n",
    "            alt.Tooltip('parameters', title='Parameters (billions)'),\n",
    "            alt.Tooltip('actual_emissions_per_10k_prompts', title='Actual Emissions per 10,000 Prompts'),\n",
    "            alt.Tooltip('pred_emissions_per_10k_prompts', title='Predicted Emissions per 10,000 Prompts'),\n",
    "            alt.Tooltip('avg_out_tok', title='Average Output Tokens per Prompt'),\n",
    "            alt.Tooltip('avg_in_tok', title='Average Input Tokens per Prompt'),\n",
    "            alt.Tooltip('num_examples', title='Number of Examples'),\n",
    "            alt.Tooltip('num_prompts', title='Number of Prompts'),\n",
    "            alt.Tooltip('model_type', title='Model Type'),\n",
    "            alt.Tooltip('test_type', title='Test Type'),\n",
    "            alt.Tooltip('test_type', title='Test Type'),\n",
    "        ]\n",
    "    ).properties(\n",
    "        title=f'Actual Emissions for {test_type} per 10,000 Prompts',\n",
    "        width=chart_width,\n",
    "        height=chart_height\n",
    "    )\n",
    "\n",
    "    # Create line plots for predicted emissions\n",
    "    line = alt.Chart(chart_data).mark_line().encode(\n",
    "        x=alt.X(x_data, title=x_title),\n",
    "        y=alt.Y('pred_emissions_per_10k_prompts', title='Predicted Emissions per 10,000 Prompts'),\n",
    "        color = alt.Color('test_type:N', title='Test Type').sort(df['test_type'].unique()),\n",
    "        tooltip=[\n",
    "            alt.Tooltip('parameters', title='Parameters (billions)'),\n",
    "            alt.Tooltip('actual_emissions_per_10k_prompts', title='Actual Emissions per 10,000 Prompts'),\n",
    "            alt.Tooltip('pred_emissions_per_10k_prompts', title='Predicted Emissions per 10,000 Prompts'),\n",
    "            alt.Tooltip('avg_out_tok', title='Average Output Tokens per Prompt'),\n",
    "            alt.Tooltip('avg_in_tok', title='Average Input Tokens per Prompt'),\n",
    "            alt.Tooltip('num_examples', title='Number of Examples'),\n",
    "            alt.Tooltip('num_prompts', title='Number of Prompts'),\n",
    "            alt.Tooltip('model_type', title='Model Type'),\n",
    "            alt.Tooltip('test_type', title='Test Type'),\n",
    "        ]\n",
    "    ).properties(\n",
    "        width=chart_width,\n",
    "        height=chart_height\n",
    "    )\n",
    "    \n",
    "    # Add note for Llama2\n",
    "    if test_type == 'Llama2 Params':\n",
    "        chart_data['Note'] = llama2_note\n",
    "        #print(chart_data)\n",
    "        scatter = scatter.encode(\n",
    "            tooltip=[\n",
    "                alt.Tooltip('parameters', title='Parameters (billions)'),\n",
    "                alt.Tooltip('actual_emissions_per_10k_prompts', title='Actual Emissions per 10,000 Prompts'),\n",
    "                alt.Tooltip('pred_emissions_per_10k_prompts', title='Predicted Emissions per 10,000 Prompts'),\n",
    "                alt.Tooltip('avg_out_tok', title='Average Output Tokens per Prompt'),\n",
    "                alt.Tooltip('avg_in_tok', title='Average Input Tokens per Prompt'),\n",
    "                alt.Tooltip('num_examples', title='Number of Examples'),\n",
    "                alt.Tooltip('num_prompts', title='Number of Prompts'),\n",
    "                alt.Tooltip('model_type', title='Model Type'),\n",
    "                alt.Tooltip('test_type', title='Test Type'),\n",
    "                alt.Tooltip('Note', title='Normalization Note')\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    if remove_x_title:\n",
    "        scatter = scatter.encode(\n",
    "            x=alt.X(x_data, title=None, axis=None),\n",
    "        )\n",
    "        line = line.encode(\n",
    "            x=alt.X(x_data, title=None, axis=None),\n",
    "        )\n",
    "\n",
    "\n",
    "    return scatter + line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9243/2212380206.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chart_data['Note'] = llama2_note\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-f0460f3da24c4ae082823beddbbcc698.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-f0460f3da24c4ae082823beddbbcc698.vega-embed details,\n",
       "  #altair-viz-f0460f3da24c4ae082823beddbbcc698.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-f0460f3da24c4ae082823beddbbcc698\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-f0460f3da24c4ae082823beddbbcc698\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-f0460f3da24c4ae082823beddbbcc698\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"vconcat\": [{\"hconcat\": [{\"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual Emissions for Output-tok per 10,000 Prompts\"}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-c32efc453175a0323079639fc03125b4\"}, \"height\": 350, \"width\": 600}, {\"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual Emissions for Input-tok per 10,000 Prompts\"}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-9fcc7a1f817d5c9ea95e1e50750e95c1\"}, \"height\": 350, \"width\": 600}]}, {\"hconcat\": [{\"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}, {\"field\": \"Note\", \"title\": \"Normalization Note\", \"type\": \"nominal\"}], \"x\": {\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual Emissions for Llama2 Params per 10,000 Prompts\"}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-fadedba24b64f4fa256e03b1bb63cf90\"}, \"height\": 350, \"width\": 600}, {\"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual Emissions for Llama3 Params per 10,000 Prompts\"}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-66b9b8483ecb74decf4dc0c6e75f3c2e\"}, \"height\": 350, \"width\": 600}]}], \"resolve\": {\"scale\": {\"y\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-c32efc453175a0323079639fc03125b4\": [{\"test_type\": \"Output-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 1, \"num_prompts\": 150, \"total_out_tok\": 2704.0, \"total_in_tok\": 27999.0, \"avg_out_tok\": 18.027, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.7175947569881, \"actual_cpu_energy_per_10k_prompts\": 0.3411986538399133, \"actual_gpu_energy_per_10k_prompts\": 0.37330111752954, \"actual_ram_energy_per_10k_prompts\": 0.36467403594864, \"pred_emissions_per_10k_prompts\": 0.3264465027004072, \"pred_cpu_energy_per_10k_prompts\": 0.3264465027004072, \"pred_gpu_energy_per_10k_prompts\": 0.3264465027004072, \"pred_ram_energy_per_10k_prompts\": 0.3264465027004072}, {\"test_type\": \"Output-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 5, \"num_prompts\": 150, \"total_out_tok\": 10958.0, \"total_in_tok\": 27999.0, \"avg_out_tok\": 73.053, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 2.93206607344282, \"actual_cpu_energy_per_10k_prompts\": 1.3810799971297933, \"actual_gpu_energy_per_10k_prompts\": 1.5522774144135267, \"actual_ram_energy_per_10k_prompts\": 1.4761075085550066, \"pred_emissions_per_10k_prompts\": 2.7474535296297633, \"pred_cpu_energy_per_10k_prompts\": 2.7474535296297633, \"pred_gpu_energy_per_10k_prompts\": 2.7474535296297633, \"pred_ram_energy_per_10k_prompts\": 2.7474535296297633}, {\"test_type\": \"Output-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 10, \"num_prompts\": 150, \"total_out_tok\": 26537.0, \"total_in_tok\": 27999.0, \"avg_out_tok\": 176.913, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 7.227773612586493, \"actual_cpu_energy_per_10k_prompts\": 3.410975038538153, \"actual_gpu_energy_per_10k_prompts\": 3.8130650637829397, \"actual_ram_energy_per_10k_prompts\": 3.6456379324887465, \"pred_emissions_per_10k_prompts\": 7.317034996879481, \"pred_cpu_energy_per_10k_prompts\": 7.317034996879481, \"pred_gpu_energy_per_10k_prompts\": 7.317034996879481, \"pred_ram_energy_per_10k_prompts\": 7.317034996879481}, {\"test_type\": \"Output-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 30, \"num_prompts\": 50, \"total_out_tok\": 29389.0, \"total_in_tok\": 9333.0, \"avg_out_tok\": 587.78, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 24.724677756913163, \"actual_cpu_energy_per_10k_prompts\": 11.68430221903522, \"actual_gpu_energy_per_10k_prompts\": 13.0104280094452, \"actual_ram_energy_per_10k_prompts\": 12.48812871774732, \"pred_emissions_per_10k_prompts\": 25.394160249185365, \"pred_cpu_energy_per_10k_prompts\": 25.394160249185365, \"pred_gpu_energy_per_10k_prompts\": 25.394160249185365, \"pred_ram_energy_per_10k_prompts\": 25.394160249185365}, {\"test_type\": \"Output-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 90, \"num_prompts\": 50, \"total_out_tok\": 106229.0, \"total_in_tok\": 9333.0, \"avg_out_tok\": 2124.58, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 93.19251785941725, \"actual_cpu_energy_per_10k_prompts\": 43.35962525777024, \"actual_gpu_energy_per_10k_prompts\": 50.44779913042922, \"actual_ram_energy_per_10k_prompts\": 46.34260225640008, \"pred_emissions_per_10k_prompts\": 93.00953478095279, \"pred_cpu_energy_per_10k_prompts\": 93.00953478095279, \"pred_gpu_energy_per_10k_prompts\": 93.00953478095279, \"pred_ram_energy_per_10k_prompts\": 93.00953478095279}], \"data-9fcc7a1f817d5c9ea95e1e50750e95c1\": [{\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 1, \"num_prompts\": 150, \"total_out_tok\": 2701.0, \"total_in_tok\": 23799.0, \"avg_out_tok\": 18.007, \"avg_in_tok\": 158.66, \"actual_emissions_per_10k_prompts\": 1.3331964486453134, \"actual_cpu_energy_per_10k_prompts\": 0.34934319376017997, \"actual_gpu_energy_per_10k_prompts\": 1.09583639333508, \"actual_ram_energy_per_10k_prompts\": 0.5597830653999732, \"pred_emissions_per_10k_prompts\": 1.3022745433576342, \"pred_cpu_energy_per_10k_prompts\": 1.3022745433576342, \"pred_gpu_energy_per_10k_prompts\": 1.3022745433576342, \"pred_ram_energy_per_10k_prompts\": 1.3022745433576342}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 5, \"num_prompts\": 150, \"total_out_tok\": 2706.0, \"total_in_tok\": 33399.0, \"avg_out_tok\": 18.04, \"avg_in_tok\": 222.66, \"actual_emissions_per_10k_prompts\": 1.3551230332913666, \"actual_cpu_energy_per_10k_prompts\": 0.3497959287815601, \"actual_gpu_energy_per_10k_prompts\": 1.1276016717102402, \"actual_ram_energy_per_10k_prompts\": 0.5605399248038867, \"pred_emissions_per_10k_prompts\": 1.3241337828455204, \"pred_cpu_energy_per_10k_prompts\": 1.3241337828455204, \"pred_gpu_energy_per_10k_prompts\": 1.3241337828455204, \"pred_ram_energy_per_10k_prompts\": 1.3241337828455204}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 10, \"num_prompts\": 150, \"total_out_tok\": 2691.0, \"total_in_tok\": 45249.0, \"avg_out_tok\": 17.94, \"avg_in_tok\": 301.66, \"actual_emissions_per_10k_prompts\": 1.3674481275016532, \"actual_cpu_energy_per_10k_prompts\": 0.34966324762878, \"actual_gpu_energy_per_10k_prompts\": 1.1465055868333598, \"actual_ram_energy_per_10k_prompts\": 0.5603041089298201, \"pred_emissions_per_10k_prompts\": 1.3511162815883802, \"pred_cpu_energy_per_10k_prompts\": 1.3511162815883802, \"pred_gpu_energy_per_10k_prompts\": 1.3511162815883802, \"pred_ram_energy_per_10k_prompts\": 1.3511162815883802}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 20, \"num_prompts\": 150, \"total_out_tok\": 2707.0, \"total_in_tok\": 69399.0, \"avg_out_tok\": 18.047, \"avg_in_tok\": 462.66, \"actual_emissions_per_10k_prompts\": 1.4107526905917533, \"actual_cpu_energy_per_10k_prompts\": 0.36020338171411337, \"actual_gpu_energy_per_10k_prompts\": 1.1841679658521334, \"actual_ram_energy_per_10k_prompts\": 0.5772263055161067, \"pred_emissions_per_10k_prompts\": 1.4061059309250945, \"pred_cpu_energy_per_10k_prompts\": 1.4061059309250945, \"pred_gpu_energy_per_10k_prompts\": 1.4061059309250945, \"pred_ram_energy_per_10k_prompts\": 1.4061059309250945}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 30, \"num_prompts\": 150, \"total_out_tok\": 2705.0, \"total_in_tok\": 94149.0, \"avg_out_tok\": 18.033, \"avg_in_tok\": 627.66, \"actual_emissions_per_10k_prompts\": 1.4599892296728665, \"actual_cpu_energy_per_10k_prompts\": 0.36834101310592665, \"actual_gpu_energy_per_10k_prompts\": 1.2370680215054732, \"actual_ram_energy_per_10k_prompts\": 0.5902342867564133, \"pred_emissions_per_10k_prompts\": 1.4624617827298014, \"pred_cpu_energy_per_10k_prompts\": 1.4624617827298014, \"pred_gpu_energy_per_10k_prompts\": 1.4624617827298014, \"pred_ram_energy_per_10k_prompts\": 1.4624617827298014}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 70, \"num_prompts\": 150, \"total_out_tok\": 2736.0, \"total_in_tok\": 201549.0, \"avg_out_tok\": 18.24, \"avg_in_tok\": 1343.66, \"actual_emissions_per_10k_prompts\": 1.6545238583522335, \"actual_cpu_energy_per_10k_prompts\": 0.4113011142075, \"actual_gpu_energy_per_10k_prompts\": 1.4177818408912268, \"actual_ram_energy_per_10k_prompts\": 0.6591163999395001, \"pred_emissions_per_10k_prompts\": 1.7070120245005305, \"pred_cpu_energy_per_10k_prompts\": 1.7070120245005305, \"pred_gpu_energy_per_10k_prompts\": 1.7070120245005305, \"pred_ram_energy_per_10k_prompts\": 1.7070120245005305}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 210, \"num_prompts\": 150, \"total_out_tok\": 2777.0, \"total_in_tok\": 561549.0, \"avg_out_tok\": 18.513, \"avg_in_tok\": 3743.66, \"actual_emissions_per_10k_prompts\": 2.4330743218152797, \"actual_cpu_energy_per_10k_prompts\": 0.5899271231960332, \"actual_gpu_energy_per_10k_prompts\": 2.123812717567327, \"actual_ram_energy_per_10k_prompts\": 0.9453031689458534, \"pred_emissions_per_10k_prompts\": 2.52673350529627, \"pred_cpu_energy_per_10k_prompts\": 2.52673350529627, \"pred_gpu_energy_per_10k_prompts\": 2.52673350529627, \"pred_ram_energy_per_10k_prompts\": 2.52673350529627}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 350, \"num_prompts\": 150, \"total_out_tok\": 2821.0, \"total_in_tok\": 921549.0, \"avg_out_tok\": 18.807, \"avg_in_tok\": 6143.66, \"actual_emissions_per_10k_prompts\": 3.4121851274647734, \"actual_cpu_energy_per_10k_prompts\": 0.81255489918236, \"actual_gpu_energy_per_10k_prompts\": 3.016946061703067, \"actual_ram_energy_per_10k_prompts\": 1.3020036653968667, \"pred_emissions_per_10k_prompts\": 3.34645498609201, \"pred_cpu_energy_per_10k_prompts\": 3.34645498609201, \"pred_gpu_energy_per_10k_prompts\": 3.34645498609201, \"pred_ram_energy_per_10k_prompts\": 3.34645498609201}], \"data-fadedba24b64f4fa256e03b1bb63cf90\": [{\"test_type\": \"Llama2 Params\", \"model_type\": \"llama2\", \"parameters\": 7, \"num_examples\": 1, \"num_prompts\": 250, \"total_out_tok\": 9404.0, \"total_in_tok\": 42000.0, \"avg_out_tok\": 37.616, \"avg_in_tok\": 168.0, \"actual_emissions_per_10k_prompts\": 1.3237250699399716, \"actual_cpu_energy_per_10k_prompts\": 0.3463242828334113, \"actual_gpu_energy_per_10k_prompts\": 1.0894202655906442, \"actual_ram_energy_per_10k_prompts\": 0.5549743213254642, \"pred_emissions_per_10k_prompts\": 1.3218192114904133, \"pred_cpu_energy_per_10k_prompts\": 1.3218192114904133, \"pred_gpu_energy_per_10k_prompts\": 1.3218192114904133, \"pred_ram_energy_per_10k_prompts\": 1.3218192114904133, \"Note\": \"Note: Emissions normalized to number of output tokens for Llama2 because the Llama2 and Llama3 output differed drastically\"}, {\"test_type\": \"Llama2 Params\", \"model_type\": \"llama2\", \"parameters\": 13, \"num_examples\": 1, \"num_prompts\": 250, \"total_out_tok\": 10252.0, \"total_in_tok\": 42000.0, \"avg_out_tok\": 41.008, \"avg_in_tok\": 168.0, \"actual_emissions_per_10k_prompts\": 1.746947580944433, \"actual_cpu_energy_per_10k_prompts\": 0.44688362843588986, \"actual_gpu_energy_per_10k_prompts\": 1.464195657117174, \"actual_ram_energy_per_10k_prompts\": 0.7161139230843433, \"pred_emissions_per_10k_prompts\": 1.7490540560728935, \"pred_cpu_energy_per_10k_prompts\": 1.7490540560728935, \"pred_gpu_energy_per_10k_prompts\": 1.7490540560728935, \"pred_ram_energy_per_10k_prompts\": 1.7490540560728935, \"Note\": \"Note: Emissions normalized to number of output tokens for Llama2 because the Llama2 and Llama3 output differed drastically\"}, {\"test_type\": \"Llama2 Params\", \"model_type\": \"llama2\", \"parameters\": 70, \"num_examples\": 1, \"num_prompts\": 250, \"total_out_tok\": 9962.0, \"total_in_tok\": 36750.0, \"avg_out_tok\": 39.848, \"avg_in_tok\": 147.0, \"actual_emissions_per_10k_prompts\": 5.807985696285356, \"actual_cpu_energy_per_10k_prompts\": 1.3884783044494993, \"actual_gpu_energy_per_10k_prompts\": 5.121169772053554, \"actual_ram_energy_per_10k_prompts\": 2.2248444426685543, \"pred_emissions_per_10k_prompts\": 5.807785079606454, \"pred_cpu_energy_per_10k_prompts\": 5.807785079606454, \"pred_gpu_energy_per_10k_prompts\": 5.807785079606454, \"pred_ram_energy_per_10k_prompts\": 5.807785079606454, \"Note\": \"Note: Emissions normalized to number of output tokens for Llama2 because the Llama2 and Llama3 output differed drastically\"}], \"data-66b9b8483ecb74decf4dc0c6e75f3c2e\": [{\"test_type\": \"Llama3 Params\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 1, \"num_prompts\": 250, \"total_out_tok\": 4524.0, \"total_in_tok\": 33165.0, \"avg_out_tok\": 18.096, \"avg_in_tok\": 132.66, \"actual_emissions_per_10k_prompts\": 1.349682797176224, \"actual_cpu_energy_per_10k_prompts\": 0.349200310841268, \"actual_gpu_energy_per_10k_prompts\": 1.120993789460976, \"actual_ram_energy_per_10k_prompts\": 0.559561982384068, \"pred_emissions_per_10k_prompts\": 1.3496827971762235, \"pred_cpu_energy_per_10k_prompts\": 1.3496827971762235, \"pred_gpu_energy_per_10k_prompts\": 1.3496827971762235, \"pred_ram_energy_per_10k_prompts\": 1.3496827971762235}, {\"test_type\": \"Llama3 Params\", \"model_type\": \"llama3\", \"parameters\": 70, \"num_examples\": 1, \"num_prompts\": 100, \"total_out_tok\": 1879.0, \"total_in_tok\": 13266.0, \"avg_out_tok\": 18.79, \"avg_in_tok\": 132.66, \"actual_emissions_per_10k_prompts\": 5.327022326297739, \"actual_cpu_energy_per_10k_prompts\": 1.26304502757291, \"actual_gpu_energy_per_10k_prompts\": 4.72424770106179, \"actual_ram_energy_per_10k_prompts\": 2.02389033751271, \"pred_emissions_per_10k_prompts\": 5.32702232629774, \"pred_cpu_energy_per_10k_prompts\": 5.32702232629774, \"pred_gpu_energy_per_10k_prompts\": 5.32702232629774, \"pred_ram_energy_per_10k_prompts\": 5.32702232629774}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.VConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create charts for each test type\n",
    "charts = []\n",
    "colors = ['blue', 'green', 'red', 'purple']\n",
    "for test_type, color in zip(df['test_type'].unique(), colors):\n",
    "    charts.append(create_chart(df, test_type, color))\n",
    "\n",
    "# Arrange the charts in a grid\n",
    "grid_chart = alt.vconcat(*[alt.hconcat(*charts[i:i+2]) for i in range(0, len(charts), 2)]).resolve_scale(\n",
    "    y='independent'\n",
    ")\n",
    "\n",
    "grid_chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9243/2212380206.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chart_data['Note'] = llama2_note\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-efd79bf4825e4fb6a04177b7724fe029.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-efd79bf4825e4fb6a04177b7724fe029.vega-embed details,\n",
       "  #altair-viz-efd79bf4825e4fb6a04177b7724fe029.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-efd79bf4825e4fb6a04177b7724fe029\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-efd79bf4825e4fb6a04177b7724fe029\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-efd79bf4825e4fb6a04177b7724fe029\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"layer\": [{\"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"avg_out_tok\", \"title\": null, \"type\": \"quantitative\"}, \"y\": {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual Emissions for Output-tok per 10,000 Prompts\"}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"avg_out_tok\", \"title\": null, \"type\": \"quantitative\"}, \"y\": {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-c840eb828dcb78b6fb295d1b7edfecf0\"}}, {\"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"avg_in_tok\", \"title\": null, \"type\": \"quantitative\"}, \"y\": {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual Emissions for Input-tok per 10,000 Prompts\"}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"avg_in_tok\", \"title\": null, \"type\": \"quantitative\"}, \"y\": {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-9fcc7a1f817d5c9ea95e1e50750e95c1\"}}, {\"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}, {\"field\": \"Note\", \"title\": \"Normalization Note\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"parameters\", \"title\": null, \"type\": \"quantitative\"}, \"y\": {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual Emissions for Llama2 Params per 10,000 Prompts\"}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"parameters\", \"title\": null, \"type\": \"quantitative\"}, \"y\": {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-fadedba24b64f4fa256e03b1bb63cf90\"}}, {\"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 100}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"parameters\", \"title\": null, \"type\": \"quantitative\"}, \"y\": {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}, \"title\": \"Actual Emissions for Llama3 Params per 10,000 Prompts\"}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"test_type\", \"sort\": [\"Output-tok\", \"Input-tok\", \"Llama2 Params\", \"Llama3 Params\"], \"title\": \"Test Type\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"parameters\", \"title\": \"Parameters (billions)\", \"type\": \"quantitative\"}, {\"field\": \"actual_emissions_per_10k_prompts\", \"title\": \"Actual Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}, {\"field\": \"avg_out_tok\", \"title\": \"Average Output Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"avg_in_tok\", \"title\": \"Average Input Tokens per Prompt\", \"type\": \"quantitative\"}, {\"field\": \"num_examples\", \"title\": \"Number of Examples\", \"type\": \"quantitative\"}, {\"field\": \"num_prompts\", \"title\": \"Number of Prompts\", \"type\": \"quantitative\"}, {\"field\": \"model_type\", \"title\": \"Model Type\", \"type\": \"nominal\"}, {\"field\": \"test_type\", \"title\": \"Test Type\", \"type\": \"nominal\"}], \"x\": {\"axis\": null, \"field\": \"parameters\", \"title\": null, \"type\": \"quantitative\"}, \"y\": {\"field\": \"pred_emissions_per_10k_prompts\", \"title\": \"Predicted Emissions per 10,000 Prompts\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-66b9b8483ecb74decf4dc0c6e75f3c2e\"}}], \"height\": 700, \"resolve\": {\"scale\": {\"x\": \"independent\"}}, \"title\": \"Emissions per Ten-Thousand Prompts by Test Type\", \"width\": 1300, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.17.0.json\", \"datasets\": {\"data-c840eb828dcb78b6fb295d1b7edfecf0\": [{\"test_type\": \"Output-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 1, \"num_prompts\": 150, \"total_out_tok\": 2704.0, \"total_in_tok\": 27999.0, \"avg_out_tok\": 18.027, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 0.7175947569881, \"actual_cpu_energy_per_10k_prompts\": 0.3411986538399133, \"actual_gpu_energy_per_10k_prompts\": 0.37330111752954, \"actual_ram_energy_per_10k_prompts\": 0.36467403594864, \"pred_emissions_per_10k_prompts\": 0.3264465027004072, \"pred_cpu_energy_per_10k_prompts\": 0.3264465027004072, \"pred_gpu_energy_per_10k_prompts\": 0.3264465027004072, \"pred_ram_energy_per_10k_prompts\": 0.3264465027004072}, {\"test_type\": \"Output-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 5, \"num_prompts\": 150, \"total_out_tok\": 10958.0, \"total_in_tok\": 27999.0, \"avg_out_tok\": 73.053, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 2.93206607344282, \"actual_cpu_energy_per_10k_prompts\": 1.3810799971297933, \"actual_gpu_energy_per_10k_prompts\": 1.5522774144135267, \"actual_ram_energy_per_10k_prompts\": 1.4761075085550066, \"pred_emissions_per_10k_prompts\": 2.7474535296297633, \"pred_cpu_energy_per_10k_prompts\": 2.7474535296297633, \"pred_gpu_energy_per_10k_prompts\": 2.7474535296297633, \"pred_ram_energy_per_10k_prompts\": 2.7474535296297633}, {\"test_type\": \"Output-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 10, \"num_prompts\": 150, \"total_out_tok\": 26537.0, \"total_in_tok\": 27999.0, \"avg_out_tok\": 176.913, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 7.227773612586493, \"actual_cpu_energy_per_10k_prompts\": 3.410975038538153, \"actual_gpu_energy_per_10k_prompts\": 3.8130650637829397, \"actual_ram_energy_per_10k_prompts\": 3.6456379324887465, \"pred_emissions_per_10k_prompts\": 7.317034996879481, \"pred_cpu_energy_per_10k_prompts\": 7.317034996879481, \"pred_gpu_energy_per_10k_prompts\": 7.317034996879481, \"pred_ram_energy_per_10k_prompts\": 7.317034996879481}, {\"test_type\": \"Output-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 30, \"num_prompts\": 50, \"total_out_tok\": 29389.0, \"total_in_tok\": 9333.0, \"avg_out_tok\": 587.78, \"avg_in_tok\": 186.66, \"actual_emissions_per_10k_prompts\": 24.724677756913163, \"actual_cpu_energy_per_10k_prompts\": 11.68430221903522, \"actual_gpu_energy_per_10k_prompts\": 13.0104280094452, \"actual_ram_energy_per_10k_prompts\": 12.48812871774732, \"pred_emissions_per_10k_prompts\": 25.394160249185365, \"pred_cpu_energy_per_10k_prompts\": 25.394160249185365, \"pred_gpu_energy_per_10k_prompts\": 25.394160249185365, \"pred_ram_energy_per_10k_prompts\": 25.394160249185365}], \"data-9fcc7a1f817d5c9ea95e1e50750e95c1\": [{\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 1, \"num_prompts\": 150, \"total_out_tok\": 2701.0, \"total_in_tok\": 23799.0, \"avg_out_tok\": 18.007, \"avg_in_tok\": 158.66, \"actual_emissions_per_10k_prompts\": 1.3331964486453134, \"actual_cpu_energy_per_10k_prompts\": 0.34934319376017997, \"actual_gpu_energy_per_10k_prompts\": 1.09583639333508, \"actual_ram_energy_per_10k_prompts\": 0.5597830653999732, \"pred_emissions_per_10k_prompts\": 1.3022745433576342, \"pred_cpu_energy_per_10k_prompts\": 1.3022745433576342, \"pred_gpu_energy_per_10k_prompts\": 1.3022745433576342, \"pred_ram_energy_per_10k_prompts\": 1.3022745433576342}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 5, \"num_prompts\": 150, \"total_out_tok\": 2706.0, \"total_in_tok\": 33399.0, \"avg_out_tok\": 18.04, \"avg_in_tok\": 222.66, \"actual_emissions_per_10k_prompts\": 1.3551230332913666, \"actual_cpu_energy_per_10k_prompts\": 0.3497959287815601, \"actual_gpu_energy_per_10k_prompts\": 1.1276016717102402, \"actual_ram_energy_per_10k_prompts\": 0.5605399248038867, \"pred_emissions_per_10k_prompts\": 1.3241337828455204, \"pred_cpu_energy_per_10k_prompts\": 1.3241337828455204, \"pred_gpu_energy_per_10k_prompts\": 1.3241337828455204, \"pred_ram_energy_per_10k_prompts\": 1.3241337828455204}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 10, \"num_prompts\": 150, \"total_out_tok\": 2691.0, \"total_in_tok\": 45249.0, \"avg_out_tok\": 17.94, \"avg_in_tok\": 301.66, \"actual_emissions_per_10k_prompts\": 1.3674481275016532, \"actual_cpu_energy_per_10k_prompts\": 0.34966324762878, \"actual_gpu_energy_per_10k_prompts\": 1.1465055868333598, \"actual_ram_energy_per_10k_prompts\": 0.5603041089298201, \"pred_emissions_per_10k_prompts\": 1.3511162815883802, \"pred_cpu_energy_per_10k_prompts\": 1.3511162815883802, \"pred_gpu_energy_per_10k_prompts\": 1.3511162815883802, \"pred_ram_energy_per_10k_prompts\": 1.3511162815883802}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 20, \"num_prompts\": 150, \"total_out_tok\": 2707.0, \"total_in_tok\": 69399.0, \"avg_out_tok\": 18.047, \"avg_in_tok\": 462.66, \"actual_emissions_per_10k_prompts\": 1.4107526905917533, \"actual_cpu_energy_per_10k_prompts\": 0.36020338171411337, \"actual_gpu_energy_per_10k_prompts\": 1.1841679658521334, \"actual_ram_energy_per_10k_prompts\": 0.5772263055161067, \"pred_emissions_per_10k_prompts\": 1.4061059309250945, \"pred_cpu_energy_per_10k_prompts\": 1.4061059309250945, \"pred_gpu_energy_per_10k_prompts\": 1.4061059309250945, \"pred_ram_energy_per_10k_prompts\": 1.4061059309250945}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 30, \"num_prompts\": 150, \"total_out_tok\": 2705.0, \"total_in_tok\": 94149.0, \"avg_out_tok\": 18.033, \"avg_in_tok\": 627.66, \"actual_emissions_per_10k_prompts\": 1.4599892296728665, \"actual_cpu_energy_per_10k_prompts\": 0.36834101310592665, \"actual_gpu_energy_per_10k_prompts\": 1.2370680215054732, \"actual_ram_energy_per_10k_prompts\": 0.5902342867564133, \"pred_emissions_per_10k_prompts\": 1.4624617827298014, \"pred_cpu_energy_per_10k_prompts\": 1.4624617827298014, \"pred_gpu_energy_per_10k_prompts\": 1.4624617827298014, \"pred_ram_energy_per_10k_prompts\": 1.4624617827298014}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 70, \"num_prompts\": 150, \"total_out_tok\": 2736.0, \"total_in_tok\": 201549.0, \"avg_out_tok\": 18.24, \"avg_in_tok\": 1343.66, \"actual_emissions_per_10k_prompts\": 1.6545238583522335, \"actual_cpu_energy_per_10k_prompts\": 0.4113011142075, \"actual_gpu_energy_per_10k_prompts\": 1.4177818408912268, \"actual_ram_energy_per_10k_prompts\": 0.6591163999395001, \"pred_emissions_per_10k_prompts\": 1.7070120245005305, \"pred_cpu_energy_per_10k_prompts\": 1.7070120245005305, \"pred_gpu_energy_per_10k_prompts\": 1.7070120245005305, \"pred_ram_energy_per_10k_prompts\": 1.7070120245005305}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 210, \"num_prompts\": 150, \"total_out_tok\": 2777.0, \"total_in_tok\": 561549.0, \"avg_out_tok\": 18.513, \"avg_in_tok\": 3743.66, \"actual_emissions_per_10k_prompts\": 2.4330743218152797, \"actual_cpu_energy_per_10k_prompts\": 0.5899271231960332, \"actual_gpu_energy_per_10k_prompts\": 2.123812717567327, \"actual_ram_energy_per_10k_prompts\": 0.9453031689458534, \"pred_emissions_per_10k_prompts\": 2.52673350529627, \"pred_cpu_energy_per_10k_prompts\": 2.52673350529627, \"pred_gpu_energy_per_10k_prompts\": 2.52673350529627, \"pred_ram_energy_per_10k_prompts\": 2.52673350529627}, {\"test_type\": \"Input-tok\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 350, \"num_prompts\": 150, \"total_out_tok\": 2821.0, \"total_in_tok\": 921549.0, \"avg_out_tok\": 18.807, \"avg_in_tok\": 6143.66, \"actual_emissions_per_10k_prompts\": 3.4121851274647734, \"actual_cpu_energy_per_10k_prompts\": 0.81255489918236, \"actual_gpu_energy_per_10k_prompts\": 3.016946061703067, \"actual_ram_energy_per_10k_prompts\": 1.3020036653968667, \"pred_emissions_per_10k_prompts\": 3.34645498609201, \"pred_cpu_energy_per_10k_prompts\": 3.34645498609201, \"pred_gpu_energy_per_10k_prompts\": 3.34645498609201, \"pred_ram_energy_per_10k_prompts\": 3.34645498609201}], \"data-fadedba24b64f4fa256e03b1bb63cf90\": [{\"test_type\": \"Llama2 Params\", \"model_type\": \"llama2\", \"parameters\": 7, \"num_examples\": 1, \"num_prompts\": 250, \"total_out_tok\": 9404.0, \"total_in_tok\": 42000.0, \"avg_out_tok\": 37.616, \"avg_in_tok\": 168.0, \"actual_emissions_per_10k_prompts\": 1.3237250699399716, \"actual_cpu_energy_per_10k_prompts\": 0.3463242828334113, \"actual_gpu_energy_per_10k_prompts\": 1.0894202655906442, \"actual_ram_energy_per_10k_prompts\": 0.5549743213254642, \"pred_emissions_per_10k_prompts\": 1.3218192114904133, \"pred_cpu_energy_per_10k_prompts\": 1.3218192114904133, \"pred_gpu_energy_per_10k_prompts\": 1.3218192114904133, \"pred_ram_energy_per_10k_prompts\": 1.3218192114904133, \"Note\": \"Note: Emissions normalized to number of output tokens for Llama2 because the Llama2 and Llama3 output differed drastically\"}, {\"test_type\": \"Llama2 Params\", \"model_type\": \"llama2\", \"parameters\": 13, \"num_examples\": 1, \"num_prompts\": 250, \"total_out_tok\": 10252.0, \"total_in_tok\": 42000.0, \"avg_out_tok\": 41.008, \"avg_in_tok\": 168.0, \"actual_emissions_per_10k_prompts\": 1.746947580944433, \"actual_cpu_energy_per_10k_prompts\": 0.44688362843588986, \"actual_gpu_energy_per_10k_prompts\": 1.464195657117174, \"actual_ram_energy_per_10k_prompts\": 0.7161139230843433, \"pred_emissions_per_10k_prompts\": 1.7490540560728935, \"pred_cpu_energy_per_10k_prompts\": 1.7490540560728935, \"pred_gpu_energy_per_10k_prompts\": 1.7490540560728935, \"pred_ram_energy_per_10k_prompts\": 1.7490540560728935, \"Note\": \"Note: Emissions normalized to number of output tokens for Llama2 because the Llama2 and Llama3 output differed drastically\"}, {\"test_type\": \"Llama2 Params\", \"model_type\": \"llama2\", \"parameters\": 70, \"num_examples\": 1, \"num_prompts\": 250, \"total_out_tok\": 9962.0, \"total_in_tok\": 36750.0, \"avg_out_tok\": 39.848, \"avg_in_tok\": 147.0, \"actual_emissions_per_10k_prompts\": 5.807985696285356, \"actual_cpu_energy_per_10k_prompts\": 1.3884783044494993, \"actual_gpu_energy_per_10k_prompts\": 5.121169772053554, \"actual_ram_energy_per_10k_prompts\": 2.2248444426685543, \"pred_emissions_per_10k_prompts\": 5.807785079606454, \"pred_cpu_energy_per_10k_prompts\": 5.807785079606454, \"pred_gpu_energy_per_10k_prompts\": 5.807785079606454, \"pred_ram_energy_per_10k_prompts\": 5.807785079606454, \"Note\": \"Note: Emissions normalized to number of output tokens for Llama2 because the Llama2 and Llama3 output differed drastically\"}], \"data-66b9b8483ecb74decf4dc0c6e75f3c2e\": [{\"test_type\": \"Llama3 Params\", \"model_type\": \"llama3\", \"parameters\": 8, \"num_examples\": 1, \"num_prompts\": 250, \"total_out_tok\": 4524.0, \"total_in_tok\": 33165.0, \"avg_out_tok\": 18.096, \"avg_in_tok\": 132.66, \"actual_emissions_per_10k_prompts\": 1.349682797176224, \"actual_cpu_energy_per_10k_prompts\": 0.349200310841268, \"actual_gpu_energy_per_10k_prompts\": 1.120993789460976, \"actual_ram_energy_per_10k_prompts\": 0.559561982384068, \"pred_emissions_per_10k_prompts\": 1.3496827971762235, \"pred_cpu_energy_per_10k_prompts\": 1.3496827971762235, \"pred_gpu_energy_per_10k_prompts\": 1.3496827971762235, \"pred_ram_energy_per_10k_prompts\": 1.3496827971762235}, {\"test_type\": \"Llama3 Params\", \"model_type\": \"llama3\", \"parameters\": 70, \"num_examples\": 1, \"num_prompts\": 100, \"total_out_tok\": 1879.0, \"total_in_tok\": 13266.0, \"avg_out_tok\": 18.79, \"avg_in_tok\": 132.66, \"actual_emissions_per_10k_prompts\": 5.327022326297739, \"actual_cpu_energy_per_10k_prompts\": 1.26304502757291, \"actual_gpu_energy_per_10k_prompts\": 4.72424770106179, \"actual_ram_energy_per_10k_prompts\": 2.02389033751271, \"pred_emissions_per_10k_prompts\": 5.32702232629774, \"pred_cpu_energy_per_10k_prompts\": 5.32702232629774, \"pred_gpu_energy_per_10k_prompts\": 5.32702232629774, \"pred_ram_energy_per_10k_prompts\": 5.32702232629774}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create charts for each test type\n",
    "stacked_charts = []\n",
    "stacked_df = df[df.num_examples != 90]\n",
    "for test_type, color in zip(stacked_df['test_type'].unique(), colors):\n",
    "    stacked_charts.append(create_chart(stacked_df, test_type, color, remove_x_title=True))\n",
    "\n",
    "\n",
    "# Create a combined chart with overlays for all emission types\n",
    "combined_chart = alt.layer(*stacked_charts).resolve_scale(\n",
    "    x='independent'\n",
    ").properties(\n",
    "    title='Emissions per Ten-Thousand Prompts by Test Type',\n",
    "    width=1300,  # Adjusted width for combined chart\n",
    "    height=700  # Adjusted height for combined chart\n",
    ")\n",
    "\n",
    "combined_chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results in a CSV file\n",
    "result_df.to_csv('results/emission_regression.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
