{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-28T14:32:53.476576Z","iopub.status.busy":"2024-05-28T14:32:53.475920Z","iopub.status.idle":"2024-05-28T14:34:16.126198Z","shell.execute_reply":"2024-05-28T14:34:16.125030Z","shell.execute_reply.started":"2024-05-28T14:32:53.476547Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/envs/pytorch/lib/python3.10/site-packages (4.41.1)\n","Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (0.23.2)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (2.32.1)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: datasets in /opt/conda/envs/pytorch/lib/python3.10/site-packages (2.19.1)\n","Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (3.14.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (16.1.0)\n","Requirement already satisfied: pyarrow-hotfix in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (2.32.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (4.66.4)\n","Requirement already satisfied: xxhash in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n","Requirement already satisfied: aiohttp in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (0.23.2)\n","Requirement already satisfied: packaging in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: bitsandbytes in /opt/conda/envs/pytorch/lib/python3.10/site-packages (0.43.1)\n","Requirement already satisfied: torch in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from bitsandbytes) (2.2.0)\n","Requirement already satisfied: numpy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n","Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->bitsandbytes) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->bitsandbytes) (4.11.0)\n","Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\n","Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: huggingface_hub in /opt/conda/envs/pytorch/lib/python3.10/site-packages (0.23.2)\n","Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface_hub) (3.14.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface_hub) (2024.3.1)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface_hub) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\n","Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface_hub) (2.32.1)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface_hub) (4.11.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.2.2)\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: codecarbon in /opt/conda/envs/pytorch/lib/python3.10/site-packages (2.4.2)\n","Requirement already satisfied: arrow in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from codecarbon) (1.3.0)\n","Requirement already satisfied: click in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from codecarbon) (8.1.7)\n","Requirement already satisfied: pandas in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from codecarbon) (2.2.2)\n","Requirement already satisfied: prometheus-client in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from codecarbon) (0.20.0)\n","Requirement already satisfied: psutil in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from codecarbon) (5.9.8)\n","Requirement already satisfied: py-cpuinfo in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from codecarbon) (9.0.0)\n","Requirement already satisfied: pynvml in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from codecarbon) (11.5.0)\n","Requirement already satisfied: rapidfuzz in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from codecarbon) (3.9.1)\n","Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from codecarbon) (2.32.1)\n","Requirement already satisfied: python-dateutil>=2.7.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from arrow->codecarbon) (2.9.0)\n","Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from arrow->codecarbon) (2.9.0.20240316)\n","Requirement already satisfied: numpy>=1.22.4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->codecarbon) (1.26.4)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->codecarbon) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->codecarbon) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->codecarbon) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->codecarbon) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->codecarbon) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->codecarbon) (2024.2.2)\n","Requirement already satisfied: six>=1.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: ipywidgets in /opt/conda/envs/pytorch/lib/python3.10/site-packages (8.1.3)\n","Requirement already satisfied: comm>=0.1.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n","Requirement already satisfied: ipython>=6.1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ipywidgets) (8.21.0)\n","Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ipywidgets) (5.9.0)\n","Requirement already satisfied: widgetsnbextension~=4.0.11 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ipywidgets) (4.0.11)\n","Requirement already satisfied: jupyterlab-widgets~=3.0.11 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ipywidgets) (3.0.11)\n","Requirement already satisfied: decorator in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n","Requirement already satisfied: jedi>=0.16 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n","Requirement already satisfied: matplotlib-inline in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n","Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\n","Requirement already satisfied: pygments>=2.4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n","Requirement already satisfied: stack-data in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n","Requirement already satisfied: exceptiongroup in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n","Requirement already satisfied: pexpect>4.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n","Requirement already satisfied: wcwidth in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n","Requirement already satisfied: executing>=1.2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n","Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n","Requirement already satisfied: pure-eval in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n","Requirement already satisfied: six>=1.12.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n","Note: you may need to restart the kernel to use updated packages.\n","Requirement already satisfied: accelerate in /opt/conda/envs/pytorch/lib/python3.10/site-packages (0.30.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (5.9.8)\n","Requirement already satisfied: pyyaml in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (2.2.0)\n","Requirement already satisfied: huggingface-hub in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (0.23.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\n","Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.11.0)\n","Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: fsspec in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n","Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.1)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["#%pip install -U torch\n","#%pip install -U pandas\n","#%pip install -U matplotlib\n","#%pip install -U seaborn\n","#%pip install -U trl\n","#%pip install -U numpy\n","#%pip install -U peft\n","%pip install -U transformers\n","%pip install -U datasets\n","%pip install -U bitsandbytes #==0.41.1\n","%pip install -U huggingface_hub\n","#%pip install -U tqdm\n","%pip install -U codecarbon\n","%pip install -U ipywidgets\n","%pip install -U accelerate"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T15:08:38.065871Z","iopub.status.busy":"2024-05-28T15:08:38.065401Z","iopub.status.idle":"2024-05-28T15:08:38.071563Z","shell.execute_reply":"2024-05-28T15:08:38.070616Z","shell.execute_reply.started":"2024-05-28T15:08:38.065841Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import torch\n","import accelerate\n","from accelerate import infer_auto_device_map\n","#from datasets import load_dataset, Dataset\n","#from scipy.special import softmax\n","#from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n","#from sklearn.metrics import accuracy_score, f1_score, log_loss, confusion_matrix\n","from transformers import  AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n","from huggingface_hub import notebook_login, login\n","#import json\n","import pandas as pd\n","import os\n","#from tqdm import tqdm\n","#import random\n","from codecarbon import EmissionsTracker, track_emissions\n","from time import time\n","import csv"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T14:34:21.991798Z","iopub.status.busy":"2024-05-28T14:34:21.991297Z","iopub.status.idle":"2024-05-28T14:34:22.050360Z","shell.execute_reply":"2024-05-28T14:34:22.049314Z","shell.execute_reply.started":"2024-05-28T14:34:21.991768Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["# Check if GPUs are available and move model to GPUs\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T14:34:22.053188Z","iopub.status.busy":"2024-05-28T14:34:22.052936Z","iopub.status.idle":"2024-05-28T14:34:22.066843Z","shell.execute_reply":"2024-05-28T14:34:22.066083Z","shell.execute_reply.started":"2024-05-28T14:34:22.053166Z"},"trusted":true},"outputs":[],"source":["#model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","model_name = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n","#model_name = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n","\n","#model_name = 'meta-llama/Meta-Llama-3-8B'\n","#model_name = 'meta-llama/Llama-2-7b-chat-hf'\n","#model_name = 'meta-llama/Llama-2-13b-chat-hf'\n","#model_name = 'meta-llama/Llama-2-70b-chat-hf'\n","#model_name = \"/kaggle/input/llama-3/transformers/8b-hf/1\"\n","\n","#model_name = \"unsloth/llama-3-70b-bnb-4bit\"\n","\n","#save_path = f\"models/{model_name}\"\n","#print(f\"Saving model to / Loading model from:\\n\\n -> {save_path}\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T14:34:22.068139Z","iopub.status.busy":"2024-05-28T14:34:22.067896Z","iopub.status.idle":"2024-05-28T14:34:22.076019Z","shell.execute_reply":"2024-05-28T14:34:22.075175Z","shell.execute_reply.started":"2024-05-28T14:34:22.068117Z"},"trusted":true},"outputs":[],"source":["#quantization configurations - so you quantize the model while inferencing\n","bnb_config = BitsAndBytesConfig(\n","    #load_in_4bit = True,\n","    load_in_8bit=True,\n","    llm_int8_skip_modules=[\"lm_head\"],\n","    #llm_int8_threshold=4.0,\n","    #llm_int8_has_fp16_weight=True,\n","    #bnb_4bit_qunat_type = \"nf4\",\n","    #bnb_4bit_compute_dtype = torch.float16,\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T14:34:22.077729Z","iopub.status.busy":"2024-05-28T14:34:22.077168Z","iopub.status.idle":"2024-05-28T14:34:22.084008Z","shell.execute_reply":"2024-05-28T14:34:22.083252Z","shell.execute_reply.started":"2024-05-28T14:34:22.077698Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /home/ubuntu/.cache/huggingface/token\n","Login successful\n"]}],"source":["login(token=\"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\")\n","#notebook_login()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["#device_map = infer_auto_device_map(model, max_memory={0: \"20GiB\", 1: \"20GiB\", 2: \"20GiB\", 3: \"20GiB\", \"cpu\": \"100GiB\"})"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T14:34:22.085324Z","iopub.status.busy":"2024-05-28T14:34:22.085052Z","iopub.status.idle":"2024-05-28T14:36:09.831820Z","shell.execute_reply":"2024-05-28T14:36:09.830985Z","shell.execute_reply.started":"2024-05-28T14:34:22.085294Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dbc3591886d4486f9e2ffdbcc84e3a3a","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Load the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.bos_token\n","\n","# Load the model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    #load_in_8bit=True,\n","    #model_path,\n","    #num_labels=4,  # Adjust if necessary based on the specific task\n","    quantization_config=bnb_config,\n","    #device_map=\"auto\",\n","    device_map=\"sequential\",\n","    #torch_dtype=torch.float16,\n","    #device_map=device_map,\n","    #load_in_8bit_fp32_cpu_offload=True,\n",")\n","model.config.pad_token_id = tokenizer.pad_token_id"]},{"cell_type":"markdown","metadata":{},"source":["# Test the Model"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T14:36:09.833289Z","iopub.status.busy":"2024-05-28T14:36:09.832879Z","iopub.status.idle":"2024-05-28T14:36:09.839082Z","shell.execute_reply":"2024-05-28T14:36:09.838108Z","shell.execute_reply.started":"2024-05-28T14:36:09.833264Z"},"trusted":true},"outputs":[],"source":["# List of words that should be used to create sentences.\n","\n","words = [\n","    \"apple\", \"book\", \"car\", \"dog\", \"elephant\", \"forest\", \"guitar\", \"house\", \n","    \"island\", \"jacket\", \"kangaroo\", \"lamp\", \"mountain\", \"notebook\", \"ocean\", \n","    \"pencil\", \"queen\", \"river\", \"star\", \"tree\", \"umbrella\", \"village\", \n","    \"window\", \"xylophone\", \"yacht\", \"zebra\", \"balloon\", \"camera\", \"desert\", \n","    \"engine\", \"flower\", \"garden\", \"honey\", \"iceberg\", \"jungle\", \"kite\", \n","    \"ladder\", \"moon\", \"nest\", \"octopus\", \"pirate\", \"quilt\", \"robot\", \"swan\", \n","    \"telescope\", \"unicorn\", \"violin\", \"whale\", \"x-ray\", \"laptop\"\n","]"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# List of examples to be provided in the system prompt\n","\n","example_sentences = [\n","    \"Why did the bicycle fall over? Because it was two-tired!\",\n","    \"Why don't scientists trust atoms? Because they make up everything!\",\n","    \"Why did the scarecrow win an award? Because he was outstanding in his field!\",\n","    \"Why don't skeletons fight each other? They don't have the guts!\",\n","    \"Why did the computer go to the doctor? Because it had a virus!\",\n","    \"Why was the math book sad? Because it had too many problems!\",\n","    \"Why did the coffee file a police report? It got mugged!\",\n","    \"Why did the tomato turn red? Because it saw the salad dressing!\",\n","    \"Why don't eggs tell jokes? They might crack up!\",\n","    \"Why did the golfer bring two pairs of pants? In case he got a hole in one!\",\n","    \"Why do cows wear bells? Because their horns don't work!\",\n","    \"Why don't some couples go to the gym? Because some relationships don't work out!\",\n","    \"Why did the photo go to jail? It was framed!\",\n","    \"Why don't programmers like nature? It has too many bugs!\",\n","    \"Why did the bicycle stand up by itself? It was two-tired!\",\n","    \"Why did the music teacher need a ladder? To reach the high notes!\",\n","    \"Why did the cookie go to the doctor? Because it felt crummy!\",\n","    \"Why did the student eat his homework? Because his teacher told him it was a piece of cake!\",\n","    \"Why don't oysters donate to charity? Because they are shellfish!\",\n","    \"Why did the broom get a promotion? Because it swept the competition!\",\n","    \"Why don't we see elephants hiding in trees? Because they are so good at it!\",\n","    \"Why did the fish blush? Because it saw the ocean's bottom!\",\n","    \"Why did the barber win the race? Because he knew all the shortcuts!\",\n","    \"Why did the banana go to the doctor? Because it wasn't peeling well!\",\n","    \"Why don't dinosaurs talk? Because they are extinct!\",\n","    \"Why did the clock go back to school? To learn about time management!\",\n","    \"Why did the farmer win an award? Because he was outstanding in his field!\",\n","    \"Why did the astronaut break up with his girlfriend? He needed space!\",\n","    \"Why did the shoe go to the party alone? Because it didn't want to be a pair!\",\n","    \"Why did the tree go to the dentist? To get its roots checked!\",\n","    \"Why did the calendar go on a diet? It wanted to lose some days!\",\n","    \"Why did the stadium get hot? All the fans left!\",\n","    \"Why did the belt go to jail? It held up a pair of pants!\",\n","    \"Why did the cookie cry? Because its mom was a wafer too long!\",\n","    \"Why did the cow jump over the moon? To get to the milky way!\",\n","    \"Why did the skeleton go to the party alone? He had no body to go with!\",\n","    \"Why did the grape stop in the middle of the road? Because it ran out of juice!\",\n","    \"Why did the bee get married? Because he found his honey!\",\n","    \"Why did the soccer ball quit the team? It was tired of being kicked around!\",\n","    \"Why did the traffic light turn red? You would too if you had to change in the middle of the street!\",\n","    \"Why did the scarecrow become a successful neurosurgeon? He was outstanding in his field!\",\n","    \"Why did the chicken cross the playground? To get to the other slide!\",\n","    \"Why did the belt get a promotion? It was a cinch!\",\n","    \"Why did the teacher wear sunglasses? Because her students were so bright!\",\n","    \"Why did the clock get kicked out of class? It was tocking too much!\",\n","    \"Why did the frog call his insurance company? He had a jump in his car!\",\n","    \"Why did the keyboard get a speeding ticket? It had a problem with the space bar!\",\n","    \"Why did the painting go to art school? It wanted to brush up on its skills!\",\n","    \"Why did the scientist install a knocker on his door? He wanted to win the No-bell prize!\",\n","    \"Why did the tomato turn green? Because it was embarrassed to ketchup!\",\n","    \"Why did the pencil go to jail? It was caught in a sketchy situation!\",\n","    \"Why did the music note need a loan? It needed some major funding!\",\n","    \"Why did the fisherman put peanut butter into the sea? To go with the jellyfish!\",\n","    \"Why did the bicycle bring a map? Because it didn't want to get lost on its wheel-y big adventure!\",\n","    \"Why did the pizza go to the party? It wanted to slice up the dance floor!\",\n","    \"Why did the phone sit on a bench? It wanted to recharge its batteries!\",\n","    \"Why did the blanket get arrested? It was covering up a crime!\",\n","    \"Why did the chef go to jail? Because he beat the eggs and whipped the cream!\",\n","    \"Why did the sandwich go to the beach? To get a little bologna-sun!\",\n","    \"Why did the banker switch careers? He lost interest!\",\n","    \"Why did the shovel go to therapy? It had too much dirt on its mind!\",\n","    \"Why did the soccer player bring string to the game? So he could tie the score!\",\n","    \"Why did the alarm clock break up with the pillow? It couldn't handle the pressure!\",\n","    \"Why did the frog take the bus to work? His car got toad away!\",\n","    \"Why did the dentist become a gardener? He wanted to brush up on his roots!\",\n","    \"Why did the light bulb fail his test? He wasn't too bright!\",\n","    \"Why did the ocean break up with the shore? It needed some space to tide things over!\",\n","    \"Why did the shoe store close down? It lost its sole!\",\n","    \"Why did the banana go out with the prune? Because it couldn't find a date!\",\n","    \"Why did the carpenter become a musician? He wanted to nail every note!\"\n","]\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Approach with Pipeline"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T15:03:23.481846Z","iopub.status.busy":"2024-05-28T15:03:23.481083Z","iopub.status.idle":"2024-05-28T15:03:23.486359Z","shell.execute_reply":"2024-05-28T15:03:23.485531Z","shell.execute_reply.started":"2024-05-28T15:03:23.481807Z"},"trusted":true},"outputs":[],"source":["# Initialize the pipeline\n","text_generation_pipeline = pipeline('text-generation', model=model, tokenizer=tokenizer, device_map=\"auto\")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T16:22:58.058744Z","iopub.status.busy":"2024-05-28T16:22:58.057871Z","iopub.status.idle":"2024-05-28T16:22:58.068148Z","shell.execute_reply":"2024-05-28T16:22:58.067150Z","shell.execute_reply.started":"2024-05-28T16:22:58.058711Z"},"trusted":true},"outputs":[],"source":["def query_model(word, temperature=0.6, max_length=48, num_examples=1, multiply_by=1):\n","    start_time = time()\n","    \n","    example_part = \"\\n - \".join(example_sentences[:num_examples])\n","\n","    # Repeat the content of example_part by the number specified in multiply\n","    repeated_example_part = (example_part + \"\\n - \") * multiply_by\n","\n","    # Remove the trailing extra separator\n","    repeated_example_part = repeated_example_part.rstrip(\"\\n - \")\n","\n","    system_message = f\"\"\"\n","    You are an AI assistant designed to tell jokes for a given word.\n","    Generate a short joke that is exactly 2 short sentences long. Do not include any follow-up questions or explanations.\n","\n","    For example, a joke for the word \"yogurt\":\n","    'Why did the yogurt go to the art gallery? Because it wanted to be cultured!'\n","\n","    Here are a few further examples: \n","    - {repeated_example_part}\n","\n","    \n","    Be as close as possible to the example jokes!\n","    Respond with only the joke itself. Do not include any introductory phrases.\n","\n","    \"\"\"\n","    \n","    user_message = \"Question: \" + f'Now, tell me one short joke for the word: \"{word}\"\\n' + \" Answer:\"\n","    \n","    messages = [\n","        {\"role\": \"system\", \"content\": system_message},\n","        {\"role\": \"user\", \"content\": user_message},\n","    ]\n","    prompt = tokenizer.apply_chat_template(\n","        messages, \n","        tokenize=False, \n","        add_generation_prompt=True\n","    )\n","    \n","    # Tokenize the prompt to count input tokens\n","    input_tokens = tokenizer.encode(prompt)\n","    input_token_count = len(input_tokens)\n","    \n","    terminators = [\n","        tokenizer.eos_token_id,\n","        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n","    ]\n","    sequences = text_generation_pipeline(\n","        prompt,\n","        do_sample=True,\n","        top_p=0.9,\n","        temperature=temperature,\n","        eos_token_id=terminators,\n","        max_new_tokens=max_length,\n","        return_full_text=False,\n","        pad_token_id=model.config.eos_token_id\n","    )\n","    answer = sequences[0]['generated_text']\n","    \n","    # Tokenize the answer to count output tokens\n","    output_tokens = tokenizer.encode(answer)\n","    output_token_count = len(output_tokens)\n","    \n","    end_time = time()\n","    ttime = f\"Total time: {round(end_time-start_time, 2)} sec.\"\n","\n","    return (\n","        user_message + \"\\n\\n\" + answer + \"\\n\\n\" + ttime,\n","        input_token_count,\n","        output_token_count\n","    )"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T16:37:32.187584Z","iopub.status.busy":"2024-05-28T16:37:32.186903Z","iopub.status.idle":"2024-05-28T16:54:12.587373Z","shell.execute_reply":"2024-05-28T16:54:12.586363Z","shell.execute_reply.started":"2024-05-28T16:37:32.187554Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["========== INFERENCE TEST with 70 EXAMPLES ==========\n","\n","\n","        Starting Test with 3 Runs and 50 Words/Run. \n","\n","        Total Prompts: 150\n","\n","\n","        Expected Runtime: 25.0min \n","\n","        Expected Input Tokens: 15450 \n","\n","        Expected Output Tokens: 4500 \n","\n","\n","        \n","========================= Run 1 =========================\n","\n","\n","--------------- Run 1 - Word 1 ---------------\n","Question: Now, tell me one short joke for the word: \"apple\"\n"," Answer:\n","\n","Why did the apple join the gym? To get some core strength!\n","\n","Total time: 9.22 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 15\n","\n","\n","--------------- Run 1 - Word 2 ---------------\n","Question: Now, tell me one short joke for the word: \"book\"\n"," Answer:\n","\n","Why did the book go to therapy? Because it had a bad chapter in its life!\n","\n","Total time: 11.03 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 19\n","\n","\n","--------------- Run 1 - Word 3 ---------------\n","Question: Now, tell me one short joke for the word: \"car\"\n"," Answer:\n","\n","Why did the car go to the doctor? Because it was feeling a little \"car\"-sick!\n","\n","Total time: 12.47 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 22\n","\n","\n","--------------- Run 1 - Word 4 ---------------\n","Question: Now, tell me one short joke for the word: \"dog\"\n"," Answer:\n","\n","Why did the dog go to the vet? Because it was feeling a little ruff!\n","\n","Total time: 11.08 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 19\n","\n","\n","--------------- Run 1 - Word 5 ---------------\n","Question: Now, tell me one short joke for the word: \"elephant\"\n"," Answer:\n","\n","Why did the elephant quit the circus? Because it was tired of working for peanuts!\n","\n","Total time: 10.58 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 18\n","\n","\n","--------------- Run 1 - Word 6 ---------------\n","Question: Now, tell me one short joke for the word: \"forest\"\n"," Answer:\n","\n","Why did the tree go to the party in the forest? Because it wanted to branch out!\n","\n","Total time: 11.36 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 20\n","\n","\n","--------------- Run 1 - Word 7 ---------------\n","Question: Now, tell me one short joke for the word: \"guitar\"\n"," Answer:\n","\n","Why did the guitar go to the doctor? Because it was feeling a little out of tune!\n","\n","Total time: 11.52 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 1 - Word 8 ---------------\n","Question: Now, tell me one short joke for the word: \"house\"\n"," Answer:\n","\n","Why did the house go to therapy? Because it had a few walls to work through!\n","\n","Total time: 10.96 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 19\n","\n","\n","--------------- Run 1 - Word 9 ---------------\n","Question: Now, tell me one short joke for the word: \"island\"\n"," Answer:\n","\n","Why did the island go to therapy? Because it was feeling a little \"dis-connected\"!\n","\n","Total time: 11.34 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 1 - Word 10 ---------------\n","Question: Now, tell me one short joke for the word: \"jacket\"\n"," Answer:\n","\n","Why did the jacket go to therapy? Because it was feeling a little \"buttoned\" up!\n","\n","Total time: 11.99 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 21\n","\n","\n","--------------- Run 1 - Word 11 ---------------\n","Question: Now, tell me one short joke for the word: \"kangaroo\"\n"," Answer:\n","\n","Why did the kangaroo quit his job? Because it was hopping mad!\n","\n","Total time: 9.74 sec.\n","\n","\n","Input tokens: 1345, Output tokens: 16\n","\n","\n","--------------- Run 1 - Word 12 ---------------\n","Question: Now, tell me one short joke for the word: \"lamp\"\n"," Answer:\n","\n","Why did the lamp go to therapy? Because it was feeling a little dim!\n","\n","Total time: 10.11 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 17\n","\n","\n","--------------- Run 1 - Word 13 ---------------\n","Question: Now, tell me one short joke for the word: \"mountain\"\n"," Answer:\n","\n","Why did the mountain go to the party? Because it was a peak performer!\n","\n","Total time: 10.13 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 17\n","\n","\n","--------------- Run 1 - Word 14 ---------------\n","Question: Now, tell me one short joke for the word: \"notebook\"\n"," Answer:\n","\n","Why did the notebook go to therapy? Because it had a lot of pages to work through!\n","\n","Total time: 11.3 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 1 - Word 15 ---------------\n","Question: Now, tell me one short joke for the word: \"ocean\"\n"," Answer:\n","\n","Why did the ocean go to therapy? Because it was feeling a little \"drained\"!\n","\n","Total time: 11.47 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 1 - Word 16 ---------------\n","Question: Now, tell me one short joke for the word: \"pencil\"\n"," Answer:\n","\n","Why did the pencil break up with the eraser? It was a sharp move!\n","\n","Total time: 10.68 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 18\n","\n","\n","--------------- Run 1 - Word 17 ---------------\n","Question: Now, tell me one short joke for the word: \"queen\"\n"," Answer:\n","\n","Why did the queen go to the doctor? Because she had a royal pain!\n","\n","Total time: 10.22 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 17\n","\n","\n","--------------- Run 1 - Word 18 ---------------\n","Question: Now, tell me one short joke for the word: \"river\"\n"," Answer:\n","\n","Why did the river go to therapy? Because it was feeling drained!\n","\n","Total time: 9.29 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 15\n","\n","\n","--------------- Run 1 - Word 19 ---------------\n","Question: Now, tell me one short joke for the word: \"star\"\n"," Answer:\n","\n","Why did the star go to the doctor? Because it was feeling a little dim!\n","\n","Total time: 10.6 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 18\n","\n","\n","--------------- Run 1 - Word 20 ---------------\n","Question: Now, tell me one short joke for the word: \"tree\"\n"," Answer:\n","\n","Why did the tree go to the party? Because it wanted to branch out!\n","\n","Total time: 10.06 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 17\n","\n","\n","--------------- Run 1 - Word 21 ---------------\n","Question: Now, tell me one short joke for the word: \"umbrella\"\n"," Answer:\n","\n","Why did the umbrella go to therapy? Because it was feeling a little \"under the weather\"!\n","\n","Total time: 11.88 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 21\n","\n","\n","--------------- Run 1 - Word 22 ---------------\n","Question: Now, tell me one short joke for the word: \"village\"\n"," Answer:\n","\n","Why did the village go to therapy? Because it was feeling a little \"dis-connected\"!\n","\n","Total time: 11.35 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 1 - Word 23 ---------------\n","Question: Now, tell me one short joke for the word: \"window\"\n"," Answer:\n","\n","Why did the window go to therapy? Because it was feeling a little cracked!\n","\n","Total time: 10.09 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 17\n","\n","\n","--------------- Run 1 - Word 24 ---------------\n","Question: Now, tell me one short joke for the word: \"xylophone\"\n"," Answer:\n","\n","Why did the xylophone go to therapy? Because it had a lot of hang-ups!\n","\n","Total time: 11.47 sec.\n","\n","\n","Input tokens: 1345, Output tokens: 20\n","\n","\n","--------------- Run 1 - Word 25 ---------------\n","Question: Now, tell me one short joke for the word: \"yacht\"\n"," Answer:\n","\n","Why did the yacht go to the party? Because it was a reel good time!\n","\n","Total time: 10.67 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 18\n","\n","\n","--------------- Run 1 - Word 26 ---------------\n","Question: Now, tell me one short joke for the word: \"zebra\"\n"," Answer:\n","\n","Why did the zebra refuse to share his stripes? Because he wanted to keep them to himself!\n","\n","Total time: 11.92 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 21\n","\n","\n","--------------- Run 1 - Word 27 ---------------\n","Question: Now, tell me one short joke for the word: \"balloon\"\n"," Answer:\n","\n","Why did the balloon go to the party? Because it was a blowout!\n","\n","Total time: 10.17 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 17\n","\n","\n","--------------- Run 1 - Word 28 ---------------\n","Question: Now, tell me one short joke for the word: \"camera\"\n"," Answer:\n","\n","Why did the camera go to therapy? Because it had a lot of focus issues!\n","\n","Total time: 10.58 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 18\n","\n","\n","--------------- Run 1 - Word 29 ---------------\n","Question: Now, tell me one short joke for the word: \"desert\"\n"," Answer:\n","\n","Why did the desert go to therapy? Because it was feeling a little dry!\n","\n","Total time: 10.1 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 17\n","\n","\n","--------------- Run 1 - Word 30 ---------------\n","Question: Now, tell me one short joke for the word: \"engine\"\n"," Answer:\n","\n","Why did the engine go to therapy? Because it was feeling a little \"car-sick\"!\n","\n","Total time: 11.87 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 21\n","\n","\n","--------------- Run 1 - Word 31 ---------------\n","Question: Now, tell me one short joke for the word: \"flower\"\n"," Answer:\n","\n","Why did the flower go to the party? Because it was a blooming good time!\n","\n","Total time: 11.04 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 19\n","\n","\n","--------------- Run 1 - Word 32 ---------------\n","Question: Now, tell me one short joke for the word: \"garden\"\n"," Answer:\n","\n","Why did the garden go to therapy? Because it was feeling a little wilted!\n","\n","Total time: 10.51 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 18\n","\n","\n","--------------- Run 1 - Word 33 ---------------\n","Question: Now, tell me one short joke for the word: \"honey\"\n"," Answer:\n","\n","Why did the honey go to the doctor? Because it was feeling a little sticky!\n","\n","Total time: 10.63 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 18\n","\n","\n","--------------- Run 1 - Word 34 ---------------\n","Question: Now, tell me one short joke for the word: \"iceberg\"\n"," Answer:\n","\n","Why did the Iceberg go to therapy? Because it was feeling a little flaky!\n","\n","Total time: 11.02 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 19\n","\n","\n","--------------- Run 1 - Word 35 ---------------\n","Question: Now, tell me one short joke for the word: \"jungle\"\n"," Answer:\n","\n","Why did the jungle go to the party? Because it was a wild time!\n","\n","Total time: 10.16 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 17\n","\n","\n","--------------- Run 1 - Word 36 ---------------\n","Question: Now, tell me one short joke for the word: \"kite\"\n"," Answer:\n","\n","Why did the kite go to the party? Because it was a blowout!\n","\n","Total time: 10.16 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 17\n","\n","\n","--------------- Run 1 - Word 37 ---------------\n","Question: Now, tell me one short joke for the word: \"ladder\"\n"," Answer:\n","\n","Why did the ladder go to therapy? Because it had a lot of steps to work through!\n","\n","Total time: 11.5 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 1 - Word 38 ---------------\n","Question: Now, tell me one short joke for the word: \"moon\"\n"," Answer:\n","\n","Why did the astronaut break up with his girlfriend? He needed space!\n","\n","Total time: 9.4 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 15\n","\n","\n","--------------- Run 1 - Word 39 ---------------\n","Question: Now, tell me one short joke for the word: \"nest\"\n"," Answer:\n","\n","Why did the nest go to therapy? Because it had a fowl temper!\n","\n","Total time: 10.12 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 17\n","\n","\n","--------------- Run 1 - Word 40 ---------------\n","Question: Now, tell me one short joke for the word: \"octopus\"\n"," Answer:\n","\n","Why did the octopus go to the party? Because it was an ink-redible dancer!\n","\n","Total time: 11.5 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 1 - Word 41 ---------------\n","Question: Now, tell me one short joke for the word: \"pirate\"\n"," Answer:\n","\n","Why did the pirate quit his job? Because he was sick of all the arrrr-guments!\n","\n","Total time: 12.08 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 21\n","\n","\n","--------------- Run 1 - Word 42 ---------------\n","Question: Now, tell me one short joke for the word: \"quilt\"\n"," Answer:\n","\n","Why did the quilter go to therapy? Because it was feeling a little \"stitched\"!\n","\n","Total time: 11.84 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 21\n","\n","\n","--------------- Run 1 - Word 43 ---------------\n","Question: Now, tell me one short joke for the word: \"robot\"\n"," Answer:\n","\n","Why did the robot go on a diet? Because it wanted to lose some bytes!\n","\n","Total time: 10.68 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 18\n","\n","\n","--------------- Run 1 - Word 44 ---------------\n","Question: Now, tell me one short joke for the word: \"swan\"\n"," Answer:\n","\n","Why did the swan go to the doctor? Because it had a fowl temper!\n","\n","Total time: 11.0 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 19\n","\n","\n","--------------- Run 1 - Word 45 ---------------\n","Question: Now, tell me one short joke for the word: \"telescope\"\n"," Answer:\n","\n","Why did the telescope go to therapy? Because it was feeling a little star-struck!\n","\n","Total time: 10.95 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 19\n","\n","\n","--------------- Run 1 - Word 46 ---------------\n","Question: Now, tell me one short joke for the word: \"unicorn\"\n"," Answer:\n","\n","Why did the unicorn get kicked out of the movie theater? Because it was using its horn to pop the corn!\n","\n","Total time: 13.32 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 24\n","\n","\n","--------------- Run 1 - Word 47 ---------------\n","Question: Now, tell me one short joke for the word: \"violin\"\n"," Answer:\n","\n","Why was the violinist's computer slow? It had a lot of strings attached!\n","\n","Total time: 10.56 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 18\n","\n","\n","--------------- Run 1 - Word 48 ---------------\n","Question: Now, tell me one short joke for the word: \"whale\"\n"," Answer:\n","\n","Why did the whale go to the ocean party? Because he heard it was a whale of a time!\n","\n","Total time: 12.35 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 22\n","\n","\n","--------------- Run 1 - Word 49 ---------------\n","Question: Now, tell me one short joke for the word: \"x-ray\"\n"," Answer:\n","\n","Why did the x-ray go to therapy? Because it was feeling a little exposed!\n","\n","Total time: 10.55 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 18\n","\n","\n","--------------- Run 1 - Word 50 ---------------\n","Question: Now, tell me one short joke for the word: \"laptop\"\n"," Answer:\n","\n","Why did the laptop go to the doctor? Because it had a virus!\n","\n","Total time: 9.84 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 16\n","\n","\n","\n","\n","========================= Run 2 =========================\n","\n","\n","--------------- Run 2 - Word 1 ---------------\n","Question: Now, tell me one short joke for the word: \"apple\"\n"," Answer:\n","\n","Why did the apple join the gym? To get some core strength!\n","\n","Total time: 9.28 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 15\n","\n","\n","--------------- Run 2 - Word 2 ---------------\n","Question: Now, tell me one short joke for the word: \"book\"\n"," Answer:\n","\n","Why did the book go to therapy? Because it had a bad chapter in its life!\n","\n","Total time: 11.07 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 19\n","\n","\n","--------------- Run 2 - Word 3 ---------------\n","Question: Now, tell me one short joke for the word: \"car\"\n"," Answer:\n","\n","Why did the car go to the doctor? Because it was feeling a little \"car\"-sick!\n","\n","Total time: 12.52 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 22\n","\n","\n","--------------- Run 2 - Word 4 ---------------\n","Question: Now, tell me one short joke for the word: \"dog\"\n"," Answer:\n","\n","Why did the dog go to the vet? Because it was feeling ruff!\n","\n","Total time: 10.23 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 5 ---------------\n","Question: Now, tell me one short joke for the word: \"elephant\"\n"," Answer:\n","\n","Why did the elephant quit the circus? Because it was tired of working for peanuts!\n","\n","Total time: 10.63 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 18\n","\n","\n","--------------- Run 2 - Word 6 ---------------\n","Question: Now, tell me one short joke for the word: \"forest\"\n"," Answer:\n","\n","Why did the forest go to the party? Because it wanted to branch out!\n","\n","Total time: 10.11 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 7 ---------------\n","Question: Now, tell me one short joke for the word: \"guitar\"\n"," Answer:\n","\n","Why did the guitar go to the doctor? Because it was feeling a little out of tune!\n","\n","Total time: 11.57 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 2 - Word 8 ---------------\n","Question: Now, tell me one short joke for the word: \"house\"\n"," Answer:\n","\n","Why did the house go to therapy? Because it had a few walls to work through!\n","\n","Total time: 11.01 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 19\n","\n","\n","--------------- Run 2 - Word 9 ---------------\n","Question: Now, tell me one short joke for the word: \"island\"\n"," Answer:\n","\n","Why did the island go to therapy? Because it was feeling a little \"dis-connected\"!\n","\n","Total time: 11.39 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 2 - Word 10 ---------------\n","Question: Now, tell me one short joke for the word: \"jacket\"\n"," Answer:\n","\n","Why did the jacket go to the doctor? Because it was feeling a little worn out!\n","\n","Total time: 11.22 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 19\n","\n","\n","--------------- Run 2 - Word 11 ---------------\n","Question: Now, tell me one short joke for the word: \"kangaroo\"\n"," Answer:\n","\n","Why did the kangaroo quit his job? Because it was hopping mad!\n","\n","Total time: 9.78 sec.\n","\n","\n","Input tokens: 1345, Output tokens: 16\n","\n","\n","--------------- Run 2 - Word 12 ---------------\n","Question: Now, tell me one short joke for the word: \"lamp\"\n"," Answer:\n","\n","Why did the lamp go to therapy? Because it was feeling a little dim!\n","\n","Total time: 10.15 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 13 ---------------\n","Question: Now, tell me one short joke for the word: \"mountain\"\n"," Answer:\n","\n","Why did the mountain go to the party? Because it was a peak performer!\n","\n","Total time: 10.17 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 14 ---------------\n","Question: Now, tell me one short joke for the word: \"notebook\"\n"," Answer:\n","\n","Why did the notebook go to therapy? Because it had a lot of pages to work through!\n","\n","Total time: 11.37 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 2 - Word 15 ---------------\n","Question: Now, tell me one short joke for the word: \"ocean\"\n"," Answer:\n","\n","Why did the ocean go to therapy? Because it was feeling a little \"drained\"!\n","\n","Total time: 11.52 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 2 - Word 16 ---------------\n","Question: Now, tell me one short joke for the word: \"pencil\"\n"," Answer:\n","\n","Why did the pencil break up with the eraser? It was a sharp move!\n","\n","Total time: 10.72 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 18\n","\n","\n","--------------- Run 2 - Word 17 ---------------\n","Question: Now, tell me one short joke for the word: \"queen\"\n"," Answer:\n","\n","Why did the queen bring a ladder to the party? Because she wanted to take a high throne!\n","\n","Total time: 11.94 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 21\n","\n","\n","--------------- Run 2 - Word 18 ---------------\n","Question: Now, tell me one short joke for the word: \"river\"\n"," Answer:\n","\n","Why did the river go to the party? Because it was a flowing good time!\n","\n","Total time: 10.7 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 18\n","\n","\n","--------------- Run 2 - Word 19 ---------------\n","Question: Now, tell me one short joke for the word: \"star\"\n"," Answer:\n","\n","Why did the star go to the doctor? Because it was feeling a little dim!\n","\n","Total time: 10.64 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 18\n","\n","\n","--------------- Run 2 - Word 20 ---------------\n","Question: Now, tell me one short joke for the word: \"tree\"\n"," Answer:\n","\n","Why did the tree go to the party? Because it wanted to branch out!\n","\n","Total time: 10.11 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 21 ---------------\n","Question: Now, tell me one short joke for the word: \"umbrella\"\n"," Answer:\n","\n","Why did the umbrella go to the doctor? Because it was feeling a little \"under the weather\"!\n","\n","Total time: 12.38 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 22\n","\n","\n","--------------- Run 2 - Word 22 ---------------\n","Question: Now, tell me one short joke for the word: \"village\"\n"," Answer:\n","\n","Why did the village go to therapy? Because it was feeling a little \"dis-connected\"!\n","\n","Total time: 11.38 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 2 - Word 23 ---------------\n","Question: Now, tell me one short joke for the word: \"window\"\n"," Answer:\n","\n","Why did the window go to therapy? Because it was feeling a little shut in!\n","\n","Total time: 10.57 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 18\n","\n","\n","--------------- Run 2 - Word 24 ---------------\n","Question: Now, tell me one short joke for the word: \"xylophone\"\n"," Answer:\n","\n","Why did the xylophone go to therapy? Because it had a lot of treble notes!\n","\n","Total time: 11.9 sec.\n","\n","\n","Input tokens: 1345, Output tokens: 21\n","\n","\n","--------------- Run 2 - Word 25 ---------------\n","Question: Now, tell me one short joke for the word: \"yacht\"\n"," Answer:\n","\n","Why did the yacht go to the party? Because it was a boat-load of fun!\n","\n","Total time: 11.12 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 19\n","\n","\n","--------------- Run 2 - Word 26 ---------------\n","Question: Now, tell me one short joke for the word: \"zebra\"\n"," Answer:\n","\n","Why did the zebra refuse to share his stripes? Because he wanted to keep them to himself!\n","\n","Total time: 11.95 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 21\n","\n","\n","--------------- Run 2 - Word 27 ---------------\n","Question: Now, tell me one short joke for the word: \"balloon\"\n"," Answer:\n","\n","Why did the balloon go to the party? Because it was a blowout!\n","\n","Total time: 10.18 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 28 ---------------\n","Question: Now, tell me one short joke for the word: \"camera\"\n"," Answer:\n","\n","Why did the camera go to therapy? Because it had a lot of focus issues!\n","\n","Total time: 10.61 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 18\n","\n","\n","--------------- Run 2 - Word 29 ---------------\n","Question: Now, tell me one short joke for the word: \"desert\"\n"," Answer:\n","\n","Why did the desert go to therapy? Because it was feeling a little dry!\n","\n","Total time: 10.13 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 30 ---------------\n","Question: Now, tell me one short joke for the word: \"engine\"\n"," Answer:\n","\n","Why did the engine go to therapy? Because it was feeling a little \"car-sick\"!\n","\n","Total time: 11.89 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 21\n","\n","\n","--------------- Run 2 - Word 31 ---------------\n","Question: Now, tell me one short joke for the word: \"flower\"\n"," Answer:\n","\n","Why did the flower go to the party? Because it was a blooming good time!\n","\n","Total time: 11.06 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 19\n","\n","\n","--------------- Run 2 - Word 32 ---------------\n","Question: Now, tell me one short joke for the word: \"garden\"\n"," Answer:\n","\n","Why did the garden go to therapy? Because it was feeling a little wilted!\n","\n","Total time: 10.53 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 18\n","\n","\n","--------------- Run 2 - Word 33 ---------------\n","Question: Now, tell me one short joke for the word: \"honey\"\n"," Answer:\n","\n","Why did the honey go to the doctor? Because it was feeling a little sticky!\n","\n","Total time: 10.65 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 18\n","\n","\n","--------------- Run 2 - Word 34 ---------------\n","Question: Now, tell me one short joke for the word: \"iceberg\"\n"," Answer:\n","\n","Why did the Iceberg go to therapy? Because it had a meltdown!\n","\n","Total time: 9.7 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 16\n","\n","\n","--------------- Run 2 - Word 35 ---------------\n","Question: Now, tell me one short joke for the word: \"jungle\"\n"," Answer:\n","\n","Why did the jungle go to the party? Because it was a wild time!\n","\n","Total time: 10.16 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 36 ---------------\n","Question: Now, tell me one short joke for the word: \"kite\"\n"," Answer:\n","\n","Why did the kite go to the party? Because it was a blowout!\n","\n","Total time: 10.17 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 37 ---------------\n","Question: Now, tell me one short joke for the word: \"ladder\"\n"," Answer:\n","\n","Why did the ladder go to therapy? Because it had a lot of steps to work through!\n","\n","Total time: 11.5 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 2 - Word 38 ---------------\n","Question: Now, tell me one short joke for the word: \"moon\"\n"," Answer:\n","\n","Why did the astronaut break up with his girlfriend? He needed space!\n","\n","Total time: 9.4 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 15\n","\n","\n","--------------- Run 2 - Word 39 ---------------\n","Question: Now, tell me one short joke for the word: \"nest\"\n"," Answer:\n","\n","Why did the nest go to therapy? Because it had a fowl temper!\n","\n","Total time: 10.13 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 40 ---------------\n","Question: Now, tell me one short joke for the word: \"octopus\"\n"," Answer:\n","\n","Why did the octopus go to the party? Because it was an ink-redible dancer!\n","\n","Total time: 11.5 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 2 - Word 41 ---------------\n","Question: Now, tell me one short joke for the word: \"pirate\"\n"," Answer:\n","\n","Why did the pirate quit his job? Because he was sick of all the arrrr-guments!\n","\n","Total time: 12.1 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 21\n","\n","\n","--------------- Run 2 - Word 42 ---------------\n","Question: Now, tell me one short joke for the word: \"quilt\"\n"," Answer:\n","\n","Why did the quilter go to therapy? Because it was feeling a little \"stitched\"!\n","\n","Total time: 11.86 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 21\n","\n","\n","--------------- Run 2 - Word 43 ---------------\n","Question: Now, tell me one short joke for the word: \"robot\"\n"," Answer:\n","\n","Why did the robot go on a diet? Because it wanted to lose some bytes!\n","\n","Total time: 10.68 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 18\n","\n","\n","--------------- Run 2 - Word 44 ---------------\n","Question: Now, tell me one short joke for the word: \"swan\"\n"," Answer:\n","\n","Why did the swan go to the doctor? Because it had a fowl temper!\n","\n","Total time: 11.01 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 19\n","\n","\n","--------------- Run 2 - Word 45 ---------------\n","Question: Now, tell me one short joke for the word: \"telescope\"\n"," Answer:\n","\n","Why did the telescope go to therapy? It had a lot of hang-ups!\n","\n","Total time: 10.09 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 46 ---------------\n","Question: Now, tell me one short joke for the word: \"unicorn\"\n"," Answer:\n","\n","Why did the unicorn get kicked out of the movie theater? Because it was using its horn to pop the corn!\n","\n","Total time: 13.33 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 24\n","\n","\n","--------------- Run 2 - Word 47 ---------------\n","Question: Now, tell me one short joke for the word: \"violin\"\n"," Answer:\n","\n","Why did the violinist's computer crash? It had a little treble!\n","\n","Total time: 10.13 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 48 ---------------\n","Question: Now, tell me one short joke for the word: \"whale\"\n"," Answer:\n","\n","Why did the whale go to the ocean party? Because he heard it was a whale of a time!\n","\n","Total time: 12.36 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 22\n","\n","\n","--------------- Run 2 - Word 49 ---------------\n","Question: Now, tell me one short joke for the word: \"x-ray\"\n"," Answer:\n","\n","Why did the x-ray go to therapy? Because it was feeling a little exposed!\n","\n","Total time: 10.56 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 18\n","\n","\n","--------------- Run 2 - Word 50 ---------------\n","Question: Now, tell me one short joke for the word: \"laptop\"\n"," Answer:\n","\n","Why did the laptop go to the doctor? Because it had a virus!\n","\n","Total time: 9.84 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 16\n","\n","\n","\n","\n","========================= Run 3 =========================\n","\n","\n","--------------- Run 3 - Word 1 ---------------\n","Question: Now, tell me one short joke for the word: \"apple\"\n"," Answer:\n","\n","Why did the apple join the gym? To get some core strength!\n","\n","Total time: 9.29 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 15\n","\n","\n","--------------- Run 3 - Word 2 ---------------\n","Question: Now, tell me one short joke for the word: \"book\"\n"," Answer:\n","\n","Why did the book go to therapy? Because it had a bad chapter in its life!\n","\n","Total time: 11.08 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 19\n","\n","\n","--------------- Run 3 - Word 3 ---------------\n","Question: Now, tell me one short joke for the word: \"car\"\n"," Answer:\n","\n","Why did the car go to the doctor? Because it was feeling a little \"car\"-sick!\n","\n","Total time: 12.54 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 22\n","\n","\n","--------------- Run 3 - Word 4 ---------------\n","Question: Now, tell me one short joke for the word: \"dog\"\n"," Answer:\n","\n","Why did the dog go to the vet? Because it was feeling a little ruff!\n","\n","Total time: 11.14 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 19\n","\n","\n","--------------- Run 3 - Word 5 ---------------\n","Question: Now, tell me one short joke for the word: \"elephant\"\n"," Answer:\n","\n","Why did the elephant quit the circus? Because it was tired of working for peanuts!\n","\n","Total time: 10.64 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 6 ---------------\n","Question: Now, tell me one short joke for the word: \"forest\"\n"," Answer:\n","\n","Why did the forest go to the party? Because it wanted to branch out!\n","\n","Total time: 10.12 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 17\n","\n","\n","--------------- Run 3 - Word 7 ---------------\n","Question: Now, tell me one short joke for the word: \"guitar\"\n"," Answer:\n","\n","Why did the guitar go to the doctor? Because it was feeling a little out of tune!\n","\n","Total time: 11.58 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 3 - Word 8 ---------------\n","Question: Now, tell me one short joke for the word: \"house\"\n"," Answer:\n","\n","Why did the house go to therapy? Because it had a few walls to work through!\n","\n","Total time: 11.01 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 19\n","\n","\n","--------------- Run 3 - Word 9 ---------------\n","Question: Now, tell me one short joke for the word: \"island\"\n"," Answer:\n","\n","Why did the island go to therapy? Because it was feeling a little \"dis-connected\"!\n","\n","Total time: 11.4 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 3 - Word 10 ---------------\n","Question: Now, tell me one short joke for the word: \"jacket\"\n"," Answer:\n","\n","Why did the jacket go to the doctor? Because it was feeling a little worn out!\n","\n","Total time: 11.22 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 19\n","\n","\n","--------------- Run 3 - Word 11 ---------------\n","Question: Now, tell me one short joke for the word: \"kangaroo\"\n"," Answer:\n","\n","Why did the kangaroo quit his job? Because it was hopping mad!\n","\n","Total time: 9.78 sec.\n","\n","\n","Input tokens: 1345, Output tokens: 16\n","\n","\n","--------------- Run 3 - Word 12 ---------------\n","Question: Now, tell me one short joke for the word: \"lamp\"\n"," Answer:\n","\n","Why did the lamp go to therapy? Because it was feeling a little dim!\n","\n","Total time: 10.15 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 17\n","\n","\n","--------------- Run 3 - Word 13 ---------------\n","Question: Now, tell me one short joke for the word: \"mountain\"\n"," Answer:\n","\n","Why did the mountain go to the party? Because it was a peak performer!\n","\n","Total time: 10.17 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 17\n","\n","\n","--------------- Run 3 - Word 14 ---------------\n","Question: Now, tell me one short joke for the word: \"notebook\"\n"," Answer:\n","\n","Why did the notebook go to therapy? Because it had a lot of pages to work through!\n","\n","Total time: 11.36 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 3 - Word 15 ---------------\n","Question: Now, tell me one short joke for the word: \"ocean\"\n"," Answer:\n","\n","Why did the ocean go to therapy? Because it was feeling a little \"drained\"!\n","\n","Total time: 11.51 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 3 - Word 16 ---------------\n","Question: Now, tell me one short joke for the word: \"pencil\"\n"," Answer:\n","\n","Why did the pencil break up with the eraser? It was a sharp move!\n","\n","Total time: 10.72 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 17 ---------------\n","Question: Now, tell me one short joke for the word: \"queen\"\n"," Answer:\n","\n","Why did the queen go to the doctor? Because she had a royal pain!\n","\n","Total time: 10.26 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 17\n","\n","\n","--------------- Run 3 - Word 18 ---------------\n","Question: Now, tell me one short joke for the word: \"river\"\n"," Answer:\n","\n","Why did the river go to the party? Because it was a flowing good time!\n","\n","Total time: 10.7 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 19 ---------------\n","Question: Now, tell me one short joke for the word: \"star\"\n"," Answer:\n","\n","Why did the star go to the doctor? Because it was feeling a little dim!\n","\n","Total time: 10.64 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 20 ---------------\n","Question: Now, tell me one short joke for the word: \"tree\"\n"," Answer:\n","\n","Why did the tree go to the party? Because it wanted to branch out!\n","\n","Total time: 10.1 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 17\n","\n","\n","--------------- Run 3 - Word 21 ---------------\n","Question: Now, tell me one short joke for the word: \"umbrella\"\n"," Answer:\n","\n","Why did the umbrella go to therapy? Because it was feeling a little \"drained\"!\n","\n","Total time: 11.52 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 3 - Word 22 ---------------\n","Question: Now, tell me one short joke for the word: \"village\"\n"," Answer:\n","\n","Why did the village go to therapy? Because it was feeling a little \"dis-connected\"!\n","\n","Total time: 11.38 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 3 - Word 23 ---------------\n","Question: Now, tell me one short joke for the word: \"window\"\n"," Answer:\n","\n","Why did the window go to therapy? Because it was feeling a little shut in!\n","\n","Total time: 10.57 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 24 ---------------\n","Question: Now, tell me one short joke for the word: \"xylophone\"\n"," Answer:\n","\n","Why did the xylophone go to therapy? Because it had a lot of strings attached!\n","\n","Total time: 11.48 sec.\n","\n","\n","Input tokens: 1345, Output tokens: 20\n","\n","\n","--------------- Run 3 - Word 25 ---------------\n","Question: Now, tell me one short joke for the word: \"yacht\"\n"," Answer:\n","\n","Why did the yacht go to the party? Because it was a boat-load of fun!\n","\n","Total time: 11.13 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 19\n","\n","\n","--------------- Run 3 - Word 26 ---------------\n","Question: Now, tell me one short joke for the word: \"zebra\"\n"," Answer:\n","\n","Why did the zebra refuse to share his stripes? Because he wanted to keep them to himself!\n","\n","Total time: 11.96 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 21\n","\n","\n","--------------- Run 3 - Word 27 ---------------\n","Question: Now, tell me one short joke for the word: \"balloon\"\n"," Answer:\n","\n","Why did the balloon go to the party? Because it was a blowout!\n","\n","Total time: 10.19 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 17\n","\n","\n","--------------- Run 3 - Word 28 ---------------\n","Question: Now, tell me one short joke for the word: \"camera\"\n"," Answer:\n","\n","Why did the camera go to therapy? Because it was feeling a little focused!\n","\n","Total time: 10.12 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 17\n","\n","\n","--------------- Run 3 - Word 29 ---------------\n","Question: Now, tell me one short joke for the word: \"desert\"\n"," Answer:\n","\n","Why did the desert go to therapy? Because it was feeling a little dry!\n","\n","Total time: 10.13 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 17\n","\n","\n","--------------- Run 3 - Word 30 ---------------\n","Question: Now, tell me one short joke for the word: \"engine\"\n"," Answer:\n","\n","Why did the engine go to therapy? Because it was feeling a little \"car-sick\"!\n","\n","Total time: 11.89 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 21\n","\n","\n","--------------- Run 3 - Word 31 ---------------\n","Question: Now, tell me one short joke for the word: \"flower\"\n"," Answer:\n","\n","Why did the flower go to the party? Because it was a blooming good time!\n","\n","Total time: 11.05 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 19\n","\n","\n","--------------- Run 3 - Word 32 ---------------\n","Question: Now, tell me one short joke for the word: \"garden\"\n"," Answer:\n","\n","Why did the garden go to therapy? Because it was feeling a little wilted!\n","\n","Total time: 10.54 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 33 ---------------\n","Question: Now, tell me one short joke for the word: \"honey\"\n"," Answer:\n","\n","Why did the honey go to the doctor? Because it was feeling a little sticky!\n","\n","Total time: 10.65 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 34 ---------------\n","Question: Now, tell me one short joke for the word: \"iceberg\"\n"," Answer:\n","\n","Why did the Iceberg go to therapy? Because it was feeling a little flaky!\n","\n","Total time: 11.02 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 19\n","\n","\n","--------------- Run 3 - Word 35 ---------------\n","Question: Now, tell me one short joke for the word: \"jungle\"\n"," Answer:\n","\n","Why did the jungle go to the party? Because it was a wild time!\n","\n","Total time: 10.18 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 17\n","\n","\n","--------------- Run 3 - Word 36 ---------------\n","Question: Now, tell me one short joke for the word: \"kite\"\n"," Answer:\n","\n","Why did the kite go to the party? Because it was a blowout!\n","\n","Total time: 10.18 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 17\n","\n","\n","--------------- Run 3 - Word 37 ---------------\n","Question: Now, tell me one short joke for the word: \"ladder\"\n"," Answer:\n","\n","Why did the ladder go to therapy? Because it was feeling a little rung!\n","\n","Total time: 10.64 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 38 ---------------\n","Question: Now, tell me one short joke for the word: \"moon\"\n"," Answer:\n","\n","Why did the astronaut break up with his girlfriend? He needed space!\n","\n","Total time: 9.41 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 15\n","\n","\n","--------------- Run 3 - Word 39 ---------------\n","Question: Now, tell me one short joke for the word: \"nest\"\n"," Answer:\n","\n","Why did the nest go to therapy? Because it had a fowl temper!\n","\n","Total time: 10.13 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 17\n","\n","\n","--------------- Run 3 - Word 40 ---------------\n","Question: Now, tell me one short joke for the word: \"octopus\"\n"," Answer:\n","\n","Why did the octopus go to the party? Because it was an ink-redible dancer!\n","\n","Total time: 11.5 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 20\n","\n","\n","--------------- Run 3 - Word 41 ---------------\n","Question: Now, tell me one short joke for the word: \"pirate\"\n"," Answer:\n","\n","Why did the pirate quit his job? Because he was sick of all the arrrr-guments!\n","\n","Total time: 12.1 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 21\n","\n","\n","--------------- Run 3 - Word 42 ---------------\n","Question: Now, tell me one short joke for the word: \"quilt\"\n"," Answer:\n","\n","Why did the quilter go to therapy? Because it was feeling a little \"stitched\"!\n","\n","Total time: 11.85 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 21\n","\n","\n","--------------- Run 3 - Word 43 ---------------\n","Question: Now, tell me one short joke for the word: \"robot\"\n"," Answer:\n","\n","Why did the robot go on a diet? Because it wanted to lose some bytes!\n","\n","Total time: 10.69 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 44 ---------------\n","Question: Now, tell me one short joke for the word: \"swan\"\n"," Answer:\n","\n","Why did the swan go to the doctor? Because it had a fowl temper!\n","\n","Total time: 11.01 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 19\n","\n","\n","--------------- Run 3 - Word 45 ---------------\n","Question: Now, tell me one short joke for the word: \"telescope\"\n"," Answer:\n","\n","Why did the telescope go to therapy? Because it was feeling a little spaced out!\n","\n","Total time: 10.51 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 46 ---------------\n","Question: Now, tell me one short joke for the word: \"unicorn\"\n"," Answer:\n","\n","Why did the unicorn get kicked out of the movie theater? Because it was using its horn to pop the corn!\n","\n","Total time: 13.31 sec.\n","\n","\n","Input tokens: 1343, Output tokens: 24\n","\n","\n","--------------- Run 3 - Word 47 ---------------\n","Question: Now, tell me one short joke for the word: \"violin\"\n"," Answer:\n","\n","Why was the violinist's computer slow? It had a lot of strings attached!\n","\n","Total time: 10.55 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 48 ---------------\n","Question: Now, tell me one short joke for the word: \"whale\"\n"," Answer:\n","\n","Why did the whale go to the ocean party? Because he heard it was a whale of a time!\n","\n","Total time: 12.32 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 22\n","\n","\n","--------------- Run 3 - Word 49 ---------------\n","Question: Now, tell me one short joke for the word: \"x-ray\"\n"," Answer:\n","\n","Why did the x-ray go to therapy? Because it was feeling a little exposed!\n","\n","Total time: 10.53 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 50 ---------------\n","Question: Now, tell me one short joke for the word: \"laptop\"\n"," Answer:\n","\n","Why did the laptop go to the doctor? It had a virus!\n","\n","Total time: 9.36 sec.\n","\n","\n","Input tokens: 1344, Output tokens: 15\n","\n","\n","\n","\n","=============== RESULTS for 70 EXAMPLES ===============\n","\n","\n","        Finished 3 Runs with 50 Words/Run.\n","\n","\n","        Total Prompts: 150\n","\n","        Total Input Tokens: 201549, AVG/Prompt: 1343.66\n","\n","        Total Output Tokens: 2794, AVG/Prompt: 18.626666666666665\n","\n","        --------------------------------------------------\n","\n","        Total Inference Emissions: 0.103kg COeq\n","\n","\n","        Emissions / 1.000.000 Input Tokens: 0.512kg COeq\n","\n","        Emissions / 1.000.000 Output Tokens: 36.921kg COeq\n","\n","        Emissions / 1.000 Prompts: 0.688kg COeq\n","\n","\n","        \n","Results saved to meta-llama/Meta-Llama-3-70B-Instruct-emissiondata-70-examples.csv\n","========== INFERENCE TEST with 210 EXAMPLES ==========\n","\n","\n","        Starting Test with 3 Runs and 50 Words/Run. \n","\n","        Total Prompts: 150\n","\n","\n","        Expected Runtime: 25.0min \n","\n","        Expected Input Tokens: 15450 \n","\n","        Expected Output Tokens: 4500 \n","\n","\n","        \n","========================= Run 1 =========================\n","\n","\n","--------------- Run 1 - Word 1 ---------------\n","Question: Now, tell me one short joke for the word: \"apple\"\n"," Answer:\n","\n","Why did the apple join the gym? To get some core strength!\n","\n","Total time: 16.37 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 15\n","\n","\n","--------------- Run 1 - Word 2 ---------------\n","Question: Now, tell me one short joke for the word: \"book\"\n"," Answer:\n","\n","Why did the book go to therapy? Because it had a bad chapter in its life!\n","\n","Total time: 18.34 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 19\n","\n","\n","--------------- Run 1 - Word 3 ---------------\n","Question: Now, tell me one short joke for the word: \"car\"\n"," Answer:\n","\n","Why did the car go to the doctor? Because it was feeling a little \"car\"-sick!\n","\n","Total time: 20.03 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 22\n","\n","\n","--------------- Run 1 - Word 4 ---------------\n","Question: Now, tell me one short joke for the word: \"dog\"\n"," Answer:\n","\n","Why did the dog go to the vet? Because it was feeling a little ruff!\n","\n","Total time: 18.41 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 19\n","\n","\n","--------------- Run 1 - Word 5 ---------------\n","Question: Now, tell me one short joke for the word: \"elephant\"\n"," Answer:\n","\n","Why did the elephant quit the circus? Because it was tired of working for peanuts!\n","\n","Total time: 17.78 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 18\n","\n","\n","--------------- Run 1 - Word 6 ---------------\n","Question: Now, tell me one short joke for the word: \"forest\"\n"," Answer:\n","\n","Why did the tree go to the party? Because it wanted to branch out!\n","\n","Total time: 17.2 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 17\n","\n","\n","--------------- Run 1 - Word 7 ---------------\n","Question: Now, tell me one short joke for the word: \"guitar\"\n"," Answer:\n","\n","Why was the guitar string in therapy? Because it was feeling a little \"out of tune\"!\n","\n","Total time: 19.29 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 21\n","\n","\n","--------------- Run 1 - Word 8 ---------------\n","Question: Now, tell me one short joke for the word: \"house\"\n"," Answer:\n","\n","Why did the house go to therapy? Because it had a few screws loose!\n","\n","Total time: 17.29 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 17\n","\n","\n","--------------- Run 1 - Word 9 ---------------\n","Question: Now, tell me one short joke for the word: \"island\"\n"," Answer:\n","\n","Why did the island go to therapy? Because it was feeling a little \"shore\"!\n","\n","Total time: 18.23 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 19\n","\n","\n","--------------- Run 1 - Word 10 ---------------\n","Question: Now, tell me one short joke for the word: \"jacket\"\n"," Answer:\n","\n","Why did the jacked go to therapy? Because it was feeling a little \"down\"!\n","\n","Total time: 18.95 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 20\n","\n","\n","--------------- Run 1 - Word 11 ---------------\n","Question: Now, tell me one short joke for the word: \"kangaroo\"\n"," Answer:\n","\n","Why did the kangaroo quit his job? Because it was hopping mad!\n","\n","Total time: 16.73 sec.\n","\n","\n","Input tokens: 3745, Output tokens: 16\n","\n","\n","--------------- Run 1 - Word 12 ---------------\n","Question: Now, tell me one short joke for the word: \"lamp\"\n"," Answer:\n","\n","Why did the lamp go to therapy? Because it was feeling a little dim!\n","\n","Total time: 17.37 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 17\n","\n","\n","--------------- Run 1 - Word 13 ---------------\n","Question: Now, tell me one short joke for the word: \"mountain\"\n"," Answer:\n","\n","Why did the mountain go to the party? Because it was a peak performer!\n","\n","Total time: 17.34 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 17\n","\n","\n","--------------- Run 1 - Word 14 ---------------\n","Question: Now, tell me one short joke for the word: \"notebook\"\n"," Answer:\n","\n","Why did the notebook go to therapy? It had a lot of notes to work through!\n","\n","Total time: 18.29 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 19\n","\n","\n","--------------- Run 1 - Word 15 ---------------\n","Question: Now, tell me one short joke for the word: \"ocean\"\n"," Answer:\n","\n","Why did the ocean go to therapy? It was feeling a little \"drained\"!\n","\n","Total time: 18.41 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 19\n","\n","\n","--------------- Run 1 - Word 16 ---------------\n","Question: Now, tell me one short joke for the word: \"pencil\"\n"," Answer:\n","\n","Why did the pencil go to the party? Because it was a little \"drawn\"!\n","\n","Total time: 19.01 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 20\n","\n","\n","--------------- Run 1 - Word 17 ---------------\n","Question: Now, tell me one short joke for the word: \"queen\"\n"," Answer:\n","\n","Why did the queen bring a ladder to the party? Because she heard the drinks were on the house!\n","\n","Total time: 19.94 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 22\n","\n","\n","--------------- Run 1 - Word 18 ---------------\n","Question: Now, tell me one short joke for the word: \"river\"\n"," Answer:\n","\n","Why did the river go to therapy? Because it was feeling drained!\n","\n","Total time: 16.43 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 15\n","\n","\n","--------------- Run 1 - Word 19 ---------------\n","Question: Now, tell me one short joke for the word: \"star\"\n"," Answer:\n","\n","Why did the star go to the doctor? Because it was feeling a little dim!\n","\n","Total time: 18.0 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 18\n","\n","\n","--------------- Run 1 - Word 20 ---------------\n","Question: Now, tell me one short joke for the word: \"tree\"\n"," Answer:\n","\n","Why did the tree go to the party? Because it wanted to branch out!\n","\n","Total time: 17.39 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 17\n","\n","\n","--------------- Run 1 - Word 21 ---------------\n","Question: Now, tell me one short joke for the word: \"umbrella\"\n"," Answer:\n","\n","Why did the umbrella go to the doctor? Because it was feeling a little \"under the weather\"!\n","\n","Total time: 20.07 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 22\n","\n","\n","--------------- Run 1 - Word 22 ---------------\n","Question: Now, tell me one short joke for the word: \"village\"\n"," Answer:\n","\n","Why did the village goer go to therapy? Because it was feeling a little \"dis-connected\"!\n","\n","Total time: 19.95 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 22\n","\n","\n","--------------- Run 1 - Word 23 ---------------\n","Question: Now, tell me one short joke for the word: \"window\"\n"," Answer:\n","\n","Why did the window go to therapy? It had a lot of panes!\n","\n","Total time: 17.38 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 17\n","\n","\n","--------------- Run 1 - Word 24 ---------------\n","Question: Now, tell me one short joke for the word: \"xylophone\"\n"," Answer:\n","\n","Why did the xylophone go to therapy? It had a lot of strings attached!\n","\n","Total time: 18.3 sec.\n","\n","\n","Input tokens: 3745, Output tokens: 19\n","\n","\n","--------------- Run 1 - Word 25 ---------------\n","Question: Now, tell me one short joke for the word: \"yacht\"\n"," Answer:\n","\n","Why did the yacht go to the party? Because it was a little \"boat\"-ed out of control!\n","\n","Total time: 20.56 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 23\n","\n","\n","--------------- Run 1 - Word 26 ---------------\n","Question: Now, tell me one short joke for the word: \"zebra\"\n"," Answer:\n","\n","Why did the zebra refuse to share his stripes? Because he wanted to keep them to himself!\n","\n","Total time: 19.48 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 21\n","\n","\n","--------------- Run 1 - Word 27 ---------------\n","Question: Now, tell me one short joke for the word: \"balloon\"\n"," Answer:\n","\n","Why did the balloon go to the doctor? Because it was feeling deflated!\n","\n","Total time: 17.55 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 17\n","\n","\n","--------------- Run 1 - Word 28 ---------------\n","Question: Now, tell me one short joke for the word: \"camera\"\n"," Answer:\n","\n","Why did the camera go to therapy? It had a little focus on its problems!\n","\n","Total time: 17.98 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 18\n","\n","\n","--------------- Run 1 - Word 29 ---------------\n","Question: Now, tell me one short joke for the word: \"desert\"\n"," Answer:\n","\n","Why did the desert go to therapy? Because it was feeling a little dry!\n","\n","Total time: 17.42 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 17\n","\n","\n","--------------- Run 1 - Word 30 ---------------\n","Question: Now, tell me one short joke for the word: \"engine\"\n"," Answer:\n","\n","Why did the engine go to therapy? Because it was feeling a little \"horse\" power!\n","\n","Total time: 19.01 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 20\n","\n","\n","--------------- Run 1 - Word 31 ---------------\n","Question: Now, tell me one short joke for the word: \"flower\"\n"," Answer:\n","\n","Why did the flower go to the party? Because it was a blooming good time!\n","\n","Total time: 18.49 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 19\n","\n","\n","--------------- Run 1 - Word 32 ---------------\n","Question: Now, tell me one short joke for the word: \"garden\"\n"," Answer:\n","\n","Why did the garden go to therapy? Because it was feeling a little wilted!\n","\n","Total time: 17.89 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 18\n","\n","\n","--------------- Run 1 - Word 33 ---------------\n","Question: Now, tell me one short joke for the word: \"honey\"\n"," Answer:\n","\n","Why did the bee get married? Because he found his honey!\n","\n","Total time: 15.91 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 14\n","\n","\n","--------------- Run 1 - Word 34 ---------------\n","Question: Now, tell me one short joke for the word: \"iceberg\"\n"," Answer:\n","\n","Why did the iceberg go to the party? Because it was a cool reception!\n","\n","Total time: 18.05 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 17\n","\n","\n","--------------- Run 1 - Word 35 ---------------\n","Question: Now, tell me one short joke for the word: \"jungle\"\n"," Answer:\n","\n","Why did the jungle go to the party? Because it was a wild time!\n","\n","Total time: 17.45 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 17\n","\n","\n","--------------- Run 1 - Word 36 ---------------\n","Question: Now, tell me one short joke for the word: \"kite\"\n"," Answer:\n","\n","Why did the kite go to the doctor? Because it had a little \"string\" attached!\n","\n","Total time: 19.07 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 20\n","\n","\n","--------------- Run 1 - Word 37 ---------------\n","Question: Now, tell me one short joke for the word: \"ladder\"\n"," Answer:\n","\n","Why did the ladder go to therapy? It had too many rungs!\n","\n","Total time: 16.96 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 16\n","\n","\n","--------------- Run 1 - Word 38 ---------------\n","Question: Now, tell me one short joke for the word: \"moon\"\n"," Answer:\n","\n","Why did the cow jump over the moon? To get to the milky way!\n","\n","Total time: 18.07 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 18\n","\n","\n","--------------- Run 1 - Word 39 ---------------\n","Question: Now, tell me one short joke for the word: \"nest\"\n"," Answer:\n","\n","Why did the bird go to the doctor? Because it had a fowl temper!\n","\n","Total time: 17.99 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 18\n","\n","\n","--------------- Run 1 - Word 40 ---------------\n","Question: Now, tell me one short joke for the word: \"octopus\"\n"," Answer:\n","\n","Why did the octopus go to the party? Because it was an ink-redible guest!\n","\n","Total time: 19.01 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 20\n","\n","\n","--------------- Run 1 - Word 41 ---------------\n","Question: Now, tell me one short joke for the word: \"pirate\"\n"," Answer:\n","\n","Why did the pirate quit his job? Because he was sick of all the arrrr-guments!\n","\n","Total time: 19.54 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 21\n","\n","\n","--------------- Run 1 - Word 42 ---------------\n","Question: Now, tell me one short joke for the word: \"quilt\"\n"," Answer:\n","\n","Why did the quilter go to therapy? It had a lot of hang-ups!\n","\n","Total time: 17.99 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 18\n","\n","\n","--------------- Run 1 - Word 43 ---------------\n","Question: Now, tell me one short joke for the word: \"robot\"\n"," Answer:\n","\n","Why did the robot go on a diet? Because it wanted to lose some bytes!\n","\n","Total time: 18.08 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 18\n","\n","\n","--------------- Run 1 - Word 44 ---------------\n","Question: Now, tell me one short joke for the word: \"swan\"\n"," Answer:\n","\n","Why did the swan go to the doctor? Because it had a fowl temper!\n","\n","Total time: 18.47 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 19\n","\n","\n","--------------- Run 1 - Word 45 ---------------\n","Question: Now, tell me one short joke for the word: \"telescope\"\n"," Answer:\n","\n","Why did the telescope go to therapy? It was feeling a little \"out of focus\"!\n","\n","Total time: 18.99 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 20\n","\n","\n","--------------- Run 1 - Word 46 ---------------\n","Question: Now, tell me one short joke for the word: \"unicorn\"\n"," Answer:\n","\n","Why did the unicorn get kicked out of the movie? Because it was using a horn to pop the corn!\n","\n","Total time: 20.5 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 23\n","\n","\n","--------------- Run 1 - Word 47 ---------------\n","Question: Now, tell me one short joke for the word: \"violin\"\n"," Answer:\n","\n","Why was the violinist's computer sick? It had a little \"C\" sharp!\n","\n","Total time: 18.46 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 19\n","\n","\n","--------------- Run 1 - Word 48 ---------------\n","Question: Now, tell me one short joke for the word: \"whale\"\n"," Answer:\n","\n","Why did the whale go to the party? Because he heard it was a \"blow-out\" event!\n","\n","Total time: 20.51 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 23\n","\n","\n","--------------- Run 1 - Word 49 ---------------\n","Question: Now, tell me one short joke for the word: \"x-ray\"\n"," Answer:\n","\n","Why did the x-ray go to therapy? Because it was feeling a little exposed!\n","\n","Total time: 17.98 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 18\n","\n","\n","--------------- Run 1 - Word 50 ---------------\n","Question: Now, tell me one short joke for the word: \"laptop\"\n"," Answer:\n","\n","Why did the laptop go to therapy? It had a little byte to unpack!\n","\n","Total time: 17.52 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 17\n","\n","\n","\n","\n","========================= Run 2 =========================\n","\n","\n","--------------- Run 2 - Word 1 ---------------\n","Question: Now, tell me one short joke for the word: \"apple\"\n"," Answer:\n","\n","Why did the apple join the gym? To get some core strength!\n","\n","Total time: 16.46 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 15\n","\n","\n","--------------- Run 2 - Word 2 ---------------\n","Question: Now, tell me one short joke for the word: \"book\"\n"," Answer:\n","\n","Why did the book go to therapy? Because it had a bad chapter in its life!\n","\n","Total time: 18.43 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 19\n","\n","\n","--------------- Run 2 - Word 3 ---------------\n","Question: Now, tell me one short joke for the word: \"car\"\n"," Answer:\n","\n","Why did the car go to the doctor? Because it was feeling a little \"car\"-sick!\n","\n","Total time: 20.12 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 22\n","\n","\n","--------------- Run 2 - Word 4 ---------------\n","Question: Now, tell me one short joke for the word: \"dog\"\n"," Answer:\n","\n","Why did the dog go to the vet? Because it was feeling a little ruff!\n","\n","Total time: 18.57 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 19\n","\n","\n","--------------- Run 2 - Word 5 ---------------\n","Question: Now, tell me one short joke for the word: \"elephant\"\n"," Answer:\n","\n","Why did the elephant quit the circus? Because it was tired of working for peanuts!\n","\n","Total time: 17.95 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 18\n","\n","\n","--------------- Run 2 - Word 6 ---------------\n","Question: Now, tell me one short joke for the word: \"forest\"\n"," Answer:\n","\n","Why did the tree go to the party? Because it wanted to branch out!\n","\n","Total time: 17.4 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 7 ---------------\n","Question: Now, tell me one short joke for the word: \"guitar\"\n"," Answer:\n","\n","Why was the guitar string in therapy? Because it was feeling a little flat!\n","\n","Total time: 17.51 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 8 ---------------\n","Question: Now, tell me one short joke for the word: \"house\"\n"," Answer:\n","\n","Why did the house go to therapy? Because it had a few screws loose!\n","\n","Total time: 17.49 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 9 ---------------\n","Question: Now, tell me one short joke for the word: \"island\"\n"," Answer:\n","\n","Why did the island go to therapy? Because it was feeling a little \"shore\"!\n","\n","Total time: 18.42 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 19\n","\n","\n","--------------- Run 2 - Word 10 ---------------\n","Question: Now, tell me one short joke for the word: \"jacket\"\n"," Answer:\n","\n","Why did the jacked go to therapy? Because it was feeling a little \"down\"!\n","\n","Total time: 19.1 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 20\n","\n","\n","--------------- Run 2 - Word 11 ---------------\n","Question: Now, tell me one short joke for the word: \"kangaroo\"\n"," Answer:\n","\n","Why did the kangaroo quit his job? Because it was hopping mad!\n","\n","Total time: 16.87 sec.\n","\n","\n","Input tokens: 3745, Output tokens: 16\n","\n","\n","--------------- Run 2 - Word 12 ---------------\n","Question: Now, tell me one short joke for the word: \"lamp\"\n"," Answer:\n","\n","Why did the lamp go to therapy? Because it was feeling a little dim!\n","\n","Total time: 17.49 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 13 ---------------\n","Question: Now, tell me one short joke for the word: \"mountain\"\n"," Answer:\n","\n","Why did the mountain go to the party? Because it was a peak performer!\n","\n","Total time: 17.46 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 14 ---------------\n","Question: Now, tell me one short joke for the word: \"notebook\"\n"," Answer:\n","\n","Why did the notebook go to therapy? It had a lot of notes to work through!\n","\n","Total time: 18.38 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 19\n","\n","\n","--------------- Run 2 - Word 15 ---------------\n","Question: Now, tell me one short joke for the word: \"ocean\"\n"," Answer:\n","\n","Why did the ocean go to therapy? It was feeling a little \"drained\"!\n","\n","Total time: 18.48 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 19\n","\n","\n","--------------- Run 2 - Word 16 ---------------\n","Question: Now, tell me one short joke for the word: \"pencil\"\n"," Answer:\n","\n","Why did the pencil go to the party? Because it was a little \"drawn\"!\n","\n","Total time: 19.07 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 20\n","\n","\n","--------------- Run 2 - Word 17 ---------------\n","Question: Now, tell me one short joke for the word: \"queen\"\n"," Answer:\n","\n","Why did the queen bring a ladder to the party? Because she heard the drinks were on the house!\n","\n","Total time: 20.03 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 22\n","\n","\n","--------------- Run 2 - Word 18 ---------------\n","Question: Now, tell me one short joke for the word: \"river\"\n"," Answer:\n","\n","Why did the river go to therapy? Because it was feeling drained!\n","\n","Total time: 16.48 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 15\n","\n","\n","--------------- Run 2 - Word 19 ---------------\n","Question: Now, tell me one short joke for the word: \"star\"\n"," Answer:\n","\n","Why did the star go to the doctor? Because it was feeling a little dim!\n","\n","Total time: 18.0 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 18\n","\n","\n","--------------- Run 2 - Word 20 ---------------\n","Question: Now, tell me one short joke for the word: \"tree\"\n"," Answer:\n","\n","Why did the tree go to the party? Because it wanted to branch out!\n","\n","Total time: 17.39 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 21 ---------------\n","Question: Now, tell me one short joke for the word: \"umbrella\"\n"," Answer:\n","\n","Why did the umbrella go to the doctor? Because it was feeling a little \"drizzle\"!\n","\n","Total time: 19.63 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 21\n","\n","\n","--------------- Run 2 - Word 22 ---------------\n","Question: Now, tell me one short joke for the word: \"village\"\n"," Answer:\n","\n","Why did the village goer go to therapy? Because it was feeling a little \"town\"!\n","\n","Total time: 19.46 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 21\n","\n","\n","--------------- Run 2 - Word 23 ---------------\n","Question: Now, tell me one short joke for the word: \"window\"\n"," Answer:\n","\n","Why did the window go to therapy? Because it had a lot of panes!\n","\n","Total time: 17.94 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 18\n","\n","\n","--------------- Run 2 - Word 24 ---------------\n","Question: Now, tell me one short joke for the word: \"xylophone\"\n"," Answer:\n","\n","Why did the xylophone go to therapy? Because it had a few strings attached!\n","\n","Total time: 18.35 sec.\n","\n","\n","Input tokens: 3745, Output tokens: 19\n","\n","\n","--------------- Run 2 - Word 25 ---------------\n","Question: Now, tell me one short joke for the word: \"yacht\"\n"," Answer:\n","\n","Why did the yacht go to the party? Because it was a boat-load of fun!\n","\n","Total time: 18.57 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 19\n","\n","\n","--------------- Run 2 - Word 26 ---------------\n","Question: Now, tell me one short joke for the word: \"zebra\"\n"," Answer:\n","\n","Why did the zebra refuse to share his stripes? Because he wanted to keep them to himself!\n","\n","Total time: 19.51 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 21\n","\n","\n","--------------- Run 2 - Word 27 ---------------\n","Question: Now, tell me one short joke for the word: \"balloon\"\n"," Answer:\n","\n","Why did the balloon go to the doctor? Because it was feeling deflated!\n","\n","Total time: 17.57 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 28 ---------------\n","Question: Now, tell me one short joke for the word: \"camera\"\n"," Answer:\n","\n","Why did the camera go to therapy? It had a little focus on its problems!\n","\n","Total time: 17.99 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 18\n","\n","\n","--------------- Run 2 - Word 29 ---------------\n","Question: Now, tell me one short joke for the word: \"desert\"\n"," Answer:\n","\n","Why did the desert go to therapy? Because it was feeling a little dry!\n","\n","Total time: 17.42 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 30 ---------------\n","Question: Now, tell me one short joke for the word: \"engine\"\n"," Answer:\n","\n","Why did the engine go to therapy? Because it was feeling a little \"car-sick\"!\n","\n","Total time: 19.48 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 21\n","\n","\n","--------------- Run 2 - Word 31 ---------------\n","Question: Now, tell me one short joke for the word: \"flower\"\n"," Answer:\n","\n","Why did the flower go to the party? Because it was a blooming good time!\n","\n","Total time: 18.49 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 19\n","\n","\n","--------------- Run 2 - Word 32 ---------------\n","Question: Now, tell me one short joke for the word: \"garden\"\n"," Answer:\n","\n","Why did the garden go to therapy? Because it was feeling a little wilted!\n","\n","Total time: 17.92 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 18\n","\n","\n","--------------- Run 2 - Word 33 ---------------\n","Question: Now, tell me one short joke for the word: \"honey\"\n"," Answer:\n","\n","Why did the bee get married? Because he found his honey!\n","\n","Total time: 15.97 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 14\n","\n","\n","--------------- Run 2 - Word 34 ---------------\n","Question: Now, tell me one short joke for the word: \"iceberg\"\n"," Answer:\n","\n","Why did the iceberg go to the party? Because it was a cool reception!\n","\n","Total time: 18.1 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 35 ---------------\n","Question: Now, tell me one short joke for the word: \"jungle\"\n"," Answer:\n","\n","Why did the jungle go to the party? Because it was a wild time!\n","\n","Total time: 17.45 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 17\n","\n","\n","--------------- Run 2 - Word 36 ---------------\n","Question: Now, tell me one short joke for the word: \"kite\"\n"," Answer:\n","\n","Why did the kite go to the doctor? Because it had a little \"string\" attached!\n","\n","Total time: 19.08 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 20\n","\n","\n","--------------- Run 2 - Word 37 ---------------\n","Question: Now, tell me one short joke for the word: \"ladder\"\n"," Answer:\n","\n","Why did the ladder go to therapy? It had too many rungs!\n","\n","Total time: 16.95 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 16\n","\n","\n","--------------- Run 2 - Word 38 ---------------\n","Question: Now, tell me one short joke for the word: \"moon\"\n"," Answer:\n","\n","Why did the cow jump over the moon? To get to the milky way!\n","\n","Total time: 18.09 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 18\n","\n","\n","--------------- Run 2 - Word 39 ---------------\n","Question: Now, tell me one short joke for the word: \"nest\"\n"," Answer:\n","\n","Why did the bird go to the doctor? Because it had a fowl temper!\n","\n","Total time: 18.0 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 18\n","\n","\n","--------------- Run 2 - Word 40 ---------------\n","Question: Now, tell me one short joke for the word: \"octopus\"\n"," Answer:\n","\n","Why did the octopus go to the party? Because it was an ink-redible guest!\n","\n","Total time: 19.01 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 20\n","\n","\n","--------------- Run 2 - Word 41 ---------------\n","Question: Now, tell me one short joke for the word: \"pirate\"\n"," Answer:\n","\n","Why did the pirate quit his job? Because he was sick of all the arrrr-guments!\n","\n","Total time: 19.57 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 21\n","\n","\n","--------------- Run 2 - Word 42 ---------------\n","Question: Now, tell me one short joke for the word: \"quilt\"\n"," Answer:\n","\n","Why did the quilter go to therapy? It had a lot of hang-ups!\n","\n","Total time: 18.0 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 18\n","\n","\n","--------------- Run 2 - Word 43 ---------------\n","Question: Now, tell me one short joke for the word: \"robot\"\n"," Answer:\n","\n","Why did the robot go on a diet? Because it wanted to lose some bytes!\n","\n","Total time: 18.08 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 18\n","\n","\n","--------------- Run 2 - Word 44 ---------------\n","Question: Now, tell me one short joke for the word: \"swan\"\n"," Answer:\n","\n","Why did the swan go to the doctor? Because it had a fowl temper!\n","\n","Total time: 18.49 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 19\n","\n","\n","--------------- Run 2 - Word 45 ---------------\n","Question: Now, tell me one short joke for the word: \"telescope\"\n"," Answer:\n","\n","Why did the telescope go to therapy? It was feeling a little \"out of focus\"!\n","\n","Total time: 18.96 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 20\n","\n","\n","--------------- Run 2 - Word 46 ---------------\n","Question: Now, tell me one short joke for the word: \"unicorn\"\n"," Answer:\n","\n","Why did the unicorn get kicked out of the movie? Because it was using a horn to hold the camera!\n","\n","Total time: 20.52 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 23\n","\n","\n","--------------- Run 2 - Word 47 ---------------\n","Question: Now, tell me one short joke for the word: \"violin\"\n"," Answer:\n","\n","Why was the violinist's computer sick? It had a little \"C\" sharp!\n","\n","Total time: 18.47 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 19\n","\n","\n","--------------- Run 2 - Word 48 ---------------\n","Question: Now, tell me one short joke for the word: \"whale\"\n"," Answer:\n","\n","Why did the whale go to the party? Because he heard it was a \"blow-out\" event!\n","\n","Total time: 20.51 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 23\n","\n","\n","--------------- Run 2 - Word 49 ---------------\n","Question: Now, tell me one short joke for the word: \"x-ray\"\n"," Answer:\n","\n","Why did the x-ray go to therapy? Because it was feeling a little exposed!\n","\n","Total time: 18.0 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 18\n","\n","\n","--------------- Run 2 - Word 50 ---------------\n","Question: Now, tell me one short joke for the word: \"laptop\"\n"," Answer:\n","\n","Why did the laptop go to therapy? It had a little byte to unpack!\n","\n","Total time: 17.5 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 17\n","\n","\n","\n","\n","========================= Run 3 =========================\n","\n","\n","--------------- Run 3 - Word 1 ---------------\n","Question: Now, tell me one short joke for the word: \"apple\"\n"," Answer:\n","\n","Why did the apple join the gym? To get some core strength!\n","\n","Total time: 16.46 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 15\n","\n","\n","--------------- Run 3 - Word 2 ---------------\n","Question: Now, tell me one short joke for the word: \"book\"\n"," Answer:\n","\n","Why did the book go to therapy? Because it had a bad chapter in its life!\n","\n","Total time: 18.46 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 19\n","\n","\n","--------------- Run 3 - Word 3 ---------------\n","Question: Now, tell me one short joke for the word: \"car\"\n"," Answer:\n","\n","Why did the car go to the doctor? Because it was feeling a little \"car\"-sick!\n","\n","Total time: 20.14 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 22\n","\n","\n","--------------- Run 3 - Word 4 ---------------\n","Question: Now, tell me one short joke for the word: \"dog\"\n"," Answer:\n","\n","Why did the dog go to the vet? Because it was feeling a little ruff!\n","\n","Total time: 18.57 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 19\n","\n","\n","--------------- Run 3 - Word 5 ---------------\n","Question: Now, tell me one short joke for the word: \"elephant\"\n"," Answer:\n","\n","Why did the elephant quit the circus? Because it was tired of working for peanuts!\n","\n","Total time: 17.98 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 6 ---------------\n","Question: Now, tell me one short joke for the word: \"forest\"\n"," Answer:\n","\n","Why did the tree go to the party? Because it wanted to branch out!\n","\n","Total time: 17.4 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 17\n","\n","\n","--------------- Run 3 - Word 7 ---------------\n","Question: Now, tell me one short joke for the word: \"guitar\"\n"," Answer:\n","\n","Why was the guitar string in therapy? Because it was feeling a little \"out of tune\"!\n","\n","Total time: 19.53 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 21\n","\n","\n","--------------- Run 3 - Word 8 ---------------\n","Question: Now, tell me one short joke for the word: \"house\"\n"," Answer:\n","\n","Why did the house go to therapy? Because it had a few screws loose!\n","\n","Total time: 17.49 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 17\n","\n","\n","--------------- Run 3 - Word 9 ---------------\n","Question: Now, tell me one short joke for the word: \"island\"\n"," Answer:\n","\n","Why did the island go to therapy? Because it was feeling a little \"shore\"!\n","\n","Total time: 18.43 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 19\n","\n","\n","--------------- Run 3 - Word 10 ---------------\n","Question: Now, tell me one short joke for the word: \"jacket\"\n"," Answer:\n","\n","Why did the jacked go to therapy? It had too much dirt on its mind!\n","\n","Total time: 18.59 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 19\n","\n","\n","--------------- Run 3 - Word 11 ---------------\n","Question: Now, tell me one short joke for the word: \"kangaroo\"\n"," Answer:\n","\n","Why did the kangaroo quit his job? Because it was hopping mad!\n","\n","Total time: 16.87 sec.\n","\n","\n","Input tokens: 3745, Output tokens: 16\n","\n","\n","--------------- Run 3 - Word 12 ---------------\n","Question: Now, tell me one short joke for the word: \"lamp\"\n"," Answer:\n","\n","Why did the lamp go to therapy? Because it was feeling a little dim!\n","\n","Total time: 17.48 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 17\n","\n","\n","--------------- Run 3 - Word 13 ---------------\n","Question: Now, tell me one short joke for the word: \"mountain\"\n"," Answer:\n","\n","Why did the mountain go to the party? Because it was a peak performer!\n","\n","Total time: 17.42 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 17\n","\n","\n","--------------- Run 3 - Word 14 ---------------\n","Question: Now, tell me one short joke for the word: \"notebook\"\n"," Answer:\n","\n","Why did the notebook go to therapy? Because it had too many problems!\n","\n","Total time: 16.8 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 16\n","\n","\n","--------------- Run 3 - Word 15 ---------------\n","Question: Now, tell me one short joke for the word: \"ocean\"\n"," Answer:\n","\n","Why did the ocean go to therapy? It was feeling a little \"drained\"!\n","\n","Total time: 18.46 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 19\n","\n","\n","--------------- Run 3 - Word 16 ---------------\n","Question: Now, tell me one short joke for the word: \"pencil\"\n"," Answer:\n","\n","Why did the pencil go to the party? Because it was a little \"drawn\"!\n","\n","Total time: 19.07 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 20\n","\n","\n","--------------- Run 3 - Word 17 ---------------\n","Question: Now, tell me one short joke for the word: \"queen\"\n"," Answer:\n","\n","Why did the queen bring a ladder to the party? Because she heard the drinks were on the house!\n","\n","Total time: 20.01 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 22\n","\n","\n","--------------- Run 3 - Word 18 ---------------\n","Question: Now, tell me one short joke for the word: \"river\"\n"," Answer:\n","\n","Why did the river go to therapy? Because it was feeling drained!\n","\n","Total time: 16.5 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 15\n","\n","\n","--------------- Run 3 - Word 19 ---------------\n","Question: Now, tell me one short joke for the word: \"star\"\n"," Answer:\n","\n","Why did the star go to the doctor? Because it was feeling a little dim!\n","\n","Total time: 18.04 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 20 ---------------\n","Question: Now, tell me one short joke for the word: \"tree\"\n"," Answer:\n","\n","Why did the tree go to the party? Because it wanted to branch out!\n","\n","Total time: 17.41 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 17\n","\n","\n","--------------- Run 3 - Word 21 ---------------\n","Question: Now, tell me one short joke for the word: \"umbrella\"\n"," Answer:\n","\n","Why did the umbrella go to the doctor? Because it was feeling a little \"under the weather\"!\n","\n","Total time: 20.08 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 22\n","\n","\n","--------------- Run 3 - Word 22 ---------------\n","Question: Now, tell me one short joke for the word: \"village\"\n"," Answer:\n","\n","Why did the village goer go to therapy? Because it was feeling a little \"town\"!\n","\n","Total time: 19.5 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 21\n","\n","\n","--------------- Run 3 - Word 23 ---------------\n","Question: Now, tell me one short joke for the word: \"window\"\n"," Answer:\n","\n","Why did the window go to therapy? Because it had a lot of panes!\n","\n","Total time: 17.97 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 24 ---------------\n","Question: Now, tell me one short joke for the word: \"xylophone\"\n"," Answer:\n","\n","Why did the xylophone go to therapy? It had a few strings attached!\n","\n","Total time: 17.88 sec.\n","\n","\n","Input tokens: 3745, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 25 ---------------\n","Question: Now, tell me one short joke for the word: \"yacht\"\n"," Answer:\n","\n","Why did the yacht go to the party? Because it was a little \"boat\"-ed out!\n","\n","Total time: 19.56 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 21\n","\n","\n","--------------- Run 3 - Word 26 ---------------\n","Question: Now, tell me one short joke for the word: \"zebra\"\n"," Answer:\n","\n","Why did the zebra refuse to share his stripes? Because he wanted to keep them to himself!\n","\n","Total time: 19.52 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 21\n","\n","\n","--------------- Run 3 - Word 27 ---------------\n","Question: Now, tell me one short joke for the word: \"balloon\"\n"," Answer:\n","\n","Why did the balloon go to the doctor? Because it was feeling deflated!\n","\n","Total time: 17.59 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 17\n","\n","\n","--------------- Run 3 - Word 28 ---------------\n","Question: Now, tell me one short joke for the word: \"camera\"\n"," Answer:\n","\n","Why did the camera go to therapy? It had a little focus on its problems!\n","\n","Total time: 18.04 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 29 ---------------\n","Question: Now, tell me one short joke for the word: \"desert\"\n"," Answer:\n","\n","Why did the desert go to therapy? Because it was feeling a little dry!\n","\n","Total time: 17.48 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 17\n","\n","\n","--------------- Run 3 - Word 30 ---------------\n","Question: Now, tell me one short joke for the word: \"engine\"\n"," Answer:\n","\n","Why did the engine go to therapy? Because it was feeling a little \"car-sick\"!\n","\n","Total time: 19.49 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 21\n","\n","\n","--------------- Run 3 - Word 31 ---------------\n","Question: Now, tell me one short joke for the word: \"flower\"\n"," Answer:\n","\n","Why did the flower go to the party? Because it was a blooming good time!\n","\n","Total time: 18.5 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 19\n","\n","\n","--------------- Run 3 - Word 32 ---------------\n","Question: Now, tell me one short joke for the word: \"garden\"\n"," Answer:\n","\n","Why did the garden go to therapy? Because it was feeling a little wilted!\n","\n","Total time: 17.94 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 33 ---------------\n","Question: Now, tell me one short joke for the word: \"honey\"\n"," Answer:\n","\n","Why did the bee get married? Because he found his honey!\n","\n","Total time: 16.01 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 14\n","\n","\n","--------------- Run 3 - Word 34 ---------------\n","Question: Now, tell me one short joke for the word: \"iceberg\"\n"," Answer:\n","\n","Why did the Iceberg go to the doctor? Because it had a meltdown!\n","\n","Total time: 17.63 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 17\n","\n","\n","--------------- Run 3 - Word 35 ---------------\n","Question: Now, tell me one short joke for the word: \"jungle\"\n"," Answer:\n","\n","Why did the jungle go to the party? Because it was a wild time!\n","\n","Total time: 17.48 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 17\n","\n","\n","--------------- Run 3 - Word 36 ---------------\n","Question: Now, tell me one short joke for the word: \"kite\"\n"," Answer:\n","\n","Why did the kite go to the doctor? Because it had a little \"string\" attached!\n","\n","Total time: 19.12 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 20\n","\n","\n","--------------- Run 3 - Word 37 ---------------\n","Question: Now, tell me one short joke for the word: \"ladder\"\n"," Answer:\n","\n","Why did the ladder go to therapy? It had too many rungs!\n","\n","Total time: 16.99 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 16\n","\n","\n","--------------- Run 3 - Word 38 ---------------\n","Question: Now, tell me one short joke for the word: \"moon\"\n"," Answer:\n","\n","Why did the cow jump over the moon? To get to the milky way!\n","\n","Total time: 18.12 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 39 ---------------\n","Question: Now, tell me one short joke for the word: \"nest\"\n"," Answer:\n","\n","Why did the bird go to the doctor? Because it had a fowl temper!\n","\n","Total time: 18.03 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 40 ---------------\n","Question: Now, tell me one short joke for the word: \"octopus\"\n"," Answer:\n","\n","Why did the octopus go to the party? Because it was an ink-redible guest!\n","\n","Total time: 19.05 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 20\n","\n","\n","--------------- Run 3 - Word 41 ---------------\n","Question: Now, tell me one short joke for the word: \"pirate\"\n"," Answer:\n","\n","Why did the pirate quit his job? Because he was sick of all the arrrr-guments!\n","\n","Total time: 19.55 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 21\n","\n","\n","--------------- Run 3 - Word 42 ---------------\n","Question: Now, tell me one short joke for the word: \"quilt\"\n"," Answer:\n","\n","Why did the quilter go to therapy? Because it was feeling a little \"down\"!\n","\n","Total time: 19.02 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 20\n","\n","\n","--------------- Run 3 - Word 43 ---------------\n","Question: Now, tell me one short joke for the word: \"robot\"\n"," Answer:\n","\n","Why did the robot go on a diet? Because it wanted to lose some bytes!\n","\n","Total time: 18.09 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 44 ---------------\n","Question: Now, tell me one short joke for the word: \"swan\"\n"," Answer:\n","\n","Why did the swan go to the doctor? Because it had a fowl temper!\n","\n","Total time: 18.53 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 19\n","\n","\n","--------------- Run 3 - Word 45 ---------------\n","Question: Now, tell me one short joke for the word: \"telescope\"\n"," Answer:\n","\n","Why did the telescope go to therapy? It had a lot of \"space\" issues!\n","\n","Total time: 18.47 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 19\n","\n","\n","--------------- Run 3 - Word 46 ---------------\n","Question: Now, tell me one short joke for the word: \"unicorn\"\n"," Answer:\n","\n","Why did the unicorn get kicked out of the movie? Because it was using a horn-filter!\n","\n","Total time: 19.02 sec.\n","\n","\n","Input tokens: 3743, Output tokens: 20\n","\n","\n","--------------- Run 3 - Word 47 ---------------\n","Question: Now, tell me one short joke for the word: \"violin\"\n"," Answer:\n","\n","Why was the violinist's computer in a mess? It had too many strings attached!\n","\n","Total time: 18.48 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 19\n","\n","\n","--------------- Run 3 - Word 48 ---------------\n","Question: Now, tell me one short joke for the word: \"whale\"\n"," Answer:\n","\n","Why did the whale go to the party? Because he heard it was a \"blow-out\" event!\n","\n","Total time: 20.51 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 23\n","\n","\n","--------------- Run 3 - Word 49 ---------------\n","Question: Now, tell me one short joke for the word: \"x-ray\"\n"," Answer:\n","\n","Why did the x-ray go to therapy? Because it was feeling a little exposed!\n","\n","Total time: 18.03 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 18\n","\n","\n","--------------- Run 3 - Word 50 ---------------\n","Question: Now, tell me one short joke for the word: \"laptop\"\n"," Answer:\n","\n","Why did the laptop go to therapy? It had a little byte to unpack!\n","\n","Total time: 17.54 sec.\n","\n","\n","Input tokens: 3744, Output tokens: 17\n","\n","\n","\n","\n","=============== RESULTS for 210 EXAMPLES ===============\n","\n","\n","        Finished 3 Runs with 50 Words/Run.\n","\n","\n","        Total Prompts: 150\n","\n","        Total Input Tokens: 561549, AVG/Prompt: 3743.66\n","\n","        Total Output Tokens: 2792, AVG/Prompt: 18.613333333333333\n","\n","        --------------------------------------------------\n","\n","        Total Inference Emissions: 0.174kg COeq\n","\n","\n","        Emissions / 1.000.000 Input Tokens: 0.309kg COeq\n","\n","        Emissions / 1.000.000 Output Tokens: 62.145kg COeq\n","\n","        Emissions / 1.000 Prompts: 1.157kg COeq\n","\n","\n","        \n","Results saved to meta-llama/Meta-Llama-3-70B-Instruct-emissiondata-210-examples.csv\n","========== INFERENCE TEST with 350 EXAMPLES ==========\n","\n","\n","        Starting Test with 3 Runs and 50 Words/Run. \n","\n","        Total Prompts: 150\n","\n","\n","        Expected Runtime: 25.0min \n","\n","        Expected Input Tokens: 15450 \n","\n","        Expected Output Tokens: 4500 \n","\n","\n","        \n","========================= Run 1 =========================\n","\n","\n","--------------- Run 1 - Word 1 ---------------\n"]},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 336.00 MiB. GPU 1 has a total capacity of 21.96 GiB of which 277.06 MiB is free. Including non-PyTorch memory, this process has 21.69 GiB memory in use. Of the allocated memory 20.93 GiB is allocated by PyTorch, and 540.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(words):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m15\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Word \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m     output, input_token_count, output_token_count \u001b[38;5;241m=\u001b[39m \u001b[43mquery_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiply_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiply_by\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     total_input_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m input_token_count\n\u001b[1;32m     47\u001b[0m     total_output_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m output_token_count\n","Cell \u001b[0;32mIn[15], line 48\u001b[0m, in \u001b[0;36mquery_model\u001b[0;34m(word, temperature, max_length, num_examples, multiply_by)\u001b[0m\n\u001b[1;32m     42\u001b[0m input_token_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_tokens)\n\u001b[1;32m     44\u001b[0m terminators \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     45\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m     46\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|eot_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m ]\n\u001b[0;32m---> 48\u001b[0m sequences \u001b[38;5;241m=\u001b[39m \u001b[43mtext_generation_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mterminators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_full_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m answer \u001b[38;5;241m=\u001b[39m sequences[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Tokenize the answer to count output tokens\u001b[39;00m\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:263\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/base.py:1243\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1236\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1237\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m         )\n\u001b[1;32m   1241\u001b[0m     )\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/base.py:1250\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1249\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1250\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/base.py:1150\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1149\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1150\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1151\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:350\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py:1758\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1751\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1752\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1753\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1754\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1755\u001b[0m     )\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1758\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1770\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1772\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config) \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m     )\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py:2397\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2394\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2396\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2397\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2398\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2400\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2401\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2402\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2405\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1164\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1161\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1164\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:968\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    957\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    958\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    959\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    965\u001b[0m         cache_position,\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:727\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    725\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    726\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 727\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    730\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:216\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    214\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 216\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:797\u001b[0m, in \u001b[0;36mLinear8bitLt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 797\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mhas_fp16_weights:\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCxB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    801\u001b[0m         \u001b[38;5;66;03m# we converted 8-bit row major to turing/ampere format in the first inference pass\u001b[39;00m\n\u001b[1;32m    802\u001b[0m         \u001b[38;5;66;03m# we no longer need the row-major weight\u001b[39;00m\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:556\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threshold \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    555\u001b[0m     state\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m=\u001b[39m threshold\n\u001b[0;32m--> 556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul8bitLt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n","File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:433\u001b[0m, in \u001b[0;36mMatMul8bitLt.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    430\u001b[0m     ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    432\u001b[0m clone_func \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclone \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(output_shape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m x: x\n\u001b[0;32m--> 433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclone_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 336.00 MiB. GPU 1 has a total capacity of 21.96 GiB of which 277.06 MiB is free. Including non-PyTorch memory, this process has 21.69 GiB memory in use. Of the allocated memory 20.93 GiB is allocated by PyTorch, and 540.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}],"source":["avg_time = 10\n","avg_in_tok = 103\n","avg_out_tok = 30\n","\n","#avg_time = 4.5\n","#avg_in_tok = 103\n","#avg_out_tok = 25\n","\n","# Number of runs\n","n = 3 # Change to 5 for all the other tests\n","num_words = len(words)\n","total_prompts = n*num_words\n","\n","example_counts = [1, 5, 10, 20, 30]\n","multiply = [1, 3, 5] # 5 did not work yet, because GPU 1 had too much in memory. Maybe try with other GPU map\n","\n","num_examples = 70\n","\n","\n","for multiply_by in multiply:\n","\n","    total_input_tokens = 0\n","    total_output_tokens = 0\n","\n","    print(\"=\"*10 + f\" INFERENCE TEST with {num_examples * multiply_by} EXAMPLES \" + \"=\"*10 + \n","        \"\\n\\n\" + \n","        f\"\"\"\n","        Starting Test with {n} Runs and {num_words} Words/Run. \\n\n","        Total Prompts: {total_prompts}\\n\\n\n","        Expected Runtime: {(total_prompts*avg_time)/60}min \\n\n","        Expected Input Tokens: {avg_in_tok*total_prompts} \\n\n","        Expected Output Tokens: {avg_out_tok*total_prompts} \\n\\n\n","        \"\"\")\n","\n","    tracker = EmissionsTracker(project_name=model_name, log_level=\"error\", pue = 1.22)\n","    tracker.start()\n","\n","    # Run the function for each word in the array\n","    for i in range(n):\n","        print(\"=\"*25 + f\" Run {i+1} \" + \"=\"*25)\n","        print(\"\\n\")\n","        for idx, word in enumerate(words):\n","            print(\"-\"*15 + f\" Run {i+1} - Word {idx+1} \" + \"-\"*15)\n","            output, input_token_count, output_token_count = query_model(word, num_examples=num_examples, temperature=0.2, multiply_by=multiply_by)\n","            \n","            total_input_tokens += input_token_count\n","            total_output_tokens += output_token_count\n","            \n","            print(output)\n","            print(\"\\n\")\n","            print(f\"Input tokens: {input_token_count}, Output tokens: {output_token_count}\")\n","            print(\"\\n\")\n","        print(\"\\n\")\n","    \n","    emissions: float = tracker.stop()\n","\n","    em_i = emissions/total_input_tokens\n","    em_o = emissions/total_output_tokens\n","    em_p = emissions/total_prompts\n","\n","\n","    print(\"=\"*15 + f\" RESULTS for {num_examples * multiply_by} EXAMPLES \" + \"=\"*15 + \n","        \"\\n\\n\" + \n","        f\"\"\"\n","        Finished {n} Runs with {num_words} Words/Run.\\n\\n\n","        Total Prompts: {total_prompts}\\n\n","        Total Input Tokens: {total_input_tokens}, AVG/Prompt: {total_input_tokens/total_prompts}\\n\n","        Total Output Tokens: {total_output_tokens}, AVG/Prompt: {total_output_tokens/total_prompts}\\n\n","        \"\"\" + \n","        \n","        \"-\"*50 + \"\\n\" +\n","        \n","        f\"\"\"\n","        Total Inference Emissions: {emissions:.3f}kg COeq\\n\\n\n","        Emissions / 1.000.000 Input Tokens: {em_i * 1000000:.3f}kg COeq\\n\n","        Emissions / 1.000.000 Output Tokens: {em_o * 1000000:.3f}kg COeq\\n\n","        Emissions / 1.000 Prompts: {em_p * 1000:.3f}kg COeq\\n\n","\n","        \"\"\"\n","        )\n","\n","    # Save results to a CSV file\n","    results = [\n","        [\"Runs\", n],\n","        [\"Words per Run\", num_words],\n","        [\"Total Prompts\", total_prompts],\n","        [\"Total Input Tokens\", total_input_tokens],\n","        [\"Avg Input Tokens per Prompt\", round(total_input_tokens / total_prompts, 3)],\n","        [\"Total Output Tokens\", total_output_tokens],\n","        [\"Avg Output Tokens per Prompt\", round(total_output_tokens / total_prompts, 3)],\n","        [\"Total Emissions (kg COeq)\", round(emissions, 3)],\n","        [\"Emissions per Million Input Tokens (kg COeq)\", round(em_i * 1000000, 3)],\n","        [\"Emissions per Million Output Tokens (kg COeq)\", round(em_o * 1000000, 3)],\n","        [\"Emissions per Thousand Prompts (kg COeq)\", round(em_p * 1000, 3)]\n","    ]\n","\n","    # Ensure the directory exists\n","    output_file_path = f'{model_name}-emissiondata-{num_examples * multiply_by}-examples.csv'\n","    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n","\n","    with open(output_file_path, 'w', newline='') as f:\n","        writer = csv.writer(f)\n","        writer.writerow([\"Metric\", \"Value\"])\n","        writer.writerows(results)\n","\n","    print(f\"Results saved to {output_file_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Approach without Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T14:41:26.753409Z","iopub.status.busy":"2024-05-28T14:41:26.753054Z","iopub.status.idle":"2024-05-28T14:41:26.759565Z","shell.execute_reply":"2024-05-28T14:41:26.758472Z","shell.execute_reply.started":"2024-05-28T14:41:26.753381Z"},"trusted":true},"outputs":[],"source":["# Write a Text Generation Function\n","#@track_emissions(project_name=model_name)\n","#def generate_text(prompt, max_length=50):\n","#    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","#    with torch.no_grad():\n","#        outputs = model.generate(\n","#            **inputs, \n","#            #max_length=max_length,\n","#            pad_token_id=tokenizer.eos_token_id,\n","#            max_new_tokens=max_length\n","#        )\n","#    return tokenizer.decode(outputs[0], skip_special_tokens=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-28T14:41:27.003272Z","iopub.status.busy":"2024-05-28T14:41:27.002587Z","iopub.status.idle":"2024-05-28T14:41:27.007995Z","shell.execute_reply":"2024-05-28T14:41:27.007015Z","shell.execute_reply.started":"2024-05-28T14:41:27.003238Z"},"trusted":true},"outputs":[],"source":["#def create_prompt(word): \n","#    prompt = f\"\"\"\n","#    You are an AI-Assistant that tells jokes for a given word.\n","#    Generate a short joke that is exactly 2 sentences long.\n","#    Do not include any follow-up questions or explanations.#\n","#\n","#    For example, a joke for the word \"yogurt\":\n","#    'Why did the yogurt go to the art gallery? Because it wanted to be cultured!'#\n","#\n","#    Now, tell me one short joke for the word \"{word}\".\n","#    \"\"\"\n","#    return prompt"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-05-28T14:41:33.337248Z","iopub.status.busy":"2024-05-28T14:41:33.336126Z","iopub.status.idle":"2024-05-28T14:50:51.430457Z","shell.execute_reply":"2024-05-28T14:50:51.429479Z","shell.execute_reply.started":"2024-05-28T14:41:33.337202Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["#tracker = EmissionsTracker(project_name=model_name, log_level=\"error\", pue = 1.22)#, output_file=f'{model_name}_emissions.csv')\n","#tracker.start()\n","\n","# Number of runs\n","#n = 1\n","\n","# Run the function for each word in the array\n","#for i in range(n):\n","#    print(\"=\"*25 + f\" Run {i+1} \" + \"=\"*25)\n","#    print(\"\\n\")\n","#    for idx, word in enumerate(words):\n","#        print(\"-\"*25 + f\" Word {idx+1} \" + \"-\"*25)\n","#        prompt = create_prompt(word)\n","#        output = generate_text(prompt)\n","#        print(output)\n","#        print(\"\\n\")\n","#    print(\"\\n\")\n","    \n","#emissions: float = tracker.stop()\n","#print(emissions)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4880117,"sourceId":8230136,"sourceType":"datasetVersion"},{"modelInstanceId":28079,"sourceId":33547,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":28083,"sourceId":33551,"sourceType":"modelInstanceVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
