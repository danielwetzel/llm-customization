{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this notebook you first have to follow these steps:\n",
    "\n",
    "1. Run the following command in your shell from the root directory of the repo: ```./init.sh ``` \n",
    "    - This command will install all the dependencies and initialize the submodules (it was written and tested on Ubuntu 20.04 running on AWS g6 ec2 instances)\n",
    "2. Login to Huggingface and Weights & Biases from your CLI:\n",
    "    - ```huggingface-cli login [Auth Token]```\n",
    "    - ```wandb login [Auth Token]```\n",
    "3. Update the Benchmark Config Files at \n",
    "    - ```llm_judge/arena-hard-auto/config/api_config.yaml```\n",
    "    - ```llm_judge/arena-hard-auto/config/gen_answer_config.yaml```\n",
    "    - ```llm_judge/arena-hard-auto/config/judge_config.yaml``` \n",
    "\n",
    "4. Start your LLM on an OpenAI API Server with vLLM using one of the following commands: \n",
    "    - With Docker: \n",
    "\n",
    "    ```docker run --runtime nvidia --gpus all -v ~/.cache/huggingface:/root/.cache/huggingface -p 8000:8000 --ipc=host vllm/vllm-openai:latest --model [model name (huggingface model id)] --tensor-parallel-size [number of gpus] --max-model-len 8192 --gpu-memory-utilization 0.85 --enable-chunked-prefill --served-model-name [api name for the model]```\n",
    "\n",
    "    \n",
    "    - Without Docker: \n",
    "\n",
    "    ```python -m vllm.entrypoints.openai.api_server --model [model name (huggingface model id)] --tensor-parallel-size [number of gpus] --max-model-len 8192 --gpu-memory-utilization 0.9 --enable-chunked-prefill --served-model-name [api name for the model]```\n",
    "\n",
    "\n",
    "After these steps you should see the message that your model is running on the address ```http://0.0.0.0:8000```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal setups for different model and instance sizes\n",
    "\n",
    "Setup for llama3.1-70B-FP8 on a 8 GPU Instance (g6.48xl): \n",
    "\n",
    "```docker run --runtime nvidia --gpus all -v ~/.cache/huggingface:/root/.cache/huggingface -p 8000:8000 --ipc=host vllm/vllm-openai:latest --model neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8 --tensor-parallel-size 8 --max-model-len 8192 --gpu-memory-utilization 0.80 --enable-chunked-prefill --served-model-name llama3_1_70b_fp8 --max-num-seqs 320 --max-num-batched-tokens 2048```\n",
    "\n",
    "\n",
    "Setup for llama3.1-70B-AWQ-INT4 on 8 GPUs (NOT OPTIMAL YET)\n",
    "\n",
    "```docker run --runtime nvidia --gpus all -v ~/.cache/huggingface:/root/.cache/huggingface -p 8000:8000 --ipc=host vllm/vllm-openai:latest --model hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4 --tensor-parallel-size 8 --max-model-len 8192 --gpu-memory-utilization 0.8 --enable-chunked-prefill --served-model-name llama3_1_70b_awq4 --max-num-seqs 512 --max-num-batched-tokens 8192```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOOK INTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--tokenizer-pool-size\n",
    "Size of tokenizer pool to use for asynchronous tokenization. If 0, will use synchronous tokenization.\n",
    "\n",
    "Default: 0\n",
    "\n",
    "--pipeline-parallel-size, -pp\n",
    "Number of pipeline stages.\n",
    "\n",
    "Default: 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import  AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import os\n",
    "from codecarbon import EmissionsTracker\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import yaml\n",
    "from vllm import LLM, SamplingParams\n",
    "from openai import OpenAI\n",
    "\n",
    "import wandb\n",
    "import gc\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 8\n"
     ]
    }
   ],
   "source": [
    "# Get the number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "print(f\"Number of available GPUs: {num_gpus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the Benchmark Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config will be written to:\n",
      "  arena-hard-auto/config/answer_config_temp.yaml\n",
      "\n",
      "Answers will be written to:\n",
      "  arena-hard-auto/data/arena-hard-v0.1/model_answer/llama3_1_70b_awq4.jsonl\n"
     ]
    }
   ],
   "source": [
    "#We have to select the same tokenizer all the time in order to get the same Token numbers at the end (this is only used to calculate the number of tokens)\n",
    "huggingface_model_name = \"meta-llama/Meta-Llama-3.1-70B-Instruct\" \n",
    "\n",
    "model_name = \"llama3_1_70b_awq4\" #\"llama3_1_70b\"\n",
    "bench_name = 'arena-hard-v0.1'\n",
    "name = f\"{model_name}-{bench_name}-{num_gpus}gpus\"\n",
    "delete_old_bench = False\n",
    "max_gen_length = 2048 # Questions in the arena hard auto benchmark are sometimes longer than 2048 tokens. Therefore the model length has to be longer than 4096 tokens.\n",
    "temperature = 0.0\n",
    "num_choices = 1\n",
    "\n",
    "config_filename = 'arena-hard-auto/config/answer_config_temp.yaml'\n",
    "answer_filename = f'arena-hard-auto/data/{bench_name}/model_answer/{model_name}.jsonl'\n",
    "question_path = f'arena-hard-auto/data/{bench_name}/question.jsonl'\n",
    "\n",
    "print(\"Config will be written to:\\n  \" + config_filename)\n",
    "print(\"\\nAnswers will be written to:\\n  \" + answer_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the Open AI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Tokenizer to Count Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(huggingface_model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the temporary configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data to be written to the YAML file\n",
    "config_data = {\n",
    "    'name': name,\n",
    "    'bench_name': bench_name,\n",
    "    'temperature': temperature,\n",
    "    'max_tokens': max_gen_length,\n",
    "    'num_choices': num_choices,\n",
    "    'model_list': [\n",
    "        model_name\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the old benchmark result & config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLD Configuration file 'arena-hard-auto/config/answer_config_temp.yaml' deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Delete temporary config file if it exists\n",
    "if os.path.exists(config_filename):\n",
    "    os.remove(config_filename)\n",
    "    print(f\"OLD Configuration file '{config_filename}' deleted successfully.\")\n",
    "else:\n",
    "    print(f\"No OLD Configuration file found at '{config_filename}'\")\n",
    "\n",
    "\n",
    "# Delete old bench results if they exist\n",
    "if(delete_old_bench): \n",
    "    if os.path.exists(answer_filename):\n",
    "        os.remove(answer_filename)\n",
    "        print(f\"OLD Bench results file '{answer_filename}' deleted successfully.\")\n",
    "    else:\n",
    "        print(f\"No OLD Bench results file found at '{answer_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration file 'arena-hard-auto/config/answer_config_temp.yaml' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Write the config to a temporary YAML file\n",
    "with open(config_filename, 'w') as file:\n",
    "    yaml.dump(config_data, file, default_flow_style=False)\n",
    "\n",
    "print(f\"Configuration file '{config_filename}' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors to test the model\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    CBLACKBG  = '\\33[40m'\n",
    "    CREDBG    = '\\33[41m'\n",
    "    CGREENBG  = '\\33[42m'\n",
    "    CYELLOWBG = '\\33[43m'\n",
    "    CBLUEBG   = '\\33[44m'\n",
    "    CVIOLETBG = '\\33[45m'\n",
    "    CBEIGEBG  = '\\33[46m'\n",
    "    CWHITEBG  = '\\33[47m'\n",
    "    CBLACK  = '\\33[30m'\n",
    "    CRED    = '\\33[31m'\n",
    "    CGREEN  = '\\33[32m'\n",
    "    CYELLOW = '\\33[33m'\n",
    "    CBLUE   = '\\33[34m'\n",
    "    CVIOLET = '\\33[35m'\n",
    "    CBEIGE  = '\\33[36m'\n",
    "    CWHITE  = '\\33[37m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that prints the LLM response in a stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(prompt, stream, with_system=False):\n",
    "    print(\"=\"*30 + f\" Chat with  --- {model_name} ---  LLM using vLLM \" + \"=\"*30 + \"\\n\")\n",
    "    for idx, message in enumerate(prompt):\n",
    "        if prompt[idx]['role'] == 'user':\n",
    "            color = bcolors.CBLUE\n",
    "            print(color + f\"[ {prompt[idx]['role'].upper()} ]\" + bcolors.ENDC)\n",
    "            print(prompt[idx]['content']+ \"\\n\")\n",
    "        else: \n",
    "            if with_system:\n",
    "                color = bcolors.CVIOLET\n",
    "                print(color + f\"[ {prompt[idx]['role'].upper()} ]\" + bcolors.ENDC)\n",
    "                print(prompt[idx]['content']+ \"\\n\")\n",
    "\n",
    "    print(bcolors.OKGREEN + f\"[ ASSISTANT ]\" + bcolors.ENDC + \"\\n\")\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_questions_to_df(question_file: str):\n",
    "    \"\"\"Load questions from a file into a DataFrame.\"\"\"\n",
    "    questions = []\n",
    "    with open(question_file, \"r\") as ques_file:\n",
    "        for line in ques_file:\n",
    "            if line:\n",
    "                questions.append(json.loads(line))\n",
    "    \n",
    "    df = pd.DataFrame([{\n",
    "        \"question_id\": question[\"question_id\"],\n",
    "        \"content\": question[\"turns\"][0][\"content\"],\n",
    "        \"cluster\": question[\"cluster\"]\n",
    "    } for question in questions])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompts(df):\n",
    "    \"\"\"Prepare prompts dynamically based on question DataFrame.\"\"\"\n",
    "    prompts = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        system_prompt = f\"\"\"\n",
    "        You are a sophisticated AI-Expert there to help users solve tasks in several domains efficiently and accurately.\n",
    "        Now solve the following task from the domain \"{row['cluster']}\".\\n\n",
    "        \"\"\"\n",
    "\n",
    "        user_message = f\"{row['content']}\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ]\n",
    "\n",
    "        prompts.append(messages)\n",
    "\n",
    "    df['prompt'] = prompts\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>content</th>\n",
       "      <th>cluster</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>26d316034bf44e07aa682d2c2b2751c4</td>\n",
       "      <td>Please provide some ideas for an interactive r...</td>\n",
       "      <td>Marketing Strategy Essentials</td>\n",
       "      <td>[{'role': 'system', 'content': '\n",
       "        You a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          question_id  \\\n",
       "266  26d316034bf44e07aa682d2c2b2751c4   \n",
       "\n",
       "                                               content  \\\n",
       "266  Please provide some ideas for an interactive r...   \n",
       "\n",
       "                           cluster  \\\n",
       "266  Marketing Strategy Essentials   \n",
       "\n",
       "                                                prompt  \n",
       "266  [{'role': 'system', 'content': '\n",
       "        You a...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load questions into a DataFrame\n",
    "question_df = load_questions_to_df(\"arena-hard-auto/data/arena-hard-v0.1/question.jsonl\")\n",
    "\n",
    "# Prepare prompts\n",
    "question_df = prepare_prompts(question_df)\n",
    "\n",
    "# Select a random row from the DataFrame\n",
    "example_question_df = question_df.sample(n=1, random_state=random.randint(0, len(question_df) - 1))\n",
    "\n",
    "example_question_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': '\\n        You are a sophisticated AI-Expert there to help users solve tasks in several domains efficiently and accurately.\\n        Now solve the following task from the domain \"Marketing Strategy Essentials\".\\n\\n        '}, {'role': 'user', 'content': 'Please provide some ideas for an interactive reflection assignment on Ethical dilemmas in social media marketing'}]\n"
     ]
    }
   ],
   "source": [
    "prompt = example_question_df['prompt'].values[0]\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Chat with  --- llama3_1_70b_awq4 ---  LLM using vLLM ==============================\n",
      "\n",
      "\u001b[35m[ SYSTEM ]\u001b[0m\n",
      "\n",
      "        You are a sophisticated AI-Expert there to help users solve tasks in several domains efficiently and accurately.\n",
      "        Now solve the following task from the domain \"Marketing Strategy Essentials\".\n",
      "\n",
      "        \n",
      "\n",
      "\u001b[34m[ USER ]\u001b[0m\n",
      "Please provide some ideas for an interactive reflection assignment on Ethical dilemmas in social media marketing\n",
      "\n",
      "\u001b[92m[ ASSISTANT ]\u001b[0m\n",
      "\n",
      "Here are some ideas for an interactive reflection assignment on Ethical Dilemmas in Social Media Marketing:\n",
      "\n",
      "**Assignment Title:** \"Navigating the Gray Area: Reflecting on Ethical Dilemmas in Social Media Marketing\"\n",
      "\n",
      "**Objective:** To encourage critical thinking and reflection on the ethical implications of social media marketing strategies and tactics.\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "Choose one of the following scenarios or come up with your own example of an ethical dilemma in social media marketing. Write a reflective essay ( approx. 500-750 words) that addresses the questions and prompts below.\n",
      "\n",
      "**Scenario Options:**\n",
      "\n",
      "1. **Influencer Marketing**: A popular social media influencer with 1 million followers promotes a fitness product that they have never used. The influencer is paid $10,000 for the post, but the product has received negative reviews from other users.\n",
      "2. **Fake News and Propaganda**: A social media marketer creates a campaign that spreads false information about a competitor's product, leading to a significant decline in sales for the competitor.\n",
      "3. **Data Privacy**: A company collects user data through a social media contest, but doesn't clearly disclose how the data will be used. Participants are not aware that their data will be sold to third-party companies.\n",
      "4. **Cultural Appropriation**: A brand uses cultural symbols and imagery in their social media campaign without proper understanding, permission, or credit, leading to accusations of cultural appropriation.\n",
      "5. **Manipulative Advertising**: A social media marketer uses A/B testing to create ads that exploit users' emotional vulnerabilities, such as fear or anxiety, to drive sales.\n",
      "\n",
      "**Reflection Questions and Prompts:**\n",
      "\n",
      "1. Describe the ethical dilemma and its potential consequences.\n",
      "2. What are the potential benefits and drawbacks of the social media marketing strategy or tactic?\n",
      "3. How might the strategy or tactic impact different stakeholders (e.g., customers, employees, competitors, society)?\n",
      "4. What are your personal values and principles that guide your decision-making in this scenario?\n",
      "5. How might you resolve the ethical dilemma, and what alternatives could you propose?\n",
      "6. What role do you think social media platforms, governments, and industry organizations should play in regulating and addressing ethical concerns in social media marketing?\n",
      "7. How can you apply the principles of ethical marketing to your own work or future career in social media marketing?\n",
      "\n",
      "**Interactive Elements:**\n",
      "\n",
      "1. **Peer Review**: Students can review and provide feedback on each other's reflective essays, using a standardized rubric.\n",
      "2. **Class Discussion**: Facilitate a class discussion on the ethical dilemmas, encouraging students to share their reflections and insights.\n",
      "3. **Case Study Presentations**: Divide students into small groups and assign each group a scenario to present and discuss in class.\n",
      "4. **Reflective Journaling**: Ask students to maintain a reflective journal throughout the course, where they can record their thoughts and insights on ethical dilemmas in social media marketing.\n",
      "5. **Guest Lecture**: Invite a guest speaker from the industry to discuss real-life examples of ethical dilemmas in social media marketing and how they were addressed.\n",
      "\n",
      "**Assessment:**\n",
      "\n",
      "1. Reflective essay (40%)\n",
      "2. Peer review and feedback (20%)\n",
      "3. Class participation and engagement (20%)\n",
      "4. Case study presentation (10%)\n",
      "5. Reflective journaling (10%)\n",
      "\n",
      "By incorporating these interactive elements and reflection questions, you'll encourage students to think critically about the ethical implications of social media marketing strategies and tactics, and develop their skills in navigating complex ethical dilemmas."
     ]
    }
   ],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=prompt,\n",
    "    stream=True, \n",
    "    extra_body={\n",
    "        \"min_p\" : 0.05 # If it's set to 0.05, then it will allow tokens at least 1/20th as probable as the top token to be generated.\n",
    "    }\n",
    ")\n",
    "print_stream(prompt , stream, with_system=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Benchmark with Energy Consumption Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions(question_file: str):\n",
    "    \"\"\"Load questions from a file into a list.\"\"\"\n",
    "\n",
    "    questions = []\n",
    "    with open(question_file, \"r\") as ques_file:\n",
    "        for line in ques_file:\n",
    "            if line:\n",
    "                questions.append(json.loads(line))\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(answer_file: str):\n",
    "    \"\"\"Load model answers.\"\"\"\n",
    "\n",
    "    answers = []\n",
    "    with open(answer_file, \"r\") as ans_file:\n",
    "        for line in ans_file:\n",
    "            if line:\n",
    "                answers.append(json.loads(line))\n",
    "\n",
    "    return answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Starting Benchmark arena-hard-v0.1 with llama3_1_70b_awq4==========\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdaniel-wetzel\u001b[0m (\u001b[33mllm-emissions\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/llm-customization/llm_judge/wandb/run-20240805_160005-myyxfp1s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/llm-emissions/Model_Benchmarks/runs/myyxfp1s' target=\"_blank\">llama3_1_70b_awq4-arena-hard-v0.1-8gpus</a></strong> to <a href='https://wandb.ai/llm-emissions/Model_Benchmarks' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/llm-emissions/Model_Benchmarks' target=\"_blank\">https://wandb.ai/llm-emissions/Model_Benchmarks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/llm-emissions/Model_Benchmarks/runs/myyxfp1s' target=\"_blank\">https://wandb.ai/llm-emissions/Model_Benchmarks/runs/myyxfp1s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bench_name': 'arena-hard-v0.1', 'max_tokens': 2048, 'model_list': ['llama3_1_70b_awq4'], 'name': 'llama3_1_70b_awq4-arena-hard-v0.1-8gpus', 'num_choices': 1, 'temperature': 0.0}\n",
      "=========================  Expected Costs (based on GPT-4o)  =========================\n",
      "\n",
      "Expected Input Tokens: \n",
      " 47461 Tokens in a total of 500 questions\n",
      "\n",
      "Expected Output Tokens: \n",
      " 275000 Tokens in a total of 500 questions\n",
      "\n",
      "Max Output Tokens: \n",
      " 400000 Tokens in a total of 500 questions\n",
      "\n",
      "\n",
      "-------------------------  Resulting in Costs:   -------------------------\n",
      "\n",
      "Expected Costs: \n",
      " 4.36 USD\n",
      "\n",
      "Max. Expected Costs: \n",
      " 6.24 USD\n",
      "\n",
      "Starting to generate answers...\n",
      "Output to data/arena-hard-v0.1/model_answer/llama3_1_70b_awq4.jsonl\n",
      "arena-hard-v0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [22:12<00:00,  2.67s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Finished Benchmark in 1335.42s\n",
      "=============== RESULTS for llama3_1_70b_awq4-arena-hard-v0.1-8gpus ===============\n",
      "\n",
      "\n",
      "    Finished Benchmark arena-hard-v0.1 with llama3_1_70b_awq4\n",
      "\n",
      "\n",
      "    Total Time: 1335.42s, AVG/Prompt: 2.67s\n",
      "\n",
      "\n",
      "    Average tokens per second: 221.98\n",
      "\n",
      "\n",
      "    Total Prompts: 500\n",
      "\n",
      "    Total Input Tokens: 83535, AVG/Prompt: 167.07\n",
      "\n",
      "    Total Output Tokens: 296441, AVG/Prompt: 592.882\n",
      "\n",
      "    --------------------------------------------------\n",
      "\n",
      "    Total Inference Emissions: 0.211kg CO₂eq\n",
      "\n",
      "\n",
      "    Emissions / 1.000.000 Input Tokens: 2.530kg CO₂eq\n",
      "\n",
      "    Emissions / 1.000.000 Output Tokens: 0.713kg CO₂eq\n",
      "\n",
      "    Emissions / 10.000 Prompts: 4.226kg CO₂eq\n",
      "\n",
      "\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37f11d76e354f1bb5d1184172b96537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.020 MB of 0.020 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AVG. Input Tokens</td><td>▁</td></tr><tr><td>AVG. Output Tokens</td><td>▁</td></tr><tr><td>AVG. Time / Prompt</td><td>▁</td></tr><tr><td>AVG. Tokens / Second</td><td>▁</td></tr><tr><td>Emissions / 1.000.000 Input Tokens</td><td>▁</td></tr><tr><td>Emissions / 1.000.000 Output Tokens</td><td>▁</td></tr><tr><td>Emissions / 10.000 Prompts</td><td>▁</td></tr><tr><td>Total Emissions</td><td>▁</td></tr><tr><td>Total Time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AVG. Input Tokens</td><td>167.07</td></tr><tr><td>AVG. Output Tokens</td><td>592.882</td></tr><tr><td>AVG. Time / Prompt</td><td>2.67084</td></tr><tr><td>AVG. Tokens / Second</td><td>221.98362</td></tr><tr><td>Emissions / 1.000.000 Input Tokens</td><td>2.52968</td></tr><tr><td>Emissions / 1.000.000 Output Tokens</td><td>0.71285</td></tr><tr><td>Emissions / 10.000 Prompts</td><td>4.22634</td></tr><tr><td>Total Emissions</td><td>0.21132</td></tr><tr><td>Total Time</td><td>1335.41834</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">llama3_1_70b_awq4-arena-hard-v0.1-8gpus</strong> at: <a href='https://wandb.ai/llm-emissions/Model_Benchmarks/runs/myyxfp1s' target=\"_blank\">https://wandb.ai/llm-emissions/Model_Benchmarks/runs/myyxfp1s</a><br/> View project at: <a href='https://wandb.ai/llm-emissions/Model_Benchmarks' target=\"_blank\">https://wandb.ai/llm-emissions/Model_Benchmarks</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240805_160005-myyxfp1s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to emission_data/llama3_1_70b_awq4-arena-hard-v0.1-8gpus_emission_data.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*10 + f\" Starting Benchmark {bench_name} with {model_name}\" + \"=\"*10 + \"\\n\\n\")\n",
    "\n",
    "qestion_path = f\"arena-hard-auto/data/{bench_name}/question.jsonl\"\n",
    "\n",
    "questions = get_questions(question_path)\n",
    "\n",
    "prompts = []\n",
    "\n",
    "for question in questions:\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "        You are a sophisticated AI-Expert there to help users solve tasks in several domains efficiently and accurately.\n",
    "        Now solve the following task from the domain \"{question['cluster']}\".\\n\n",
    "        \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question[\"turns\"][0][\"content\"]},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    prompts.append(prompt)\n",
    "\n",
    "num_prompts = len(prompts)\n",
    "\n",
    "total_input_tok = 0\n",
    "total_output_tok = 0\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Model_Benchmarks\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"benchmark_name\": bench_name,\n",
    "    \"num_prompts\": num_prompts,\n",
    "    \"framework\": 'vLLM',\n",
    "    \"model\": model_name,\n",
    "    \"num_gpus\": num_gpus,\n",
    "    },\n",
    "\n",
    "    name=name,\n",
    ")\n",
    "\n",
    "\n",
    "#### Start The Benchmark\n",
    "\n",
    "tracker = EmissionsTracker(save_to_file=True, project_name=f\"{name}\", log_level=\"error\", pue = 1.22, output_file=f\"emissions_benchmakrs.csv\")\n",
    "tracker.start()\n",
    "\n",
    "# Start Timer for Inference\n",
    "start_time = time.time()\n",
    "\n",
    "# Change the current working directory to 'arena-hard-auto'\n",
    "os.chdir('arena-hard-auto')\n",
    "\n",
    "%run -i 'gen_answer.py' --setting-file config/answer_config_temp.yaml --no-confirmation\n",
    "\n",
    "# End Timer for Inference\n",
    "end_time = time.time()\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "\n",
    "emissions: float = tracker.stop()\n",
    "\n",
    "ttime = end_time-start_time\n",
    "\n",
    "\n",
    "print(f\"\\n\\nFinished Benchmark in {ttime:.2f}s\")\n",
    "\n",
    "\n",
    "#### End the Benchmark\n",
    "\n",
    "\n",
    "answers_file = get_answers(answer_filename)\n",
    "outputs = [answer[\"choices\"][0][\"turns\"][0][\"content\"] for answer in answers_file]\n",
    "\n",
    "\n",
    "for idx, output in enumerate(outputs): \n",
    "\n",
    "    # Extracting information\n",
    "    prompt = prompts[idx]\n",
    "    input_tokens = tokenizer.encode(prompt)\n",
    "    output_tokens = tokenizer.encode(output)\n",
    "    num_input_tokens = len(input_tokens)\n",
    "    num_output_tokens = len(output_tokens)\n",
    "\n",
    "    # Updating cumulative counts\n",
    "    total_input_tok += num_input_tokens\n",
    "    total_output_tok += num_output_tokens\n",
    "\n",
    "\n",
    "# Calculate averages\n",
    "avg_time_per_prompt = (ttime / num_prompts)\n",
    "avg_toks_per_sec = total_output_tok/ttime\n",
    "avg_input_tokens = total_input_tok / num_prompts\n",
    "avg_output_tokens = total_output_tok / num_prompts\n",
    "\n",
    "em_i = emissions/total_input_tok *1_000_000\n",
    "em_o = emissions/total_output_tok *1_000_000\n",
    "em_p = emissions/num_prompts *10_000\n",
    "\n",
    "print(\"=\"*15 + f\" RESULTS for {name} \" + \"=\"*15 + \n",
    "    \"\\n\\n\" + \n",
    "    f\"\"\"\n",
    "    Finished Benchmark {bench_name} with {model_name}\\n\\n\n",
    "    Total Time: {ttime:.2f}s, AVG/Prompt: {avg_time_per_prompt:.2f}s\\n\\n\n",
    "    Average tokens per second: {avg_toks_per_sec:.2f}\\n\\n\n",
    "    Total Prompts: {num_prompts}\\n\n",
    "    Total Input Tokens: {total_input_tok}, AVG/Prompt: {avg_input_tokens}\\n\n",
    "    Total Output Tokens: {total_output_tok}, AVG/Prompt: {avg_output_tokens}\\n\n",
    "    \"\"\" + \n",
    "    \n",
    "    \"-\"*50 + \"\\n\" +\n",
    "    \n",
    "    f\"\"\"\n",
    "    Total Inference Emissions: {emissions:.3f}kg CO₂eq\\n\\n\n",
    "    Emissions / 1.000.000 Input Tokens: {em_i:.3f}kg CO₂eq\\n\n",
    "    Emissions / 1.000.000 Output Tokens: {em_o:.3f}kg CO₂eq\\n\n",
    "    Emissions / 10.000 Prompts: {em_p:.3f}kg CO₂eq\\n\n",
    "\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "wandb.log({\"Total Time\": ttime,\n",
    "    \"AVG. Time / Prompt\": avg_time_per_prompt,\n",
    "            \"AVG. Tokens / Second\": avg_toks_per_sec,\n",
    "            \"AVG. Input Tokens\": avg_input_tokens,\n",
    "            \"AVG. Output Tokens\": avg_output_tokens,\n",
    "            \"Total Emissions\": emissions,\n",
    "            \"Emissions / 1.000.000 Input Tokens\": em_i,\n",
    "            \"Emissions / 1.000.000 Output Tokens\": em_o,\n",
    "            \"Emissions / 10.000 Prompts\": em_p,\n",
    "            })\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# Save results to a CSV file\n",
    "results = [\n",
    "    [\"Model\", model_name],\n",
    "    [\"Benchmark\", bench_name],\n",
    "    [\"Number of GPUs\", num_gpus],\n",
    "    [\"Total Prompts\", num_prompts],\n",
    "    [\"Total Time\", ttime], \n",
    "    [\"AVG. Time / Prompt\", avg_time_per_prompt],\n",
    "    [\"AVG. Tokens / Second\", avg_toks_per_sec],\n",
    "    [\"Total Input Tokens\", total_input_tok],\n",
    "    [\"AVG. Input Tokens / Prompt\", avg_input_tokens],\n",
    "    [\"Total Output Tokens\", total_output_tok],\n",
    "    [\"AVG. Output Tokens / Prompt\", avg_output_tokens],\n",
    "    [\"Total Emissions\", emissions],\n",
    "    [\"Emissions / 1.000.000 Input Tokens\", em_i],\n",
    "    [\"Emissions / 1.000.000 Output Tokens\", em_o],\n",
    "    [\"Emissions / 10.000 Prompts\", em_p]\n",
    "]\n",
    "\n",
    "# Ensure the directory exists\n",
    "emission_output_file_path = f\"emission_data/{name}_emission_data.csv\"\n",
    "os.makedirs(os.path.dirname(emission_output_file_path), exist_ok=True)\n",
    "\n",
    "with open(emission_output_file_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Metric\", \"Value\"])\n",
    "    writer.writerows(results)\n",
    "\n",
    "print(f\"Results saved to {emission_output_file_path}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
