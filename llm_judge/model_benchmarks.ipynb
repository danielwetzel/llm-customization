{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this notebook you first have to follow these steps:\n",
    "\n",
    "1. Run the following command in your shell from the root directory of the repo: ```./init.sh ``` \n",
    "    - This command will install all the dependencies and initialize the submodules (it was written and tested on Ubuntu 20.04 running on AWS g6 ec2 instances)\n",
    "2. Login to Huggingface and Weights & Biases from your CLI:\n",
    "    - ```huggingface-cli login [Auth Token]```\n",
    "    - ```wandb login [Auth Token]```\n",
    "3. Update the Benchmark Config Files at \n",
    "    - ```llm_judge/arena-hard-auto/config/api_config.yaml```\n",
    "    - ```llm_judge/arena-hard-auto/config/gen_answer_config.yaml```\n",
    "    - ```llm_judge/arena-hard-auto/config/judge_config.yaml``` \n",
    "\n",
    "4. Start your LLM on an OpenAI API Server with vLLM using one of the following commands: \n",
    "    - With Docker: \n",
    "\n",
    "    ```docker run --runtime nvidia --gpus all -v ~/.cache/huggingface:/root/.cache/huggingface -p 8000:8000 --ipc=host vllm/vllm-openai:latest --model [model name (huggingface model id)] --tensor-parallel-size [number of gpus] --max-model-len 8192 --gpu-memory-utilization 0.85 --enable-chunked-prefill --served-model-name [api name for the model]```\n",
    "\n",
    "    \n",
    "    - Without Docker: \n",
    "\n",
    "    ```python -m vllm.entrypoints.openai.api_server --model [model name (huggingface model id)] --tensor-parallel-size [number of gpus] --max-model-len 8192 --gpu-memory-utilization 0.9 --enable-chunked-prefill --served-model-name [api name for the model]```\n",
    "\n",
    "\n",
    "After these steps you should see the message that your model is running on the address ```http://0.0.0.0:8000```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Additionally:\n",
    "Running the model on CPU without GPU: \n",
    "\n",
    "1. Build the Docker image. Run the following command from within the vllm submodule folder: \n",
    "- ```docker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g .```\n",
    "\n",
    "2. Run the Docker Container: \n",
    "- ```docker run -it --rm --network=host -v ~/.cache/huggingface:/root/.cache/huggingface -p 8000:8000 vllm-cpu-env --model [model name (huggingface model id)] --max-model-len 8192 --enable-chunked-prefill --served-model-name [api name for the model]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal setups for different model and instance sizes\n",
    "\n",
    "Setup for llama3.1-70B-FP8 on a 8 GPU Instance (g6.48xl): \n",
    "\n",
    "```docker run --runtime nvidia --gpus all -v ~/.cache/huggingface:/root/.cache/huggingface -p 8000:8000 --ipc=host vllm/vllm-openai:latest --model neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8 --tensor-parallel-size 8 --max-model-len 8192 --gpu-memory-utilization 0.80 --enable-chunked-prefill --served-model-name llama3_1_70b_fp8```\n",
    "\n",
    "\n",
    "Setup for llama3.1-70B-AWQ-INT4 on 8 GPUs (NOT OPTIMAL YET)\n",
    "\n",
    "```docker run --runtime nvidia --gpus all -v ~/.cache/huggingface:/root/.cache/huggingface -p 8000:8000 --ipc=host vllm/vllm-openai:latest --model hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4 --tensor-parallel-size 8 --max-model-len 8192 --gpu-memory-utilization 0.85 --enable-chunked-prefill --served-model-name llama3_1_70b_awq_int4 --tokenizer-pool-size 32```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOOK INTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--tokenizer-pool-size\n",
    "Size of tokenizer pool to use for asynchronous tokenization. If 0, will use synchronous tokenization.\n",
    "\n",
    "Default: 0\n",
    "\n",
    "--pipeline-parallel-size, -pp\n",
    "Number of pipeline stages.\n",
    "\n",
    "Default: 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import  AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import os\n",
    "from codecarbon import EmissionsTracker\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import yaml\n",
    "from vllm import LLM, SamplingParams\n",
    "from openai import OpenAI\n",
    "\n",
    "import wandb\n",
    "import gc\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 8\n"
     ]
    }
   ],
   "source": [
    "# Get the number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "print(f\"Number of available GPUs: {num_gpus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the Benchmark Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config will be written to:\n",
      "  arena-hard-auto/config/answer_config_temp.yaml\n",
      "\n",
      "Answers will be written to:\n",
      "  arena-hard-auto/data/arena-hard-v0.1/model_answer/llama3_1_70b_no_prompt_2.jsonl\n"
     ]
    }
   ],
   "source": [
    "#We have to select the same tokenizer all the time in order to get the same Token numbers at the end (this is only used to calculate the number of tokens)\n",
    "huggingface_model_name = \"meta-llama/Meta-Llama-3.1-70B-Instruct\" \n",
    "\n",
    "model_name = \"llama3_1_70b_no_prompt_2\"\n",
    "bench_name = 'arena-hard-v0.1'\n",
    "name = f\"{model_name}-{bench_name}-{num_gpus}gpus\"\n",
    "delete_old_bench = True\n",
    "max_gen_length = 2048 # Questions in the arena hard auto benchmark are sometimes longer than 2048 tokens. Therefore the model length has to be longer than 4096 tokens.\n",
    "temperature = 0.0\n",
    "num_choices = 1\n",
    "\n",
    "config_filename = 'arena-hard-auto/config/answer_config_temp.yaml'\n",
    "answer_filename = f'arena-hard-auto/data/{bench_name}/model_answer/{model_name}.jsonl'\n",
    "question_path = f'arena-hard-auto/data/{bench_name}/question.jsonl'\n",
    "\n",
    "print(\"Config will be written to:\\n  \" + config_filename)\n",
    "print(\"\\nAnswers will be written to:\\n  \" + answer_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the Open AI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Tokenizer to Count Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(huggingface_model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the temporary configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data to be written to the YAML file\n",
    "config_data = {\n",
    "    'name': name,\n",
    "    'bench_name': bench_name,\n",
    "    'temperature': temperature,\n",
    "    'max_tokens': max_gen_length,\n",
    "    'num_choices': num_choices,\n",
    "    'model_list': [\n",
    "        model_name\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the old benchmark result & config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLD Configuration file 'arena-hard-auto/config/answer_config_temp.yaml' deleted successfully.\n",
      "OLD Bench results file 'arena-hard-auto/data/arena-hard-v0.1/model_answer/llama3_1_70b_no_prompt_2.jsonl' deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Delete temporary config file if it exists\n",
    "if os.path.exists(config_filename):\n",
    "    os.remove(config_filename)\n",
    "    print(f\"OLD Configuration file '{config_filename}' deleted successfully.\")\n",
    "else:\n",
    "    print(f\"No OLD Configuration file found at '{config_filename}'\")\n",
    "\n",
    "\n",
    "# Delete old bench results if they exist\n",
    "if(delete_old_bench): \n",
    "    if os.path.exists(answer_filename):\n",
    "        os.remove(answer_filename)\n",
    "        print(f\"OLD Bench results file '{answer_filename}' deleted successfully.\")\n",
    "    else:\n",
    "        print(f\"No OLD Bench results file found at '{answer_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration file 'arena-hard-auto/config/answer_config_temp.yaml' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Write the config to a temporary YAML file\n",
    "with open(config_filename, 'w') as file:\n",
    "    yaml.dump(config_data, file, default_flow_style=False)\n",
    "\n",
    "print(f\"Configuration file '{config_filename}' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors to test the model\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    CBLACKBG  = '\\33[40m'\n",
    "    CREDBG    = '\\33[41m'\n",
    "    CGREENBG  = '\\33[42m'\n",
    "    CYELLOWBG = '\\33[43m'\n",
    "    CBLUEBG   = '\\33[44m'\n",
    "    CVIOLETBG = '\\33[45m'\n",
    "    CBEIGEBG  = '\\33[46m'\n",
    "    CWHITEBG  = '\\33[47m'\n",
    "    CBLACK  = '\\33[30m'\n",
    "    CRED    = '\\33[31m'\n",
    "    CGREEN  = '\\33[32m'\n",
    "    CYELLOW = '\\33[33m'\n",
    "    CBLUE   = '\\33[34m'\n",
    "    CVIOLET = '\\33[35m'\n",
    "    CBEIGE  = '\\33[36m'\n",
    "    CWHITE  = '\\33[37m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that prints the LLM response in a stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(prompt, stream, with_system=False):\n",
    "    print(\"=\"*30 + f\" Chat with  --- {model_name} ---  LLM using vLLM \" + \"=\"*30 + \"\\n\")\n",
    "    for idx, message in enumerate(prompt):\n",
    "        if prompt[idx]['role'] == 'user':\n",
    "            color = bcolors.CBLUE\n",
    "            print(color + f\"[ {prompt[idx]['role'].upper()} ]\" + bcolors.ENDC)\n",
    "            print(prompt[idx]['content']+ \"\\n\")\n",
    "        else: \n",
    "            if with_system:\n",
    "                color = bcolors.CVIOLET\n",
    "                print(color + f\"[ {prompt[idx]['role'].upper()} ]\" + bcolors.ENDC)\n",
    "                print(prompt[idx]['content']+ \"\\n\")\n",
    "\n",
    "    print(bcolors.OKGREEN + f\"[ ASSISTANT ]\" + bcolors.ENDC + \"\\n\")\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_questions_to_df(question_file: str):\n",
    "    \"\"\"Load questions from a file into a DataFrame.\"\"\"\n",
    "    questions = []\n",
    "    with open(question_file, \"r\") as ques_file:\n",
    "        for line in ques_file:\n",
    "            if line:\n",
    "                questions.append(json.loads(line))\n",
    "    \n",
    "    df = pd.DataFrame([{\n",
    "        \"question_id\": question[\"question_id\"],\n",
    "        \"content\": question[\"turns\"][0][\"content\"],\n",
    "        \"cluster\": question[\"cluster\"]\n",
    "    } for question in questions])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompts(df):\n",
    "    \"\"\"Prepare prompts dynamically based on question DataFrame.\"\"\"\n",
    "    prompts = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        system_prompt = f\"\"\"\n",
    "        You are a sophisticated AI-Expert there to help users solve tasks in several domains efficiently and accurately.\n",
    "        Now solve the following task from the domain \"{row['cluster']}\".\\n\n",
    "        \"\"\"\n",
    "\n",
    "        user_message = f\"{row['content']}\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ]\n",
    "\n",
    "        prompts.append(messages)\n",
    "\n",
    "    df['prompt'] = prompts\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>content</th>\n",
       "      <th>cluster</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>b2206e1b4310427b80d7af334940f08c</td>\n",
       "      <td>explain the following to me like I'm 5 years o...</td>\n",
       "      <td>Finance and Banking Operations</td>\n",
       "      <td>[{'role': 'system', 'content': '\n",
       "        You a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          question_id  \\\n",
       "173  b2206e1b4310427b80d7af334940f08c   \n",
       "\n",
       "                                               content  \\\n",
       "173  explain the following to me like I'm 5 years o...   \n",
       "\n",
       "                            cluster  \\\n",
       "173  Finance and Banking Operations   \n",
       "\n",
       "                                                prompt  \n",
       "173  [{'role': 'system', 'content': '\n",
       "        You a...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load questions into a DataFrame\n",
    "question_df = load_questions_to_df(\"arena-hard-auto/data/arena-hard-v0.1/question.jsonl\")\n",
    "\n",
    "# Prepare prompts\n",
    "question_df = prepare_prompts(question_df)\n",
    "\n",
    "# Select a random row from the DataFrame\n",
    "example_question_df = question_df.sample(n=1, random_state=random.randint(0, len(question_df) - 1))\n",
    "\n",
    "example_question_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': '\\n        You are a sophisticated AI-Expert there to help users solve tasks in several domains efficiently and accurately.\\n        Now solve the following task from the domain \"Finance and Banking Operations\".\\n\\n        '}, {'role': 'user', 'content': \"explain the following to me like I'm 5 years old: what is a PayFac model in integrated payments and how does this differ from what a normal merchant acquirer provides?\"}]\n"
     ]
    }
   ],
   "source": [
    "prompt = example_question_df['prompt'].values[0]\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Chat with  --- llama3_1_70b_no_prompt ---  LLM using vLLM ==============================\n",
      "\n",
      "\u001b[35m[ SYSTEM ]\u001b[0m\n",
      "\n",
      "        You are a sophisticated AI-Expert there to help users solve tasks in several domains efficiently and accurately.\n",
      "        Now solve the following task from the domain \"Finance and Banking Operations\".\n",
      "\n",
      "        \n",
      "\n",
      "\u001b[34m[ USER ]\u001b[0m\n",
      "explain the following to me like I'm 5 years old: what is a PayFac model in integrated payments and how does this differ from what a normal merchant acquirer provides?\n",
      "\n",
      "\u001b[92m[ ASSISTANT ]\u001b[0m\n",
      "\n",
      "Imagine you have a lemonade stand, and you want to sell lemonade to people who walk by. You need a way for them to pay you, right?\n",
      "\n",
      "A \"normal merchant acquirer\" is like a big bank that helps you take payments from customers. They give you a special machine (like a credit card reader) that lets people pay with their credit or debit cards. The bank then takes care of moving the money from the customer's account to your account.\n",
      "\n",
      "Now, a \"PayFac\" (short for Payment Facilitator) is like a special helper that makes it easier for you to take payments. Instead of dealing directly with the big bank, you deal with the PayFac. They give you a special way to take payments, like a credit card reader or a way to pay online.\n",
      "\n",
      "Here's the magic part: the PayFac is like a mini-bank that helps lots of little businesses like your lemonade stand. They take care of all the hard work of dealing with the big bank, so you don't have to. They also give you tools to help you run your business better, like reports on how much lemonade you're selling.\n",
      "\n",
      "The big difference between a PayFac and a normal merchant acquirer is that a PayFac is like a middleman that helps lots of small businesses. They make it easier for you to take payments and run your business, and they also give you more tools and help than a big bank would.\n",
      "\n",
      "Think of it like this:\n",
      "\n",
      "* Normal merchant acquirer: Big bank -> You\n",
      "* PayFac: Big bank -> PayFac -> You\n",
      "\n",
      "The PayFac is like a special helper that makes it easier for you to deal with the big bank, and they also give you more tools and support to help your business grow!"
     ]
    }
   ],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=prompt,\n",
    "    stream=True, \n",
    "    extra_body={\n",
    "        \"min_p\" : 0.05 # If it's set to 0.05, then it will allow tokens at least 1/20th as probable as the top token to be generated.\n",
    "    }\n",
    ")\n",
    "print_stream(prompt , stream, with_system=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Benchmark with Energy Consumption Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions(question_file: str):\n",
    "    \"\"\"Load questions from a file into a list.\"\"\"\n",
    "\n",
    "    questions = []\n",
    "    with open(question_file, \"r\") as ques_file:\n",
    "        for line in ques_file:\n",
    "            if line:\n",
    "                questions.append(json.loads(line))\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(answer_file: str):\n",
    "    \"\"\"Load model answers.\"\"\"\n",
    "\n",
    "    answers = []\n",
    "    with open(answer_file, \"r\") as ans_file:\n",
    "        for line in ans_file:\n",
    "            if line:\n",
    "                answers.append(json.loads(line))\n",
    "\n",
    "    return answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Starting Benchmark arena-hard-v0.1 with llama3_1_70b_no_prompt_2==========\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/llm-customization/llm_judge/wandb/run-20240808_173051-uiq2mio5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/llm-emissions/Model_Benchmarks/runs/uiq2mio5' target=\"_blank\">llama3_1_70b_no_prompt_2-arena-hard-v0.1-8gpus</a></strong> to <a href='https://wandb.ai/llm-emissions/Model_Benchmarks' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/llm-emissions/Model_Benchmarks' target=\"_blank\">https://wandb.ai/llm-emissions/Model_Benchmarks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/llm-emissions/Model_Benchmarks/runs/uiq2mio5' target=\"_blank\">https://wandb.ai/llm-emissions/Model_Benchmarks/runs/uiq2mio5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bench_name': 'arena-hard-v0.1', 'max_tokens': 2048, 'model_list': ['llama3_1_70b_no_prompt_2'], 'name': 'llama3_1_70b_no_prompt_2-arena-hard-v0.1-8gpus', 'num_choices': 1, 'temperature': 0.0}\n",
      "=========================  Expected Costs (based on GPT-4o)  =========================\n",
      "\n",
      "Expected Input Tokens: \n",
      " 47461 Tokens in a total of 500 questions\n",
      "\n",
      "Expected Output Tokens: \n",
      " 275000 Tokens in a total of 500 questions\n",
      "\n",
      "Max Output Tokens: \n",
      " 400000 Tokens in a total of 500 questions\n",
      "\n",
      "\n",
      "-------------------------  Resulting in Costs:   -------------------------\n",
      "\n",
      "Expected Costs: \n",
      " 4.36 USD\n",
      "\n",
      "Max. Expected Costs: \n",
      " 6.24 USD\n",
      "\n",
      "Starting to generate answers...\n",
      "Output to data/arena-hard-v0.1/model_answer/llama3_1_70b_no_prompt_2.jsonl\n",
      "arena-hard-v0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [31:06<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Finished Benchmark in 1867.38s\n",
      "=============== RESULTS for llama3_1_70b_no_prompt_2-arena-hard-v0.1-8gpus ===============\n",
      "\n",
      "\n",
      "    Finished Benchmark arena-hard-v0.1 with llama3_1_70b_no_prompt_2\n",
      "\n",
      "\n",
      "    Total Time: 1867.38s, AVG/Prompt: 3.73s\n",
      "\n",
      "\n",
      "    Average tokens per second: 180.99\n",
      "\n",
      "\n",
      "    Total Prompts: 500\n",
      "\n",
      "    Total Input Tokens: 83535, AVG/Prompt: 167.07\n",
      "\n",
      "    Total Output Tokens: 337972, AVG/Prompt: 675.944\n",
      "\n",
      "    --------------------------------------------------\n",
      "\n",
      "    Total Inference Emissions: 0.346kg CO₂eq\n",
      "\n",
      "\n",
      "    Emissions / 1.000.000 Input Tokens: 4.138kg CO₂eq\n",
      "\n",
      "    Emissions / 1.000.000 Output Tokens: 1.023kg CO₂eq\n",
      "\n",
      "    Emissions / 10.000 Prompts: 6.914kg CO₂eq\n",
      "\n",
      "\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82da9c7867654406b4bf4269d83a4928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.020 MB of 0.020 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AVG. Input Tokens</td><td>▁</td></tr><tr><td>AVG. Output Tokens</td><td>▁</td></tr><tr><td>AVG. Time / Prompt</td><td>▁</td></tr><tr><td>AVG. Tokens / Second</td><td>▁</td></tr><tr><td>Emissions / 1.000.000 Input Tokens</td><td>▁</td></tr><tr><td>Emissions / 1.000.000 Output Tokens</td><td>▁</td></tr><tr><td>Emissions / 10.000 Prompts</td><td>▁</td></tr><tr><td>Total Emissions</td><td>▁</td></tr><tr><td>Total Time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AVG. Input Tokens</td><td>167.07</td></tr><tr><td>AVG. Output Tokens</td><td>675.944</td></tr><tr><td>AVG. Time / Prompt</td><td>3.73476</td></tr><tr><td>AVG. Tokens / Second</td><td>180.98738</td></tr><tr><td>Emissions / 1.000.000 Input Tokens</td><td>4.13811</td></tr><tr><td>Emissions / 1.000.000 Output Tokens</td><td>1.0228</td></tr><tr><td>Emissions / 10.000 Prompts</td><td>6.91354</td></tr><tr><td>Total Emissions</td><td>0.34568</td></tr><tr><td>Total Time</td><td>1867.37877</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">llama3_1_70b_no_prompt_2-arena-hard-v0.1-8gpus</strong> at: <a href='https://wandb.ai/llm-emissions/Model_Benchmarks/runs/uiq2mio5' target=\"_blank\">https://wandb.ai/llm-emissions/Model_Benchmarks/runs/uiq2mio5</a><br/> View project at: <a href='https://wandb.ai/llm-emissions/Model_Benchmarks' target=\"_blank\">https://wandb.ai/llm-emissions/Model_Benchmarks</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240808_173051-uiq2mio5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to emission_data/llama3_1_70b_no_prompt_2-arena-hard-v0.1-8gpus_emission_data.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*10 + f\" Starting Benchmark {bench_name} with {model_name}\" + \"=\"*10 + \"\\n\\n\")\n",
    "\n",
    "qestion_path = f\"arena-hard-auto/data/{bench_name}/question.jsonl\"\n",
    "\n",
    "questions = get_questions(question_path)\n",
    "\n",
    "prompts = []\n",
    "\n",
    "for question in questions:\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "        You are a sophisticated AI-Expert there to help users solve tasks in several domains efficiently and accurately.\n",
    "        Now solve the following task from the domain \"{question['cluster']}\".\\n\n",
    "        \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question[\"turns\"][0][\"content\"]},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    prompts.append(prompt)\n",
    "\n",
    "num_prompts = len(prompts)\n",
    "\n",
    "total_input_tok = 0\n",
    "total_output_tok = 0\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Model_Benchmarks\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"benchmark_name\": bench_name,\n",
    "    \"num_prompts\": num_prompts,\n",
    "    \"framework\": 'vLLM',\n",
    "    \"model\": model_name,\n",
    "    \"num_gpus\": num_gpus,\n",
    "    },\n",
    "\n",
    "    name=name,\n",
    ")\n",
    "\n",
    "\n",
    "#### Start The Benchmark\n",
    "\n",
    "tracker = EmissionsTracker(save_to_file=True, project_name=f\"{name}\", log_level=\"error\", pue = 1.22, output_file=f\"emissions_benchmakrs.csv\")\n",
    "tracker.start()\n",
    "\n",
    "# Start Timer for Inference\n",
    "start_time = time.time()\n",
    "\n",
    "# Change the current working directory to 'arena-hard-auto'\n",
    "os.chdir('arena-hard-auto')\n",
    "\n",
    "%run -i 'gen_answer.py' --setting-file config/answer_config_temp.yaml --no-confirmation\n",
    "\n",
    "# End Timer for Inference\n",
    "end_time = time.time()\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "\n",
    "emissions: float = tracker.stop()\n",
    "\n",
    "ttime = end_time-start_time\n",
    "\n",
    "\n",
    "print(f\"\\n\\nFinished Benchmark in {ttime:.2f}s\")\n",
    "\n",
    "\n",
    "#### End the Benchmark\n",
    "\n",
    "\n",
    "answers_file = get_answers(answer_filename)\n",
    "outputs = [answer[\"choices\"][0][\"turns\"][0][\"content\"] for answer in answers_file]\n",
    "\n",
    "\n",
    "for idx, output in enumerate(outputs): \n",
    "\n",
    "    # Extracting information\n",
    "    prompt = prompts[idx]\n",
    "    input_tokens = tokenizer.encode(prompt)\n",
    "    output_tokens = tokenizer.encode(output)\n",
    "    num_input_tokens = len(input_tokens)\n",
    "    num_output_tokens = len(output_tokens)\n",
    "\n",
    "    # Updating cumulative counts\n",
    "    total_input_tok += num_input_tokens\n",
    "    total_output_tok += num_output_tokens\n",
    "\n",
    "\n",
    "# Calculate averages\n",
    "avg_time_per_prompt = (ttime / num_prompts)\n",
    "avg_toks_per_sec = total_output_tok/ttime\n",
    "avg_input_tokens = total_input_tok / num_prompts\n",
    "avg_output_tokens = total_output_tok / num_prompts\n",
    "\n",
    "em_i = emissions/total_input_tok *1_000_000\n",
    "em_o = emissions/total_output_tok *1_000_000\n",
    "em_p = emissions/num_prompts *10_000\n",
    "\n",
    "print(\"=\"*15 + f\" RESULTS for {name} \" + \"=\"*15 + \n",
    "    \"\\n\\n\" + \n",
    "    f\"\"\"\n",
    "    Finished Benchmark {bench_name} with {model_name}\\n\\n\n",
    "    Total Time: {ttime:.2f}s, AVG/Prompt: {avg_time_per_prompt:.2f}s\\n\\n\n",
    "    Average tokens per second: {avg_toks_per_sec:.2f}\\n\\n\n",
    "    Total Prompts: {num_prompts}\\n\n",
    "    Total Input Tokens: {total_input_tok}, AVG/Prompt: {avg_input_tokens}\\n\n",
    "    Total Output Tokens: {total_output_tok}, AVG/Prompt: {avg_output_tokens}\\n\n",
    "    \"\"\" + \n",
    "    \n",
    "    \"-\"*50 + \"\\n\" +\n",
    "    \n",
    "    f\"\"\"\n",
    "    Total Inference Emissions: {emissions:.3f}kg CO₂eq\\n\\n\n",
    "    Emissions / 1.000.000 Input Tokens: {em_i:.3f}kg CO₂eq\\n\n",
    "    Emissions / 1.000.000 Output Tokens: {em_o:.3f}kg CO₂eq\\n\n",
    "    Emissions / 10.000 Prompts: {em_p:.3f}kg CO₂eq\\n\n",
    "\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "wandb.log({\"Total Time\": ttime,\n",
    "    \"AVG. Time / Prompt\": avg_time_per_prompt,\n",
    "            \"AVG. Tokens / Second\": avg_toks_per_sec,\n",
    "            \"AVG. Input Tokens\": avg_input_tokens,\n",
    "            \"AVG. Output Tokens\": avg_output_tokens,\n",
    "            \"Total Emissions\": emissions,\n",
    "            \"Emissions / 1.000.000 Input Tokens\": em_i,\n",
    "            \"Emissions / 1.000.000 Output Tokens\": em_o,\n",
    "            \"Emissions / 10.000 Prompts\": em_p,\n",
    "            })\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# Save results to a CSV file\n",
    "results = [\n",
    "    [\"Model\", model_name],\n",
    "    [\"Benchmark\", bench_name],\n",
    "    [\"Number of GPUs\", num_gpus],\n",
    "    [\"Total Prompts\", num_prompts],\n",
    "    [\"Total Time\", ttime], \n",
    "    [\"AVG. Time / Prompt\", avg_time_per_prompt],\n",
    "    [\"AVG. Tokens / Second\", avg_toks_per_sec],\n",
    "    [\"Total Input Tokens\", total_input_tok],\n",
    "    [\"AVG. Input Tokens / Prompt\", avg_input_tokens],\n",
    "    [\"Total Output Tokens\", total_output_tok],\n",
    "    [\"AVG. Output Tokens / Prompt\", avg_output_tokens],\n",
    "    [\"Total Emissions\", emissions],\n",
    "    [\"Emissions / 1.000.000 Input Tokens\", em_i],\n",
    "    [\"Emissions / 1.000.000 Output Tokens\", em_o],\n",
    "    [\"Emissions / 10.000 Prompts\", em_p]\n",
    "]\n",
    "\n",
    "# Ensure the directory exists\n",
    "emission_output_file_path = f\"emission_data/{name}_emission_data.csv\"\n",
    "os.makedirs(os.path.dirname(emission_output_file_path), exist_ok=True)\n",
    "\n",
    "with open(emission_output_file_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Metric\", \"Value\"])\n",
    "    writer.writerows(results)\n",
    "\n",
    "print(f\"Results saved to {emission_output_file_path}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
