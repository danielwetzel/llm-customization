{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this notebook you first have to follow these steps:\n",
    "\n",
    "1. Run the following command in your shell from the root directory of the repo: ```./init.sh ``` \n",
    "    - This command will install all the dependencies and initialize the submodules (it was written and tested on Ubuntu 20.04 running on AWS g6 ec2 instances)\n",
    "2. Login to Huggingface and Weights & Biases from your CLI:\n",
    "    - ```huggingface-cli login [Auth Token]```\n",
    "    - ```wandb login [Auth Token]```\n",
    "3. Update the Benchmark Config Files at \n",
    "    - ```llm_judge/arena-hard-auto/config/api_config.yaml```\n",
    "    - ```llm_judge/arena-hard-auto/config/gen_answer_config.yaml```\n",
    "    - ```llm_judge/arena-hard-auto/config/judge_config.yaml``` \n",
    "\n",
    "4. Start your LLM on an OpenAI API Server with vLLM using one of the following commands: \n",
    "    - With Docker: \n",
    "\n",
    "    ```docker run --runtime nvidia --gpus all -v ~/.cache/huggingface:/root/.cache/huggingface -p 8000:8000 --ipc=host vllm/vllm-openai:latest --model [model name (huggingface model id)] --tensor-parallel-size [number of gpus] --max-model-len 8192 --gpu-memory-utilization 0.85 --enable-chunked-prefill --served-model-name [api name for the model]```\n",
    "\n",
    "    \n",
    "    - Without Docker: \n",
    "\n",
    "    ```python -m vllm.entrypoints.openai.api_server --model [model name (huggingface model id)] --tensor-parallel-size [number of gpus] --max-model-len 8192 --gpu-memory-utilization 0.9 --enable-chunked-prefill --served-model-name [api name for the model]```\n",
    "\n",
    "\n",
    "After these steps you should see the message that your model is running on the address ```http://0.0.0.0:8000```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal setups for different model and instance sizes\n",
    "\n",
    "Setup for llama3.1-70B-FP8 on a 8 GPU Instance (g6.48xl): \n",
    "\n",
    "```docker run --runtime nvidia --gpus all -v ~/.cache/huggingface:/root/.cache/huggingface -p 8000:8000 --ipc=host vllm/vllm-openai:latest --model neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8 --tensor-parallel-size 8 --max-model-len 8192 --gpu-memory-utilization 0.80 --enable-chunked-prefill --served-model-name llama3_1_70b_fp8 --max-num-seqs 320 --max-num-batched-tokens 2048```\n",
    "\n",
    "\n",
    "Setup for llama3.1-70B-AWQ-INT4 on 8 GPUs (NOT OPTIMAL YET)\n",
    "\n",
    "```docker run --runtime nvidia --gpus all -v ~/.cache/huggingface:/root/.cache/huggingface -p 8000:8000 --ipc=host vllm/vllm-openai:latest --model hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4 --tensor-parallel-size 8 --max-model-len 8192 --gpu-memory-utilization 0.85 --enable-chunked-prefill --served-model-name llama3_1_70b_awq_int4 --tokenizer-pool-size 32```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOOK INTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--tokenizer-pool-size\n",
    "Size of tokenizer pool to use for asynchronous tokenization. If 0, will use synchronous tokenization.\n",
    "\n",
    "Default: 0\n",
    "\n",
    "--pipeline-parallel-size, -pp\n",
    "Number of pipeline stages.\n",
    "\n",
    "Default: 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import  AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import os\n",
    "from codecarbon import EmissionsTracker\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import yaml\n",
    "from vllm import LLM, SamplingParams\n",
    "from openai import OpenAI\n",
    "\n",
    "import wandb\n",
    "import gc\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 4\n"
     ]
    }
   ],
   "source": [
    "# Get the number of available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "print(f\"Number of available GPUs: {num_gpus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the Benchmark Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config will be written to:\n",
      "  arena-hard-auto/config/answer_config_temp.yaml\n",
      "\n",
      "Answers will be written to:\n",
      "  arena-hard-auto/data/arena-hard-v0.1/model_answer/llama3_8b_fp8.jsonl\n"
     ]
    }
   ],
   "source": [
    "#We have to select the same tokenizer all the time in order to get the same Token numbers at the end (this is only used to calculate the number of tokens)\n",
    "huggingface_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\" \n",
    "\n",
    "model_name = \"llama3_8b_fp8\" #\"llama3_1_70b\"\n",
    "bench_name = 'arena-hard-v0.1'\n",
    "name = f\"{model_name}-{bench_name}-{num_gpus}gpus\"\n",
    "delete_old_bench = True\n",
    "max_gen_length = 2048 # Questions in the arena hard auto benchmark are sometimes longer than 2048 tokens. Therefore the model length has to be longer than 4096 tokens.\n",
    "temperature = 0.0\n",
    "num_choices = 1\n",
    "\n",
    "config_filename = 'arena-hard-auto/config/answer_config_temp.yaml'\n",
    "answer_filename = f'arena-hard-auto/data/{bench_name}/model_answer/{model_name}.jsonl'\n",
    "question_path = f'arena-hard-auto/data/{bench_name}/question.jsonl'\n",
    "\n",
    "print(\"Config will be written to:\\n  \" + config_filename)\n",
    "print(\"\\nAnswers will be written to:\\n  \" + answer_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the Open AI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Tokenizer to Count Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(huggingface_model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the temporary configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data to be written to the YAML file\n",
    "config_data = {\n",
    "    'name': name,\n",
    "    'bench_name': bench_name,\n",
    "    'temperature': temperature,\n",
    "    'max_tokens': max_gen_length,\n",
    "    'num_choices': num_choices,\n",
    "    'model_list': [\n",
    "        model_name\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the old benchmark result & config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLD Configuration file 'arena-hard-auto/config/answer_config_temp.yaml' deleted successfully.\n",
      "No OLD Bench results file found at 'arena-hard-auto/data/arena-hard-v0.1/model_answer/llama3_8b_fp8.jsonl'\n"
     ]
    }
   ],
   "source": [
    "# Delete temporary config file if it exists\n",
    "if os.path.exists(config_filename):\n",
    "    os.remove(config_filename)\n",
    "    print(f\"OLD Configuration file '{config_filename}' deleted successfully.\")\n",
    "else:\n",
    "    print(f\"No OLD Configuration file found at '{config_filename}'\")\n",
    "\n",
    "\n",
    "# Delete old bench results if they exist\n",
    "if(delete_old_bench): \n",
    "    if os.path.exists(answer_filename):\n",
    "        os.remove(answer_filename)\n",
    "        print(f\"OLD Bench results file '{answer_filename}' deleted successfully.\")\n",
    "    else:\n",
    "        print(f\"No OLD Bench results file found at '{answer_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration file 'arena-hard-auto/config/answer_config_temp.yaml' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Write the config to a temporary YAML file\n",
    "with open(config_filename, 'w') as file:\n",
    "    yaml.dump(config_data, file, default_flow_style=False)\n",
    "\n",
    "print(f\"Configuration file '{config_filename}' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors to test the model\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKCYAN = '\\033[96m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    CBLACKBG  = '\\33[40m'\n",
    "    CREDBG    = '\\33[41m'\n",
    "    CGREENBG  = '\\33[42m'\n",
    "    CYELLOWBG = '\\33[43m'\n",
    "    CBLUEBG   = '\\33[44m'\n",
    "    CVIOLETBG = '\\33[45m'\n",
    "    CBEIGEBG  = '\\33[46m'\n",
    "    CWHITEBG  = '\\33[47m'\n",
    "    CBLACK  = '\\33[30m'\n",
    "    CRED    = '\\33[31m'\n",
    "    CGREEN  = '\\33[32m'\n",
    "    CYELLOW = '\\33[33m'\n",
    "    CBLUE   = '\\33[34m'\n",
    "    CVIOLET = '\\33[35m'\n",
    "    CBEIGE  = '\\33[36m'\n",
    "    CWHITE  = '\\33[37m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that prints the LLM response in a stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(prompt, stream, with_system=False):\n",
    "    print(\"=\"*30 + f\" Chat with  --- {model_name} ---  LLM using vLLM \" + \"=\"*30 + \"\\n\")\n",
    "    for idx, message in enumerate(prompt):\n",
    "        if prompt[idx]['role'] == 'user':\n",
    "            color = bcolors.CBLUE\n",
    "            print(color + f\"[ {prompt[idx]['role'].upper()} ]\" + bcolors.ENDC)\n",
    "            print(prompt[idx]['content']+ \"\\n\")\n",
    "        else: \n",
    "            if with_system:\n",
    "                color = bcolors.CVIOLET\n",
    "                print(color + f\"[ {prompt[idx]['role'].upper()} ]\" + bcolors.ENDC)\n",
    "                print(prompt[idx]['content']+ \"\\n\")\n",
    "\n",
    "    print(bcolors.OKGREEN + f\"[ ASSISTANT ]\" + bcolors.ENDC + \"\\n\")\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_questions_to_df(question_file: str):\n",
    "    \"\"\"Load questions from a file into a DataFrame.\"\"\"\n",
    "    questions = []\n",
    "    with open(question_file, \"r\") as ques_file:\n",
    "        for line in ques_file:\n",
    "            if line:\n",
    "                questions.append(json.loads(line))\n",
    "    \n",
    "    df = pd.DataFrame([{\n",
    "        \"question_id\": question[\"question_id\"],\n",
    "        \"content\": question[\"turns\"][0][\"content\"],\n",
    "        \"cluster\": question[\"cluster\"]\n",
    "    } for question in questions])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompts(df):\n",
    "    \"\"\"Prepare prompts dynamically based on question DataFrame.\"\"\"\n",
    "    prompts = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        system_prompt = f\"\"\"\n",
    "        You are a sophisticated AI-Expert there to help users solve tasks in several domains efficiently and accurately.\n",
    "        Now solve the following task from the domain \"{row['cluster']}\".\\n\n",
    "        \"\"\"\n",
    "\n",
    "        user_message = f\"{row['content']}\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ]\n",
    "\n",
    "        prompts.append(messages)\n",
    "\n",
    "    df['prompt'] = prompts\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>content</th>\n",
       "      <th>cluster</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>d38fc9d20bd947f38abe497ae7d65522</td>\n",
       "      <td>Can you tell me how to get various HuggingFace...</td>\n",
       "      <td>HuggingFace Ecosystem Exploration</td>\n",
       "      <td>[{'role': 'system', 'content': '\n",
       "        You a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          question_id  \\\n",
       "204  d38fc9d20bd947f38abe497ae7d65522   \n",
       "\n",
       "                                               content  \\\n",
       "204  Can you tell me how to get various HuggingFace...   \n",
       "\n",
       "                               cluster  \\\n",
       "204  HuggingFace Ecosystem Exploration   \n",
       "\n",
       "                                                prompt  \n",
       "204  [{'role': 'system', 'content': '\n",
       "        You a...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load questions into a DataFrame\n",
    "question_df = load_questions_to_df(\"arena-hard-auto/data/arena-hard-v0.1/question.jsonl\")\n",
    "\n",
    "# Prepare prompts\n",
    "question_df = prepare_prompts(question_df)\n",
    "\n",
    "# Select a random row from the DataFrame\n",
    "example_question_df = question_df.sample(n=1, random_state=random.randint(0, len(question_df) - 1))\n",
    "\n",
    "example_question_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': '\\n        You are a sophisticated AI-Expert there to help users solve tasks in several domains efficiently and accurately.\\n        Now solve the following task from the domain \"HuggingFace Ecosystem Exploration\".\\n\\n        '}, {'role': 'user', 'content': 'Can you tell me how to get various HuggingFace LanguageModels working on my local machine using AutoGen'}]\n"
     ]
    }
   ],
   "source": [
    "prompt = example_question_df['prompt'].values[0]\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Chat with  --- llama3_8b_fp8 ---  LLM using vLLM ==============================\n",
      "\n",
      "\u001b[35m[ SYSTEM ]\u001b[0m\n",
      "\n",
      "        You are a sophisticated AI-Expert there to help users solve tasks in several domains efficiently and accurately.\n",
      "        Now solve the following task from the domain \"HuggingFace Ecosystem Exploration\".\n",
      "\n",
      "        \n",
      "\n",
      "\u001b[34m[ USER ]\u001b[0m\n",
      "Can you tell me how to get various HuggingFace LanguageModels working on my local machine using AutoGen\n",
      "\n",
      "\u001b[92m[ ASSISTANT ]\u001b[0m\n",
      "\n",
      "I'd be happy to help!\n",
      "\n",
      "To get various HuggingFace Language Models working on your local machine using AutoGen, follow these steps:\n",
      "\n",
      "**Prerequisites:**\n",
      "\n",
      "1. Install the `transformers` library by running `pip install transformers` in your terminal.\n",
      "2. Install the `autogen` library by running `pip install autogen` in your terminal.\n",
      "3. Make sure you have a compatible version of Python installed (Python 3.6 or later).\n",
      "\n",
      "**Step 1: Install the Model**\n",
      "\n",
      "Choose the language model you want to work with (e.g., BERT, RoBERTa, DistilBERT, etc.). You can find the list of available models on the HuggingFace website.\n",
      "\n",
      "Run the following command to install the model:\n",
      "```bash\n",
      "python -m transformers download <model_name>\n",
      "```\n",
      "Replace `<model_name>` with the actual name of the model you want to install (e.g., `bert-base-uncased`).\n",
      "\n",
      "**Step 2: Generate the AutoGen Configuration File**\n",
      "\n",
      "Create a new directory for your project and navigate to it in your terminal.\n",
      "\n",
      "Run the following command to generate the AutoGen configuration file:\n",
      "```bash\n",
      "autogen init --model-name <model_name> --output-dir .\n",
      "```\n",
      "Replace `<model_name>` with the actual name of the model you installed in Step 1.\n",
      "\n",
      "This command will generate a `config.json` file in the current directory.\n",
      "\n",
      "**Step 3: Configure the AutoGen Configuration File**\n",
      "\n",
      "Open the `config.json` file in a text editor and update the following settings:\n",
      "\n",
      "* `model_name`: Set this to the actual name of the model you installed (e.g., `bert-base-uncased`).\n",
      "* `num_workers`: Set this to the number of CPU cores you want to use for training (e.g., `4`).\n",
      "* `max_seq_length`: Set this to the maximum sequence length you want to use for training (e.g., `512`).\n",
      "\n",
      "Here's an example `config.json` file:\n",
      "```json\n",
      "{\n",
      "  \"model_name\": \"bert-base-uncased\",\n",
      "  \"num_workers\": 4,\n",
      "  \"max_seq_length\": 512\n",
      "}\n",
      "```\n",
      "**Step 4: Run AutoGen**\n",
      "\n",
      "Run the following command to start the AutoGen process:\n",
      "```\n",
      "autogen run --config config.json\n",
      "```\n",
      "This command will start the AutoGen process, which will download the pre-trained model weights, convert them to the desired format, and generate the necessary files for training.\n",
      "\n",
      "**Step 5: Verify the Model**\n",
      "\n",
      "After the AutoGen process completes, verify that the model is working correctly by running a simple example:\n",
      "```python\n",
      "import torch\n",
      "from transformers import BertTokenizer, BertModel\n",
      "\n",
      "# Load the pre-trained model and tokenizer\n",
      "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "model = BertModel.from_pretrained('bert-base-uncased')\n",
      "\n",
      "# Prepare the input text\n",
      "input_text = \"This is an example input text.\"\n",
      "\n",
      "# Encode the input text\n",
      "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
      "\n",
      "# Run the model\n",
      "outputs = model(input_ids)\n",
      "\n",
      "# Print the output\n",
      "print(outputs)\n",
      "```\n",
      "This code should output the model's output for the given input text.\n",
      "\n",
      "That's it! You should now have the HuggingFace Language Model working on your local machine using AutoGen."
     ]
    }
   ],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=prompt,\n",
    "    stream=True, \n",
    "    extra_body={\n",
    "        \"min_p\" : 0.05 # If it's set to 0.05, then it will allow tokens at least 1/20th as probable as the top token to be generated.\n",
    "    }\n",
    ")\n",
    "print_stream(prompt , stream, with_system=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Benchmark with Energy Consumption Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions(question_file: str):\n",
    "    \"\"\"Load questions from a file into a list.\"\"\"\n",
    "\n",
    "    questions = []\n",
    "    with open(question_file, \"r\") as ques_file:\n",
    "        for line in ques_file:\n",
    "            if line:\n",
    "                questions.append(json.loads(line))\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(answer_file: str):\n",
    "    \"\"\"Load model answers.\"\"\"\n",
    "\n",
    "    answers = []\n",
    "    with open(answer_file, \"r\") as ans_file:\n",
    "        for line in ans_file:\n",
    "            if line:\n",
    "                answers.append(json.loads(line))\n",
    "\n",
    "    return answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Starting Benchmark arena-hard-v0.1 with llama3_8b_fp8==========\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/llm-customization/llm_judge/wandb/run-20240806_161834-q6toj7xt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/llm-emissions/Model_Benchmarks/runs/q6toj7xt' target=\"_blank\">llama3_8b_fp8-arena-hard-v0.1-4gpus</a></strong> to <a href='https://wandb.ai/llm-emissions/Model_Benchmarks' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/llm-emissions/Model_Benchmarks' target=\"_blank\">https://wandb.ai/llm-emissions/Model_Benchmarks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/llm-emissions/Model_Benchmarks/runs/q6toj7xt' target=\"_blank\">https://wandb.ai/llm-emissions/Model_Benchmarks/runs/q6toj7xt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bench_name': 'arena-hard-v0.1', 'max_tokens': 2048, 'model_list': ['llama3_8b_fp8'], 'name': 'llama3_8b_fp8-arena-hard-v0.1-4gpus', 'num_choices': 1, 'temperature': 0.0}\n",
      "=========================  Expected Costs (based on GPT-4o)  =========================\n",
      "\n",
      "Expected Input Tokens: \n",
      " 47461 Tokens in a total of 500 questions\n",
      "\n",
      "Expected Output Tokens: \n",
      " 275000 Tokens in a total of 500 questions\n",
      "\n",
      "Max Output Tokens: \n",
      " 400000 Tokens in a total of 500 questions\n",
      "\n",
      "\n",
      "-------------------------  Resulting in Costs:   -------------------------\n",
      "\n",
      "Expected Costs: \n",
      " 4.36 USD\n",
      "\n",
      "Max. Expected Costs: \n",
      " 6.24 USD\n",
      "\n",
      "Starting to generate answers...\n",
      "Output to data/arena-hard-v0.1/model_answer/llama3_8b_fp8.jsonl\n",
      "arena-hard-v0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:43<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Finished Benchmark in 224.92s\n",
      "=============== RESULTS for llama3_8b_fp8-arena-hard-v0.1-4gpus ===============\n",
      "\n",
      "\n",
      "    Finished Benchmark arena-hard-v0.1 with llama3_8b_fp8\n",
      "\n",
      "\n",
      "    Total Time: 224.92s, AVG/Prompt: 0.45s\n",
      "\n",
      "\n",
      "    Average tokens per second: 1253.21\n",
      "\n",
      "\n",
      "    Total Prompts: 500\n",
      "\n",
      "    Total Input Tokens: 73535, AVG/Prompt: 147.07\n",
      "\n",
      "    Total Output Tokens: 281871, AVG/Prompt: 563.742\n",
      "\n",
      "    --------------------------------------------------\n",
      "\n",
      "    Total Inference Emissions: 0.015kg CO₂eq\n",
      "\n",
      "\n",
      "    Emissions / 1.000.000 Input Tokens: 0.204kg CO₂eq\n",
      "\n",
      "    Emissions / 1.000.000 Output Tokens: 0.053kg CO₂eq\n",
      "\n",
      "    Emissions / 10.000 Prompts: 0.300kg CO₂eq\n",
      "\n",
      "\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5f912b4d8d468d9bdb5d0ff35f9a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AVG. Input Tokens</td><td>▁</td></tr><tr><td>AVG. Output Tokens</td><td>▁</td></tr><tr><td>AVG. Time / Prompt</td><td>▁</td></tr><tr><td>AVG. Tokens / Second</td><td>▁</td></tr><tr><td>Emissions / 1.000.000 Input Tokens</td><td>▁</td></tr><tr><td>Emissions / 1.000.000 Output Tokens</td><td>▁</td></tr><tr><td>Emissions / 10.000 Prompts</td><td>▁</td></tr><tr><td>Total Emissions</td><td>▁</td></tr><tr><td>Total Time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AVG. Input Tokens</td><td>147.07</td></tr><tr><td>AVG. Output Tokens</td><td>563.742</td></tr><tr><td>AVG. Time / Prompt</td><td>0.44984</td></tr><tr><td>AVG. Tokens / Second</td><td>1253.20592</td></tr><tr><td>Emissions / 1.000.000 Input Tokens</td><td>0.20399</td></tr><tr><td>Emissions / 1.000.000 Output Tokens</td><td>0.05322</td></tr><tr><td>Emissions / 10.000 Prompts</td><td>0.30001</td></tr><tr><td>Total Emissions</td><td>0.015</td></tr><tr><td>Total Time</td><td>224.91994</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">llama3_8b_fp8-arena-hard-v0.1-4gpus</strong> at: <a href='https://wandb.ai/llm-emissions/Model_Benchmarks/runs/q6toj7xt' target=\"_blank\">https://wandb.ai/llm-emissions/Model_Benchmarks/runs/q6toj7xt</a><br/> View project at: <a href='https://wandb.ai/llm-emissions/Model_Benchmarks' target=\"_blank\">https://wandb.ai/llm-emissions/Model_Benchmarks</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240806_161834-q6toj7xt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to emission_data/llama3_8b_fp8-arena-hard-v0.1-4gpus_emission_data.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*10 + f\" Starting Benchmark {bench_name} with {model_name}\" + \"=\"*10 + \"\\n\\n\")\n",
    "\n",
    "qestion_path = f\"arena-hard-auto/data/{bench_name}/question.jsonl\"\n",
    "\n",
    "questions = get_questions(question_path)\n",
    "\n",
    "prompts = []\n",
    "\n",
    "for question in questions:\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "        You are a sophisticated AI-Expert there to help users solve tasks in several domains efficiently and accurately.\n",
    "        Now solve the following task from the domain \"{question['cluster']}\".\\n\n",
    "        \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question[\"turns\"][0][\"content\"]},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    prompts.append(prompt)\n",
    "\n",
    "num_prompts = len(prompts)\n",
    "\n",
    "total_input_tok = 0\n",
    "total_output_tok = 0\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Model_Benchmarks\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"benchmark_name\": bench_name,\n",
    "    \"num_prompts\": num_prompts,\n",
    "    \"framework\": 'vLLM',\n",
    "    \"model\": model_name,\n",
    "    \"num_gpus\": num_gpus,\n",
    "    },\n",
    "\n",
    "    name=name,\n",
    ")\n",
    "\n",
    "\n",
    "#### Start The Benchmark\n",
    "\n",
    "tracker = EmissionsTracker(save_to_file=True, project_name=f\"{name}\", log_level=\"error\", pue = 1.22, output_file=f\"emissions_benchmakrs.csv\")\n",
    "tracker.start()\n",
    "\n",
    "# Start Timer for Inference\n",
    "start_time = time.time()\n",
    "\n",
    "# Change the current working directory to 'arena-hard-auto'\n",
    "os.chdir('arena-hard-auto')\n",
    "\n",
    "%run -i 'gen_answer.py' --setting-file config/answer_config_temp.yaml --no-confirmation\n",
    "\n",
    "# End Timer for Inference\n",
    "end_time = time.time()\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "\n",
    "emissions: float = tracker.stop()\n",
    "\n",
    "ttime = end_time-start_time\n",
    "\n",
    "\n",
    "print(f\"\\n\\nFinished Benchmark in {ttime:.2f}s\")\n",
    "\n",
    "\n",
    "#### End the Benchmark\n",
    "\n",
    "\n",
    "answers_file = get_answers(answer_filename)\n",
    "outputs = [answer[\"choices\"][0][\"turns\"][0][\"content\"] for answer in answers_file]\n",
    "\n",
    "\n",
    "for idx, output in enumerate(outputs): \n",
    "\n",
    "    # Extracting information\n",
    "    prompt = prompts[idx]\n",
    "    input_tokens = tokenizer.encode(prompt)\n",
    "    output_tokens = tokenizer.encode(output)\n",
    "    num_input_tokens = len(input_tokens)\n",
    "    num_output_tokens = len(output_tokens)\n",
    "\n",
    "    # Updating cumulative counts\n",
    "    total_input_tok += num_input_tokens\n",
    "    total_output_tok += num_output_tokens\n",
    "\n",
    "\n",
    "# Calculate averages\n",
    "avg_time_per_prompt = (ttime / num_prompts)\n",
    "avg_toks_per_sec = total_output_tok/ttime\n",
    "avg_input_tokens = total_input_tok / num_prompts\n",
    "avg_output_tokens = total_output_tok / num_prompts\n",
    "\n",
    "em_i = emissions/total_input_tok *1_000_000\n",
    "em_o = emissions/total_output_tok *1_000_000\n",
    "em_p = emissions/num_prompts *10_000\n",
    "\n",
    "print(\"=\"*15 + f\" RESULTS for {name} \" + \"=\"*15 + \n",
    "    \"\\n\\n\" + \n",
    "    f\"\"\"\n",
    "    Finished Benchmark {bench_name} with {model_name}\\n\\n\n",
    "    Total Time: {ttime:.2f}s, AVG/Prompt: {avg_time_per_prompt:.2f}s\\n\\n\n",
    "    Average tokens per second: {avg_toks_per_sec:.2f}\\n\\n\n",
    "    Total Prompts: {num_prompts}\\n\n",
    "    Total Input Tokens: {total_input_tok}, AVG/Prompt: {avg_input_tokens}\\n\n",
    "    Total Output Tokens: {total_output_tok}, AVG/Prompt: {avg_output_tokens}\\n\n",
    "    \"\"\" + \n",
    "    \n",
    "    \"-\"*50 + \"\\n\" +\n",
    "    \n",
    "    f\"\"\"\n",
    "    Total Inference Emissions: {emissions:.3f}kg CO₂eq\\n\\n\n",
    "    Emissions / 1.000.000 Input Tokens: {em_i:.3f}kg CO₂eq\\n\n",
    "    Emissions / 1.000.000 Output Tokens: {em_o:.3f}kg CO₂eq\\n\n",
    "    Emissions / 10.000 Prompts: {em_p:.3f}kg CO₂eq\\n\n",
    "\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "wandb.log({\"Total Time\": ttime,\n",
    "    \"AVG. Time / Prompt\": avg_time_per_prompt,\n",
    "            \"AVG. Tokens / Second\": avg_toks_per_sec,\n",
    "            \"AVG. Input Tokens\": avg_input_tokens,\n",
    "            \"AVG. Output Tokens\": avg_output_tokens,\n",
    "            \"Total Emissions\": emissions,\n",
    "            \"Emissions / 1.000.000 Input Tokens\": em_i,\n",
    "            \"Emissions / 1.000.000 Output Tokens\": em_o,\n",
    "            \"Emissions / 10.000 Prompts\": em_p,\n",
    "            })\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# Save results to a CSV file\n",
    "results = [\n",
    "    [\"Model\", model_name],\n",
    "    [\"Benchmark\", bench_name],\n",
    "    [\"Number of GPUs\", num_gpus],\n",
    "    [\"Total Prompts\", num_prompts],\n",
    "    [\"Total Time\", ttime], \n",
    "    [\"AVG. Time / Prompt\", avg_time_per_prompt],\n",
    "    [\"AVG. Tokens / Second\", avg_toks_per_sec],\n",
    "    [\"Total Input Tokens\", total_input_tok],\n",
    "    [\"AVG. Input Tokens / Prompt\", avg_input_tokens],\n",
    "    [\"Total Output Tokens\", total_output_tok],\n",
    "    [\"AVG. Output Tokens / Prompt\", avg_output_tokens],\n",
    "    [\"Total Emissions\", emissions],\n",
    "    [\"Emissions / 1.000.000 Input Tokens\", em_i],\n",
    "    [\"Emissions / 1.000.000 Output Tokens\", em_o],\n",
    "    [\"Emissions / 10.000 Prompts\", em_p]\n",
    "]\n",
    "\n",
    "# Ensure the directory exists\n",
    "emission_output_file_path = f\"emission_data/{name}_emission_data.csv\"\n",
    "os.makedirs(os.path.dirname(emission_output_file_path), exist_ok=True)\n",
    "\n",
    "with open(emission_output_file_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Metric\", \"Value\"])\n",
    "    writer.writerows(results)\n",
    "\n",
    "print(f\"Results saved to {emission_output_file_path}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
